{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 快速开始\n",
    "\n",
    "## 哲学\n",
    "\n",
    "Transformers是为寻求使用/研究/扩展大型变压器模型的NLP研究人员而建的自有图书馆。\n",
    "\n",
    "该库在设计时考虑了两个强烈的目标：\n",
    "\n",
    "- 尽可能容易且快速地使用：\n",
    "  - 我们严格限制了要学习的面向用户抽象的数量，实际上几乎没有抽象，使用每个模型只需要三个标准类：配置，模型和令牌生成器，\n",
    "  - 所有这些类都可以使用通用的`from_pretrained()`实例化方法，以简单统一的方式从预训练的实例中初始化，该方法将负责从库或您自己提供的预训练实例中下载（如果需要），缓存和加载相关类。保存的实例。\n",
    "  - 因此，该库不是神经网络构建模块的模块化工具箱。如果要扩展/构建该库，只需使用常规的Python / PyTorch模块并从该库的基类继承即可重用模型加载/保存之类的功能。\n",
    "- 提供性能与原始模型尽可能接近的最新模型：\n",
    "  - 对于每种架构，我们至少提供一个示例，该示例再现了该架构的正式作者提供的结果，\n",
    "  - 该代码通常尽可能地接近原始代码库，这意味着某些PyTorch代码可能不像转化为TensorFlow代码那样具有*自爆性*。\n",
    "\n",
    "其他一些目标：\n",
    "\n",
    "- 尽可能一致地公开模型的内部：\n",
    "  - 我们使用单个API授予访问权限的全部隐藏状态和关注权重，\n",
    "  - 标记器和基本模型的API已标准化，可以在模型之间轻松切换。\n",
    "- 纳入主观选择的有前途的工具，以对这些模型进行微调/研究：\n",
    "  - 一种简单/一致的方法，可以向词汇表和嵌入物中添加新标记以进行微调，\n",
    "  - 遮盖和修剪变压器头的简单方法。\n",
    "\n",
    "## 主要概念\n",
    "\n",
    "该库针对每种模型围绕三种类型的类构建：\n",
    "\n",
    "- **模型类**是`torch.nn.Modules`库中当前提供的8种模型架构的PyTorch模型（），例如`BertModel`\n",
    "- **配置类**，用于存储构建模型所需的所有参数，例如`BertConfig`。您不必总是实例化这些自身，尤其是如果您使用的是经过预训练的模型而未进行任何修改，则创建模型将自动照顾实例化配置（这是模型的一部分）\n",
    "- **标记程序类**，用于存储每个模型的词汇表，并提供用于编码/解码要嵌入模型的标记嵌入索引列表中的字符串的方法，例如`BertTokenizer`\n",
    "\n",
    "所有这些类都可以从经过预训练的实例中实例化，并使用两种方法在本地保存：\n",
    "\n",
    "- `from_pretrained()`让你实例/配置/标记者从库本身可能提供了预训练版本（目前27款是作为上市模式[在这里](https://huggingface.co/transformers/pretrained_models.html)）由用户或存储在本地（或服务器上），\n",
    "- `save_pretrained()`使您可以在本地保存模型/配置/令牌，以便可以使用来重新加载它`from_pretrained()`。\n",
    "\n",
    "我们将通过一些简单的快速入门示例来结束本快速入门之旅，以了解如何实例化和使用这些类。本文档的其余部分分为两部分：\n",
    "\n",
    "- “ **主要类别”**部分详细介绍了三种主要类别（配置，模型，令牌生成器）的常见功能/方法/属性，以及一些作为培训实用程序提供的与优化相关的类别，\n",
    "- 该**装基准**部分的所有细节每个班级每个模型架构的变体，特别是呼吁他们每个人在输入/输出，你应该期望。\n",
    "\n",
    "## 快速导览：用法\n",
    "\n",
    "这是两个示例，展示了一些`Bert`和`GPT2`类以及预训练的模型。\n",
    "\n",
    "有关每个模型类的示例，请参见完整的API参考。\n",
    "\n",
    "### BERT示例\n",
    "\n",
    "让我们从使用文本字符串准备标记化的输入（将要馈送到Bert的标记嵌入索引的列表）开始 `BertTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/modongsong/.pyenv/versions/miniconda3-4.3.30/envs/transformers/lib/python3.7/site-packages/torch/_C.cpython-37m-darwin.so, 9): Symbol not found: _mkl_blas_caxpy\n  Referenced from: /Users/modongsong/.pyenv/versions/miniconda3-4.3.30/envs/transformers/lib/python3.7/site-packages/torch/lib/../../../../libmkl_intel_lp64.dylib\n  Expected in: flat namespace\n in /Users/modongsong/.pyenv/versions/miniconda3-4.3.30/envs/transformers/lib/python3.7/site-packages/torch/lib/../../../../libmkl_intel_lp64.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6533b2fb8252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.3.30/envs/transformers/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_dl_flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/modongsong/.pyenv/versions/miniconda3-4.3.30/envs/transformers/lib/python3.7/site-packages/torch/_C.cpython-37m-darwin.so, 9): Symbol not found: _mkl_blas_caxpy\n  Referenced from: /Users/modongsong/.pyenv/versions/miniconda3-4.3.30/envs/transformers/lib/python3.7/site-packages/torch/lib/../../../../libmkl_intel_lp64.dylib\n  Expected in: flat namespace\n in /Users/modongsong/.pyenv/versions/miniconda3-4.3.30/envs/transformers/lib/python3.7/site-packages/torch/lib/../../../../libmkl_intel_lp64.dylib"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们看看如何使用`BertModel`隐藏状态对输入进行编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0b344539db24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pre-trained model (weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Set the model in evaluation mode to deactivate the DropOut modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# This is IMPORTANT to have reproducible results during evaluation!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    # Transformers models always output tuples.\n",
    "    # See the models docstrings for the detail of all the outputs\n",
    "    # In our case, the first element is the hidden state of the last layer of the Bert model\n",
    "    encoded_layers = outputs[0]\n",
    "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
    "assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-57b472d13483>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-57b472d13483>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    以及如何用于`BertForMaskedLM`预测屏蔽令牌：\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "以及如何用于`BertForMaskedLM`预测屏蔽令牌："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
