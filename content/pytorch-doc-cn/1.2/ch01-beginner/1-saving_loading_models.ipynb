{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存和加载模型\n",
    "\n",
    "> 译者 [bruce1408](https://github.com/bruce1408)\n",
    "\n",
    "**作者:** [Matthew Inkawhich](https://github.com/MatthewInkawhich)\n",
    "\n",
    "本文提供有关Pytorch模型保存和加载的各种用例的解决方案。您可以随意阅读整个文档，或者只是跳转到所需用例的代码部分。\n",
    "\n",
    "当保存和加载模型时，有三个核心功能需要熟悉：\n",
    "\n",
    "1.  [torch.save](https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save): 将序列化对象保存到磁盘。 此函数使用 Python 的[pickle](https://docs.python.org/3/library/pickle.html)模块进行序列化。使用此函数可以保存如模型、tensor、字典等各种对象。\n",
    "2.  [torch.load](https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load): 使用 [pickle](https://docs.python.org/3/library/pickle.html)的 unpickling 功能将pickle对象文件反序列化到内存。 此功能还可以有助于设备加载数据(详见 [Saving & Loading Model Across Devices](#saving-loading-model-across-devices)).\n",
    "3.  [torch.nn.Module.load_state_dict](https://pytorch.org/docs/stable/nn.html?highlight=load_state_dict#torch.nn.Module.load_state_dict): 使用反序列化函数 _state_dict_ 来加载模型的参数字典。更多有关 _state_dict_ 的信息，请参考[What is a state_dict?](#what-is-a-state-dict).\n",
    "\n",
    "**内容:**\n",
    "\n",
    "*   [什么是`状态字典`?](#what-is-a-state-dict)\n",
    "*   [保存和加载推断模型](#saving-loading-model-for-inference)\n",
    "*   [保存 和 加载 Checkpoint](#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training)\n",
    "*   [在一个文件中保存多个模型](#saving-multiple-models-in-one-file)\n",
    "*   [使用在不同模型参数下的热启动模式](#warmstarting-model-using-parameters-from-a-different-model)\n",
    "*   [Saving & Loading Model Across Devices](#saving-loading-model-across-devices)\n",
    "\n",
    "## 什么是 `状态字典`?\n",
    "\n",
    "在Pytorch中，`torch.nn.Module` 模型的可学习参数(即权重和偏差)包含在模型的 _parameters_ 中，(使用`model.parameters()`可以进行访问)。 _state_dict_ 仅仅是python字典对象，它将每一层映射到其参数张量。注意，只有具有可学习参数的层(如卷积层、线性层等)的模型才具有 _state_dict_ 这一项。优化目标 `torch.optim` 也有 _state_dict_ 属性，它包含有关优化器的状态信息，以及使用的超参数。\n",
    "\n",
    "因为 _state_dict_ 的对象是python字典，所以他们可以很容易的保存、更新、更改和恢复，为Pytorch模型和优化器添加了大量模块。\n",
    "\n",
    "### 示例:\n",
    "\n",
    "让我们从 简单模型[训练一个分类器](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py)中了解一下 _state_dict_ 的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"Applies a 2D convolution over an input signal composed of several input\u001b[0m\n",
       "\u001b[0;34m    planes.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    In the simplest case, the output value of the layer with input size\u001b[0m\n",
       "\u001b[0;34m    :math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\u001b[0m\n",
       "\u001b[0;34m    can be precisely described as:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. math::\u001b[0m\n",
       "\u001b[0;34m        \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\u001b[0m\n",
       "\u001b[0;34m        \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\u001b[0m\n",
       "\u001b[0;34m    :math:`N` is a batch size, :math:`C` denotes a number of channels,\u001b[0m\n",
       "\u001b[0;34m    :math:`H` is a height of input planes in pixels, and :math:`W` is\u001b[0m\n",
       "\u001b[0;34m    width in pixels.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    * :attr:`stride` controls the stride for the cross-correlation, a single\u001b[0m\n",
       "\u001b[0;34m      number or a tuple.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    * :attr:`padding` controls the amount of implicit zero-paddings on both\u001b[0m\n",
       "\u001b[0;34m      sides for :attr:`padding` number of points for each dimension.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    * :attr:`dilation` controls the spacing between the kernel points; also\u001b[0m\n",
       "\u001b[0;34m      known as the à trous algorithm. It is harder to describe, but this `link`_\u001b[0m\n",
       "\u001b[0;34m      has a nice visualization of what :attr:`dilation` does.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    * :attr:`groups` controls the connections between inputs and outputs.\u001b[0m\n",
       "\u001b[0;34m      :attr:`in_channels` and :attr:`out_channels` must both be divisible by\u001b[0m\n",
       "\u001b[0;34m      :attr:`groups`. For example,\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        * At groups=1, all inputs are convolved to all outputs.\u001b[0m\n",
       "\u001b[0;34m        * At groups=2, the operation becomes equivalent to having two conv\u001b[0m\n",
       "\u001b[0;34m          layers side by side, each seeing half the input channels,\u001b[0m\n",
       "\u001b[0;34m          and producing half the output channels, and both subsequently\u001b[0m\n",
       "\u001b[0;34m          concatenated.\u001b[0m\n",
       "\u001b[0;34m        * At groups= :attr:`in_channels`, each input channel is convolved with\u001b[0m\n",
       "\u001b[0;34m          its own set of filters, of size:\u001b[0m\n",
       "\u001b[0;34m          :math:`\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        - a single ``int`` -- in which case the same value is used for the height and width dimension\u001b[0m\n",
       "\u001b[0;34m        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\u001b[0m\n",
       "\u001b[0;34m          and the second `int` for the width dimension\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. note::\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m         Depending of the size of your kernel, several (of the last)\u001b[0m\n",
       "\u001b[0;34m         columns of the input might be lost, because it is a valid `cross-correlation`_,\u001b[0m\n",
       "\u001b[0;34m         and not a full `cross-correlation`_.\u001b[0m\n",
       "\u001b[0;34m         It is up to the user to add proper padding.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. note::\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        When `groups == in_channels` and `out_channels == K * in_channels`,\u001b[0m\n",
       "\u001b[0;34m        where `K` is a positive integer, this operation is also termed in\u001b[0m\n",
       "\u001b[0;34m        literature as depthwise convolution.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        In other words, for an input of size :math:`(N, C_{in}, H_{in}, W_{in})`,\u001b[0m\n",
       "\u001b[0;34m        a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments\u001b[0m\n",
       "\u001b[0;34m        :math:`(in\\_channels=C_{in}, out\\_channels=C_{in} \\times K, ..., groups=C_{in})`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. include:: cudnn_deterministic.rst\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        in_channels (int): Number of channels in the input image\u001b[0m\n",
       "\u001b[0;34m        out_channels (int): Number of channels produced by the convolution\u001b[0m\n",
       "\u001b[0;34m        kernel_size (int or tuple): Size of the convolving kernel\u001b[0m\n",
       "\u001b[0;34m        stride (int or tuple, optional): Stride of the convolution. Default: 1\u001b[0m\n",
       "\u001b[0;34m        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\u001b[0m\n",
       "\u001b[0;34m        padding_mode (string, optional). Accepted values `zeros` and `circular` Default: `zeros`\u001b[0m\n",
       "\u001b[0;34m        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\u001b[0m\n",
       "\u001b[0;34m        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\u001b[0m\n",
       "\u001b[0;34m        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Shape:\u001b[0m\n",
       "\u001b[0;34m        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`\u001b[0m\n",
       "\u001b[0;34m        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m          .. math::\u001b[0m\n",
       "\u001b[0;34m              H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\u001b[0m\n",
       "\u001b[0;34m                        \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m          .. math::\u001b[0m\n",
       "\u001b[0;34m              W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\u001b[0m\n",
       "\u001b[0;34m                        \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Attributes:\u001b[0m\n",
       "\u001b[0;34m        weight (Tensor): the learnable weights of the module of shape\u001b[0m\n",
       "\u001b[0;34m                         :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\u001b[0m\n",
       "\u001b[0;34m                         :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\u001b[0m\n",
       "\u001b[0;34m                         The values of these weights are sampled from\u001b[0m\n",
       "\u001b[0;34m                         :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\u001b[0m\n",
       "\u001b[0;34m                         :math:`k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\u001b[0m\n",
       "\u001b[0;34m        bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,\u001b[0m\n",
       "\u001b[0;34m                         then the values of these weights are\u001b[0m\n",
       "\u001b[0;34m                         sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\u001b[0m\n",
       "\u001b[0;34m                         :math:`k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Examples::\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        >>> # With square kernels and equal stride\u001b[0m\n",
       "\u001b[0;34m        >>> m = nn.Conv2d(16, 33, 3, stride=2)\u001b[0m\n",
       "\u001b[0;34m        >>> # non-square kernels and unequal stride and with padding\u001b[0m\n",
       "\u001b[0;34m        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\u001b[0m\n",
       "\u001b[0;34m        >>> # non-square kernels and unequal stride and with padding and dilation\u001b[0m\n",
       "\u001b[0;34m        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\u001b[0m\n",
       "\u001b[0;34m        >>> input = torch.randn(20, 16, 50, 100)\u001b[0m\n",
       "\u001b[0;34m        >>> output = m(input)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. _cross-correlation:\u001b[0m\n",
       "\u001b[0;34m        https://en.wikipedia.org/wiki/Cross-correlation\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    .. _link:\u001b[0m\n",
       "\u001b[0;34m        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdilation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'circular'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mexpanded_padding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                                \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_padding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'circular'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                            \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                            \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/anaconda3-5.3.0/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import nn\n",
    "nn.Conv2d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = TheModelClass()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**输出:**\n",
    "\n",
    "```py\n",
    "Model's state_dict:\n",
    "conv1.weight     torch.Size([6, 3, 5, 5])\n",
    "conv1.bias   torch.Size([6])\n",
    "conv2.weight     torch.Size([16, 6, 5, 5])\n",
    "conv2.bias   torch.Size([16])\n",
    "fc1.weight   torch.Size([120, 400])\n",
    "fc1.bias     torch.Size([120])\n",
    "fc2.weight   torch.Size([84, 120])\n",
    "fc2.bias     torch.Size([84])\n",
    "fc3.weight   torch.Size([10, 84])\n",
    "fc3.bias     torch.Size([10])\n",
    "\n",
    "Optimizer's state_dict:\n",
    "state    {}\n",
    "param_groups     [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4675713712, 4675713784, 4675714000, 4675714072, 4675714216, 4675714288, 4675714432, 4675714504, 4675714648, 4675714720]}]\n",
    "\n",
    "```\n",
    "\n",
    "## 保存和加载推断模型\n",
    "\n",
    "### 保存/加载 `state_dict` (推荐使用)\n",
    "\n",
    "**保存:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当保存好模型用来推断的时候，只需要保存模型学习到的参数，使用 `torch.save()` 函数来保存模型 _state_dict_ ,它会给模型恢复提供最大的灵活性，这就是为什么要推荐它来保存的原因。\n",
    "\n",
    "在 Pytorch 中最常见的模型保存使用 ‘.pt’ 或者是 ‘.pth’ 作为模型文件扩展名。\n",
    "  \n",
    "请记住，在运行推理之前，务必调用 `model.eval()` 去设置 dropout 和 batch normalization 层为评估模式。如果不这么做，可能导致模型推断结果不一致。\n",
    "\n",
    "注意\n",
    "\n",
    "请注意 `load_state_dict()` 函数只接受字典对象，而不是保存对象的路径。这就意味着在你传给 `load_state_dict()` 函数之前，你必须反序列化你保存的 _state_dict_。例如，你无法通过 `model.load_state_dict(PATH)`来加载模型。\n",
    "\n",
    "### 保存/加载完整模型\n",
    "\n",
    "**保存:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Model class must be defined somewhere\n",
    "model = torch.load(PATH)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此部分保存/加载过程使用最直观的语法并涉及最少量的代码。以Python[pickle](https://docs.python.org/3/library/pickle.html)模块的方式来保存模型。这种方法的缺点是序列化数据受限于某种特殊的类而且需要确切的字典结构。这是因为pickle无法保存模型类本身。相反，它保存包含类的文件的路径，该文件在加载时使用。因此，当在其他项目使用或者重构之后，您的代码可能会以各种方式中断。\n",
    "\n",
    "在 Pytorch 中最常见的模型保存使用 ‘.pt’ 或者是 ‘.pth’ 作为模型文件扩展名。\n",
    "\n",
    "请记住，在运行推理之前，务必调用 `model.eval()` 去设置 dropout 和 batch normalization 层为评估模式。如果不这么做，可能导致模型推断结果不一致。\n",
    "\n",
    "## 保存 和 加载 Checkpoint 用于推理/继续训练\n",
    "\n",
    "### 保存:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            ...\n",
    "            }, PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当保存成 checkpoint 的时候，可用于推理或者是恢复训练，您保存的不仅仅是模型的 _state_dict_ 。 保存优化器的 _state_dict_ 也很重要, 因为它包含作为模型训练更新的缓冲区和参数。你也许想保存其他项目，比如最新记录的训练损失，外部的 `torch.nn.Embedding` 层等等。\n",
    "\n",
    "要保存多个组件，请在字典中组织它们并使用 `torch.save()` 来序列化字典。 Pytorch 中常见的保存checkpoint 是使用 `.tar` 文件扩展名。\n",
    "\n",
    "要加载项目，首先需要初始化模型和优化器，然后使用 `torch.load()` 来加载本地字典。 这里，您可以非常容易的通过简单查询字典来访问您所保存的项目。\n",
    "\n",
    "请记住在运行推理之前，务必调用 `model.eval()` 去设置 dropout 和 batch normalization 为评估。如果不这样做，有可能得到不一致的推断结果。如果你想要恢复训练，请调用 `model.train()` 以确保这些层处于训练模式。\n",
    "\n",
    "## 在一个文件中保存多个模型\n",
    "\n",
    "### 保存:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'modelA_state_dict': modelA.state_dict(),\n",
    "            'modelB_state_dict': modelB.state_dict(),\n",
    "            'optimizerA_state_dict': optimizerA.state_dict(),\n",
    "            'optimizerB_state_dict': optimizerB.state_dict(),\n",
    "            ...\n",
    "            }, PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = TheModelAClass(*args, **kwargs)\n",
    "modelB = TheModelBClass(*args, **kwargs)\n",
    "optimizerA = TheOptimizerAClass(*args, **kwargs)\n",
    "optimizerB = TheOptimizerBClass(*args, **kwargs)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "modelA.load_state_dict(checkpoint['modelA_state_dict'])\n",
    "modelB.load_state_dict(checkpoint['modelB_state_dict'])\n",
    "optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
    "optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])\n",
    "\n",
    "modelA.eval()\n",
    "modelB.eval()\n",
    "# - or -\n",
    "modelA.train()\n",
    "modelB.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当保存一个模型由多个 `torch.nn.Modules`组成时，例如GAN(对抗生成网络), sequence-to-sequence (序列到序列模型), 或者是多个模型融合, 您可以采用与保存常规检查点相同的方法。换句话说，保存每个模型的 _state_dict_ 的字典和相对应的优化器。如前所述，您可以通过简单地将它们附加到字典的方式来保存任何其他项目，这样有助于您恢复训练。\n",
    "\n",
    "Pytorch 中常见的保存checkpoint 是使用 `.tar` 文件扩展名。\n",
    "\n",
    "要加载项目，首先需要初始化模型和优化器，然后使用 `torch.load()` 来加载本地字典。 这里，您可以非常容易的通过简单查询字典来访问您所保存的项目。\n",
    "\n",
    "请记住在运行推理之前，务必调用 `model.eval()` 去设置 dropout 和 batch normalization 为评估。如果不这样做，有可能得到不一致的推断结果。如果你想要恢复训练，请调用 `model.train()` 以确保这些层处于训练模式。\n",
    "\n",
    "## 使用在不同模型参数下的热启动模式\n",
    "\n",
    "### 保存:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelA.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelB = TheModelBClass(*args, **kwargs)\n",
    "modelB.load_state_dict(torch.load(PATH), strict=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在迁移学习或训练新的复杂模型时， 部分加载模型或加载部分模型是常见的情况。利用训练好的参数，有助于热启动训练过程，并希望帮助您的模型比从头开始训练更快地收敛\n",
    "\n",
    "无论是从缺少某些键的 _state_dict_ 加载还是从键数多于加载模型的 _state_dict_ , 您可以通过在`load_state_dict()`函数中将`strict`参数设置为 **False** 来忽略非匹配键的函数。\n",
    "\n",
    "如果要将参数从一个层加载到另一个层，但是某些键不匹配，主要修改正在加载的 _state_dict_ 中的参数键的名称以匹配要在加载到模型中的键即可。\n",
    "\n",
    "## 通过设备保存/加载模型\n",
    "\n",
    "### 保存到 GPU, 加载到 CPU\n",
    "\n",
    "**保存:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, map_location=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当从CPU上加载模型在GPU上训练时, 将 `torch.device('cpu')` 传递给 `torch.load()` 函数中的 `map_location`参数.在这种情况下，使用`map_location` 参数将张量下的存储器动态的重新映射到CPU设备。\n",
    "\n",
    "### 保存到 GPU, 加载到 GPU\n",
    "\n",
    "**保存:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)\n",
    "# Make sure to call input = input.to(device) on any input tensors that you feed to the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当在GPU上训练并把模型保存在GPU，只需要使用 `model.to(torch.device('cuda'))`，将初始化的 `model` 转换为CUDA优化模型。另外，请务必在所有模型输入上使用 `.to(torch.device('cuda'))` 函数来为模型准备数据。请注意，调用 `my_tensor.to(device)` 会在GPU上返回`my_tensor` 的副本。因此，请记住手动覆盖张量：`my_tensor= my_tensor.to(torch.device('cuda'))`。\n",
    "\n",
    "### 保存到 CPU, 加载到 GPU\n",
    "\n",
    "**保存:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))  # Choose whatever GPU device number you want\n",
    "model.to(device)\n",
    "# Make sure to call input = input.to(device) on any input tensors that you feed to the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在CPU上训练好并保存的模型加载到GPU时， 将`torch.load()` 函数中的 `map_location` 参数设置为 _cuda:device_id_。这会将模型加载到指定的GPU设备。接下来，请务必调用 `model.to(torch.device('cuda'))` 将模型的参数张量转换为 CUDA 张量。最后，确保在所有模型输入上使用 `.to(torch.device('cuda'))` 函数来为CUDA优化模型。请注意， 调用 `my_tensor.to(device)` 会在GPU上返回 `my_tensor` 的新副本。 它不会覆盖 `my_tensor`。因此， 请手动覆盖张量 `my_tensor = my_tensor.to(torch.device('cuda'))`。\n",
    "\n",
    "### 保存 `torch.nn.DataParallel` 模型\n",
    "\n",
    "**保存:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.module.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to whatever device you want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.DataParallel` 是一个模型封装，支持并行GPU使用。要一般性的保存 `DataParallel` 模型, 请保存 `model.module.state_dict()`。这样，您就可以非常灵活地以任何方式加载模型到您想要的设备中。"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
