{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *torch.nn* 到底是什么？\n",
    "\n",
    "> 译者：[lhc741](https://github.com/lhc741)\n",
    ">\n",
    "> 校验：[DrDavidS](https://github.com/DrDavidS)\n",
    "\n",
    "**作者**：Jeremy Howard，[fast.ai](https://www.fast.ai/)。感谢Rachel Thomas和Francisco Ingham的帮助和支持。\n",
    "\n",
    "我们推荐使用notebook来运行这个教程，而不是脚本，点击[这里](https://pytorch.org/tutorials/beginner/nn_tutorial.html#sphx-glr-download-beginner-nn-tutorial-py)下载notebook(.ipynb)文件。\n",
    "\n",
    "Pytorch提供了[torch.nn](https://pytorch.org/docs/stable/nn.html)、[torch.optim](https://pytorch.org/docs/stable/optim.html)、[Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)和[DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)这些设计优雅的模块和类以帮助使用者创建和训练神经网络。\n",
    "为了最大化利用这些模块和类的功能，并使用它们做出适用于你所研究问题的模型，你需要真正理解他们是如何工作的。\n",
    "为了做到这一点，我们首先基于MNIST数据集训练一个没有任何特征的简单神经网络。\n",
    "最开始我们只会用到PyTorch中最基本的tensor功能，然后我们将会逐渐的从`torch.nn`，`torch.optim`，`Dataset`，`DataLoader`中选择一个特征加入到模型中，来展示新加入的特征会对模型产生什么样的效果，以及它是如何使模型变得更简洁或更灵活。\n",
    "\n",
    "**在这个教程中，我们假设你已经安装好了PyTorch，并且已经熟悉了基本的tensor运算。**(如果你熟悉Numpy的数组运算，你将会发现这里用到的PyTorch tensor运算和numpy几乎是一样的)\n",
    "\n",
    "## MNIST数据安装\n",
    "\n",
    "我们将要使用经典的[MNIST](http://deeplearning.net/data/mnist/)数据集，这个数据集由手写数字（0到9）的黑白图片组成。\n",
    "\n",
    "我们将使用[pathlib](https://docs.python.org/3/library/pathlib.html)来处理文件路径的相关操作（python3中的一个标准库），使用[request](http://docs.python-requests.org/en/master/)来下载数据集。\n",
    "我们只会在用到相关库的时候进行引用，这样你就可以明确在每个操作中用到了哪些库。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该数据集采用numpy数组格式，并已使用pickle存储，pickle是一个用来把数据序列化为python特定格式的库。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一幅图像都是28 x 28的，并被拉平成长度为784(=28x28)的一行。\n",
    "我们以其中一个为例展示一下，首先需要将这个一行的数据重新变形为一个2d的数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://pytorch.org/tutorials/_images/sphx_glr_nn_tutorial_001.png)\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "(50000, 784)\n",
    "```\n",
    "\n",
    "PyTorch使用`torch.tensor`，而不是numpy数组，所以我们需要将数据转换。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        ...,\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
    "torch.Size([50000, 784])\n",
    "tensor(0) tensor(9)\n",
    "```\n",
    "\n",
    "## 神经网络从零开始（不使用torch.nn）\n",
    "\n",
    "我们先来建立一个只使用PyTorch张量运算的模型。\n",
    "我们假设你已经熟悉神经网络的基础。（如果你还不熟悉，可以访问[course.fast.ai](https://course.fast.ai/)进行学习）。\n",
    "\n",
    "PyTorch提供创建随机数填充或全零填充张量的方法，我们使用该方法初始化一个简单线性模型的权重和偏置。\n",
    "这两个都是普通的张量，但它们有一个特殊的附加条件：设置需要计算梯度的参数为True。这样PyTorch就会记录所有与这个张量相关的运算，使其能在反向传播阶段自动计算梯度。\n",
    "\n",
    "对于weights而言，由于我们希望初始化张量过程中存在梯度，所以我们在初始化**之后**设置`requires_grad`。（注意：尾缀为`_`的方法在PyTorch中表示这个操作会被立即被执行。）\n",
    "\n",
    "> - 注意：\n",
    ">\n",
    "> 我们以[Xavier初始化](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)方法（每个元素都除以1/sqrt(n)）为例来对权重进行初始化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多亏了PyTorch具有自动梯度计算功能，我们可以使用Python中任何标准函数（或者可调用对象）来创建模型！\n",
    "因此，让我们编写一个普通的矩阵乘法和广播加法建立一个简单的线性模型。\n",
    "我们还需要一个激活函数，所以我们编写并使用一个log_softmax函数。\n",
    "请记住：尽管Pytorch提供了许多预先编写好的损失函数、激活函数等等，你仍然可以使用纯python轻松实现你自己的函数。\n",
    "Pytorch甚至可以自动地为你的函数创建快速的GPU代码或向量化的CPU代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的一段代码中，`@`表示点积运算符。我们将调用我们的函数计算一个批次的数据（本例中为64幅图像）。\n",
    "这是一次模型前向传递的过程。\n",
    "请注意，因为我们使用了随机数来初始化权重，所以在这个阶段我们的预测值并不会比随机的更好。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64  # 一批数据个数\n",
    "\n",
    "xb = x_train[0:bs]  # 从x获取一小批数据\n",
    "preds = model(xb)  # 预测值\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "tensor([-2.5125, -2.3091, -2.0139, -1.8648, -2.7132, -2.5598, -2.3423, -1.8809,\n",
    "        -2.5860, -2.7542], grad_fn=<SelectBackward>) torch.Size([64, 10])\n",
    "```\n",
    "\n",
    "可以从上面的结果不难看出，张量`preds`不仅包括了张量值，还包括了梯度函数。这个梯度函数我们可以在后面的反向传播阶段用到。\n",
    "\n",
    "下面我们来实现一个负的对数似然函数（Negative log-likehood）作为损失函数（同样也使用纯python实现）：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们查看下随机模型的损失值，这样我们就可以确认在执行反向传播的步骤后，模型的预测效果是否有了改进。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "tensor(2.3860, grad_fn=<NegBackward>)\n",
    "```\n",
    "\n",
    "我们再来实现一个用来计算模型准确率的函数。对于每次预测，我们规定如果预测结果中概率最大的数字和图片实际对应的数字是相同的，那么这次预测就是正确的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先来看一下被随机初始化的模型的准确率，这样我们就可以看到损失值降低的时候准确率是否提高了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "tensor(0.1094)\n",
    "```\n",
    "\n",
    "现在我们可以运行一个完整的训练步骤了，每次迭代，我们会进行以下几个操作：\n",
    "\n",
    "- 从全部数据中选择一小批数据（大小为`bs`）\n",
    "- 使用模型进行预测\n",
    "- 计算当前预测的损失值\n",
    "- 使用`loss.backward()`更新模型中的梯度，在这个例子中，更新的是`weights`和`bias`\n",
    "\n",
    "现在，我们来利用计算出的梯度对权值和偏置项进行更新，因为我们不希望这一步的操作被用于下一次迭代的梯度计算，所以我们在`torch.no_grad()`这个上下文管理器中完成。想要了解更多关于PyTorch Autograd是如何记录操作的，可以点击[这里](https://pytorch.org/docs/stable/notes/autograd.html)。\n",
    "\n",
    "接下来，我们将梯度设置为0，来为下一次循环做准备。否则我们的梯度将会记录所有已经执行过的运算（如，`loss.backward()`会将梯度变化值直接与变量已有值进行累加，而不是替换变量原有的值）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 小贴士\n",
    ">\n",
    "> 您可以使用标准python调试器对PyTorch代码进行单步调试，从而在每一步检查不同的变量值。取消下面的`set_trace()`注释，来尝试该功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5  # 学习率\n",
    "epochs = 2  # 训练的轮数\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        #         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前为止，我们已经从零开始完成了建立和训练一个最小的神经网络（因为我们建立的logistic回归模型不包含隐层）！\n",
    "\n",
    "现在，我们来看一下模型的损失值和准确率，并于我们之前输出的值进行比较。结果正如我们预期的，损失值下降，准确率提高。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "tensor(0.0831, grad_fn=<NegBackward>) tensor(1.)\n",
    "```\n",
    "\n",
    "## torch.nn.functional的使用\n",
    "\n",
    "现在，我们要对前面的代码进行重构，使代码在完成相同功能的同时，用PyTorch的`nn`来使代码变得更加简洁和灵活。\n",
    "从现在开始，接下来的每一步我们都会使代码变得更短，更好理解或更灵活。\n",
    "\n",
    "要进行的第一步也是最简单的一步，是使用`torch.nn.functional`（通过会在引用时用F表示）中的函数替换我们自己的激活函数和损失函数使代码变得更短。\n",
    "这个模块包含了`torch.nn`库中的所有函数（这个库的其它部分是各种类），所以在这个模块中还会找到其它便于建立神经网络的函数，比如池化函数。（模块中还包含卷积函数，线性函数等等，不过在后面的内容中我们会看到，这些操作使用库中的其它部分会更好。）\n",
    "\n",
    "如果你使用负对数似然损失和对数柔性最大值(softmax)激活函数，PyTorch有一个结合了这两个函数的简单函数`F.cross_entropy`供你使用，这样我们就可以删掉模型中的激活函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意在`model`函数中我们不再调用`log_softmax`。现在我们来确认一下损失值和准确率与之前相同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "tensor(0.0822, grad_fn=<NllLossBackward>) tensor(1.)\n",
    "```\n",
    "\n",
    "## 使用nn.Module进行重构\n",
    "接下来，我们将会用到`nn.Model`和`nn.Parameter`来完成一个更加清晰简洁的训练循环。我们继承`nn.Module`(它是一个能够跟踪状态的类)。在这个例子中，我们想要新建一个类，实现存储权重，偏置和前向传播步骤中所有用到方法。`nn.Module`包含了许多属性和方法（比如`.parameters()`和`.zero_grad()`），我们会在后面用到。\n",
    "\n",
    "> 注意\n",
    "> \n",
    "> `nn.Module`（M大写）是一个PyTorch中特有的概念，它是一个会经常用到的类。不要和Python中[module](https://docs.python.org/3/tutorial/modules.html)（`m`小写）混淆，module是一个可以被引入的Python代码文件。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然现在我们要使用一个对象而不是函数，我们要先对模型进行实例化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以像之前那样计算损失值了。注意`nn.Module`对象的使用方式很像函数（例如它们是*可调用的*），但是PyTorch将会自动调用我们的`forward`函数\n",
    "\n",
    "```py\n",
    "print(loss_func(model(xb), yb))\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "tensor(2.2001, grad_fn=<NllLossBackward>)\n",
    "```\n",
    "\n",
    "之前在每个训练循环中，我们通过变量名对每个变量的值进行更新，并手动的将每个变量的梯度置为0，像这样：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    weights -= weights.grad * lr\n",
    "    bias -= bias.grad * lr\n",
    "    weights.grad.zero_()\n",
    "    bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以利用`model.parameters()`和`model.zero_grad()`（这两个都是PyTorch定义在`nn.Module`中的）使这些步骤变得更加简洁并且更不容易忘记更新部分参数，尤其是模型很复杂的情况：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们把训练循环封装进`fit`函数中，这样就能在后面再次运行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来再次检查一下我们的损失值是否下降。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```py\n",
    "tensor(0.0812, grad_fn=<NllLossBackward>)\n",
    "```\n",
    "\n",
    "## 使用nn.Linear进行重构\n",
    "我们继续对代码进行重构。我们将用PyTorch中的[nn.Linear](https://pytorch.org/docs/stable/nn.html#linear-layers)代替手动定义和初始化`self.weights`和`self.bias`以及计算`xb @ self.weights + self.bias`, 因为`nn.Linear`可以完成这些操作。\n",
    "PyTorch中预设了很多类型的神经网络层，使用它们可以极大的简化我们的代码，通常还会带来速度上的提升。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们初始化模型并像之前那样计算损失值：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "tensor(2.2731, grad_fn=<NllLossBackward>)\n",
    "```\n",
    "\n",
    "我们仍然可以像之前那样使用`fit`函数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "tensor(0.0817, grad_fn=<NllLossBackward>)\n",
    "```\n",
    "\n",
    "## 使用optim进行重构\n",
    "PyTorch还有一个包含很多优化算法的包————`torch.optim`。我们可以使用优化器中的`step`方法执行前向传播过程中的步骤来替换手动更新每个参数。\n",
    "\n",
    "这个方法将允许我们替换之前手动编写的优化步骤：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "替换后如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()\n",
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（`optim.zero_grad()`将梯度重置为0，我们需要在计算下一次梯度之前调用它）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将建立模型和优化器的步骤定义为一个小函数方便将来复用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "tensor(2.2795, grad_fn=<NllLossBackward>)\n",
    "tensor(0.0813, grad_fn=<NllLossBackward>)\n",
    "```\n",
    "\n",
    "## 使用Dataset进行重构\n",
    "Pytorch包含一个Dataset抽象类。Dataset可以是任何东西，但它始终包含一个`__len__`函数（通过Python中的标准函数`len`调用）和一个用来索引到内容中的`__getitem__`函数。\n",
    "[这篇教程](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)以创建`Dataset`的自定义子类`FacialLandmarkDataset`为例进行介绍。\n",
    "\n",
    "PyTorch中的[TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset)是一个封装了张量的Dataset。通过定义长度和索引的方式，是我们可以对张量的第一维进行迭代，索引和切片。这将使我们在训练中，获取同一行中的自变量和因变量更加容易。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以把`x_train`和`y_train`中的数据合并成一个简单的`TensorDataset`，这样就可以方便的进行迭代和切片操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前，我们不得不分别对x和y的值进行迭代循环。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = x_train[start_i:end_i]\n",
    "yb = y_train[start_i:end_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以将这两步合二为一了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "xb,yb = train_ds[i*bs : i*bs+bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "tensor(0.0825, grad_fn=<NllLossBackward>)\n",
    "```\n",
    "\n",
    "## 使用DataLoader进行重构\n",
    "PyTorch的`DataLoader`负责批量数据管理，你可以使用任意的`Dataset`创建一个`DataLoader`。`DataLoader`使得对批量数据的迭代更容易。`DataLoader`自动的为我们提供每一小批量的数据来代替切片的方式`train_ds[i*bs : i*bs+bs]`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前我们像下面这样按批(xb,yb)对数据进行迭代：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range((n-1)//bs + 1):\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "    pred = model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们的循环变得更加简洁，因为使用了data loader来自动获取数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for xb,yb in train_dl:\n",
    "    pred = model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "tensor(0.0823, grad_fn=<NllLossBackward>)\n",
    "```\n",
    "\n",
    "多亏PyTorch中的`nn.Module`，`nn.Parameter`，`Dataset`和`DataLoader`，我们的训练代码变得非常简洁易懂。下面我们来试着增加一些用于提高模型效率所必需的的基本特征。\n",
    "\n",
    "## 增加验证集\n",
    " 在第一部分，我们仅仅是试着为我们的训练集构建一个合理的训练步骤，但实际上，我们**始终**应该有一个[验证集](https://www.fast.ai/2017/11/13/validation-sets/)来确认模型是否过拟合。\n",
    " \n",
    " 打乱训练数据的顺序通常是避免不同批数据中存在相关性和过拟合的[重要步骤](https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks)。另一方面，无论是否打乱顺序计算出的验证集损失值都是一样的。鉴于打乱顺序还会消耗额外的时间，所以打乱验证集数据是没有任何意义的。\n",
    " \n",
    " 我们在验证集上用到的每批数据的数量是训练集的两倍，这是因为在验证集上不需要进行反向传播，这样就会占用较小的内存（因为它并不需要储存梯度）。我们利用了这一点，使用了更大的batchsize，更快的计算出了损失值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将会在每轮(epoch)结束后计算并输出验证集上的损失值。\n",
    "\n",
    "（注意：在训练前我们总是会调用`model.train()`函数，在推断之前调用`model.eval()`函数，因为这些会被`nn.BatchNorm2d`，`nn.Dropout`等层使用，确保在不同阶段的准确性。）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "\n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "0 tensor(0.3039)\n",
    "1 tensor(0.2842)\n",
    "```\n",
    "\n",
    "## 编写fit()和get_data()函数\n",
    "现在我们来重构一下我们自己的函数。\n",
    "我们在计算训练集和验证集上的损失值时执行了差不多的过程两次，因此我们将这部分代码提炼成一个函数`loss_batch`，用来计算每个批的损失值。\n",
    "\n",
    "我们为训练集传递一个优化器参数来执行反向传播。对于验证集我们不传优化器参数，这样就不会执行反向传播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit`执行了训练模型的必要操作，并在每轮(epoch)结束后计算模型在训练集和测试集上的损失。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_data`返回训练集和验证集需要使用到的dataloaders。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们只需要三行代码就可以获取数据、拟合模型了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "    model, opt = get_model()\n",
    "    fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "0 0.3368099178314209\n",
    "1 0.28037907302379605\n",
    "```\n",
    "\n",
    "现在你能用这三行代码训练各种各样的模型。我们来看一下能否用它们来训练一个卷积神经网络（CNN）吧！\n",
    "\n",
    "## 应用到卷积神经网络\n",
    "\n",
    "我们现在将要创建一个包含三个卷积层的神经网络。因为前面章节中没有一个函数涉及到模型的具体形式，所以我们不需要对它们进行任何修改就可以训练一个卷积神经网络。\n",
    "\n",
    "我们将会使用PyTorch中预先定义好的[Conv2d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d)类作为我们的卷积层。我们定义一个有三个卷积层的卷积神经网络。每个卷积层之后会执行ReLu。在最后，我们会执行一个平均池化操作。\n",
    "（注意：`view`是PyTorch版的numpy `reshape`）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Momentum](https://cs231n.github.io/neural-networks-3/#sgd)是随机梯度下降的一个变型，它将前面步骤的更新也考虑在内，通常能够加快训练速度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```py\n",
    "0 0.3590330636262894\n",
    "1 0.24007404916286468\n",
    "```\n",
    "\n",
    "## nn.Sequential\n",
    "`torch.nn`中还有另一个类可以方便的用来简化我们的代码：[Sequential](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential)。一个`Sequential`对象可以序列化运行它包含的模块。这是一个更简单的搭建神经网络的方式。\n",
    "\n",
    "想要充分利用这一优势，我们要能够使用给定的函数轻松的定义一个**自定义层**。比如说，PyTorch中没有`view`层，我们需要为我们的网络定义一个。\n",
    "`Lambda`函数将会创建一个层，并在后面使用`Sequential`定义神经网络的时候用到。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`Sequential`创建模型非常简单：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```py\n",
    "0 0.340390869307518\n",
    "1 0.2571977460026741\n",
    "```\n",
    "\n",
    "## 封装 DataLoader\n",
    "我们的卷积神经网络已经非常简洁了，但是它只能运行在MNIST数据集上，原因如下：\n",
    "- 它假定输入是长度为28\\*28的向量\n",
    "- 它假定卷积神经网络最终输出是大小为4\\*4的网格（因为这是平均值池化操作时我们使用的核大小）\n",
    "\n",
    "让我们摆脱这两种假定，这样我们的模型就可以运行在任意的2d单通道图像上。\n",
    "首先，我们可以删除最初的Lambda层，并将数据预处理放在一个生成器中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们用`nn.AdaptiveAvgPool2d`替换`nn.AvgPool2d`，这个函数允许我们定义期望*输出*张量的大小，而不是定义已有*输入*的大小。\n",
    "这样我们的模型就可以处理任意大小的输入了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来试一下新的模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```py\n",
    "0 0.44035838775634767\n",
    "1 0.2957034994959831\n",
    "```\n",
    "\n",
    "## 使用你的GPU\n",
    "如果你有幸拥有支持CUDA的GPU（你可以租一个，大部分云服务提供商的价格使0.5$/每小时），那你可以用GPU来加速你的代码。\n",
    "首先检查一下的GPU是否可以被PyTorch调用：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "True\n",
    "```\n",
    "\n",
    "接下来，新建一个设备对象：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后更新一下`preprocess`函数将批运算移到GPU上计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们可以把模型移动到GPU上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你现在应该能发现运算变快了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```py\n",
    "0 0.2397482816696167\n",
    "1 0.2180072937965393\n",
    "```\n",
    "\n",
    "## 总结\n",
    "现在我们有一个用PyTorch构建的通用数据管道和训练循环可以用来训练很多类型的模型。\n",
    "想要知道训练一个模型有多么简单，可以参照`mnist_sample`这个例子。\n",
    "\n",
    "当然了，你可能还想要在模型中加入很多其它的东西，比如数据扩充，超参数调整，训练监控，迁移学习等。这些特性可以在fastai库中获取到，该库使用与本教程中介绍的相同设计方法开发的，为想要扩展模型的学习者提供了合理的后续步骤。\n",
    "\n",
    "在教程的开始部分，我们说了要通过例子对`torch.nn`，`torch.optim`，`Dataset`和`DataLoader`进行说明。\n",
    "现在我们来总结一下，我们都讲了些什么：\n",
    "- **torch.nn**\n",
    "    - `Module`：创建一个可调用的，其表现类似于函数，但又可以包含状态（比如神经网络层的权重）的对象。该对象知道它包含的`Parameter`（s），并可以将梯度置为0，以及对梯度进行循环以更新权重等。\n",
    "    - `Parameter`：是一个对张量的封装，它告诉`Module`在反向传播阶段更新权重。只有设置了*requires_grad*属性的张量会被更新。\n",
    "    - `functional`：一个包含了梯度函数、损失函数等以及一些无状态的层，如卷积层和线性层的模块（通常使用`F`作为导入的别名）。\n",
    "- `torch.optim`：包含了优化器，比如在反向阶段更新`Parameter`中权重的`SGD`。\n",
    "- `Dataset`：一个抽象接口，包含了`__len__`和`__getitem__`，还包含了PyTorch提供的类，如`TensorDataset`。\n",
    "- `DataLoader`：接受任意的`Dataset`并生成一个可以批量返回数据的迭代器（iterator ）。"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
