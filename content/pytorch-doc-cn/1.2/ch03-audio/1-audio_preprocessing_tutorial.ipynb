{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchaudio教程\n",
    "\n",
    "> 译者：[片刻](https://github.com/jiangzhonglian)\n",
    "> \n",
    "> 校验：[片刻](https://github.com/jiangzhonglian)\n",
    "\n",
    "PyTorch是一个开源深度学习平台，提供了从研究原型到具有GPU支持的生产部署的无缝路径。\n",
    "\n",
    "解决机器学习问题的巨大努力在于数据准备。torchaudio利用PyTorch的GPU支持，并提供许多工具来简化数据加载并使其更具可读性。在本教程中，我们将看到如何从简单的数据集中加载和预处理数据。\n",
    "\n",
    "对于本教程，请确保`matplotlib`已安装该软件包, 以方便查看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torch\n",
    "    import torchaudio\n",
    "    import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 打开数据集\n",
    "\n",
    "torchaudio支持以wav和mp3格式加载声音文件。我们将波形称为原始音频信号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    filename = \"https://pytorch.org/tutorials/_static/img/steam-train-whistle-daniel_simon-converted-from-mp3.wav\"\n",
    "    waveform, sample_rate = torchaudio.load(filename)\n",
    "\n",
    "    print(\"Shape of waveform: {}\".format(waveform.size()))\n",
    "    print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(waveform.t().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_001.png](https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_001.png)\n",
    "\n",
    "日期：\n",
    "\n",
    "    Shape of waveform: torch.Size([2, 276858])\n",
    "    Sample rate of waveform: 44100\n",
    "    \n",
    "\n",
    "转换\n",
    "torchaudio支持越来越多的 [转换](https://pytorch.org/audio/transforms.html)\n",
    "\n",
    "  * **Resample** ：将波形重采样为其他采样率。\n",
    "  * **Spectrogram** ：根据波形创建频谱图。\n",
    "  * **MelScale** ：使用转换矩阵将普通STFT转换为Mel频率STFT。\n",
    "  * **AmplitudeToDB** ：这将频谱图从功率/振幅标度转换为分贝标度。\n",
    "  * **MFCC** ：从波形创建梅尔频率倒谱系数。\n",
    "  * **MelSpectrogram** ：使用PyTorch中的STFT功能从波形创建MEL频谱图。\n",
    "  * **MuLawEncoding** ：基于mu-law压扩对波形进行编码。\n",
    "  * **MuLawDecoding** ：解码mu-law编码的波形。\n",
    "\n",
    "由于所有变换都是nn.Modules或jit.ScriptModules，因此它们可以随时用作神经网络的一部分。\n",
    "\n",
    "首先，我们可以以对数刻度查看频谱图的对数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    specgram = torchaudio.transforms.Spectrogram()(waveform)\n",
    "    \n",
    "    print(\"Shape of spectrogram: {}\".format(specgram.size()))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(specgram.log2()[0,:,:].numpy(), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_002.png](https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_002.png)\n",
    "\n",
    "Out:\n",
    "\n",
    "    Shape of spectrogram: torch.Size([2, 201, 1385])\n",
    "\n",
    "或者我们可以以对数刻度查看梅尔光谱图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    specgram = torchaudio.transforms.MelSpectrogram()(waveform)\n",
    "    \n",
    "    print(\"Shape of spectrogram: {}\".format(specgram.size()))\n",
    "    \n",
    "    plt.figure()\n",
    "    p = plt.imshow(specgram.log2()[0,:,:].detach().numpy(), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_003.png](https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_003.png)\n",
    "\n",
    "Out:\n",
    "    \n",
    "    Shape of spectrogram: torch.Size([2, 128, 1385])\n",
    "    \n",
    "\n",
    "我们可以一次对一个通道重新采样波形。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    new_sample_rate = sample_rate/10\n",
    "    \n",
    "    # Since Resample applies to a single channel, we resample first channel here\n",
    "    channel = 0\n",
    "    transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(1,-1))\n",
    "    \n",
    "    print(\"Shape of transformed waveform: {}\".format(transformed.size()))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(transformed[0,:].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_004.png](https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_004.png)\n",
    "\n",
    "Out:\n",
    "    \n",
    "    Shape of transformed waveform: torch.Size([1, 27686])\n",
    "    \n",
    "作为变换的另一个示例，我们可以基于Mu-Law编码对信号进行编码。但是要这样做，我们需要信号在-1和1之间。由于张量只是常规的PyTorch张量，因此我们可以在其上应用标准运算符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Let's check if the tensor is in the interval [-1,1]\n",
    "    print(\"Min of waveform: {}\\nMax of waveform: {}\\nMean of waveform: {}\".format(waveform.min(), waveform.max(), waveform.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out:\n",
    "\n",
    "    Min of waveform: -0.572845458984375\n",
    "    Max of waveform: 0.575958251953125\n",
    "    Mean of waveform: 9.293758921558037e-05\n",
    "    \n",
    "由于波形已经在-1和1之间，因此我们不需要对其进行归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def normalize(tensor):\n",
    "        # Subtract the mean, and scale to the interval [-1,1]\n",
    "        tensor_minusmean = tensor - tensor.mean()\n",
    "        return tensor_minusmean/tensor_minusmean.abs().max()\n",
    "    \n",
    "    # Let's normalize to the full interval [-1,1]\n",
    "    # waveform = normalize(waveform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们对波形进行编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    transformed = torchaudio.transforms.MuLawEncoding()(waveform)\n",
    "    \n",
    "    print(\"Shape of transformed waveform: {}\".format(transformed.size()))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(transformed[0,:].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_005.png](https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_005.png)\n",
    "\n",
    "Out:\n",
    "\n",
    "    Shape of transformed waveform: torch.Size([2, 276858])\n",
    "    \n",
    "\n",
    "现在解码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    reconstructed = torchaudio.transforms.MuLawDecoding()(transformed)\n",
    "    \n",
    "    print(\"Shape of recovered waveform: {}\".format(reconstructed.size()))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(reconstructed[0,:].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_006.png](https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_006.png)\n",
    "\n",
    "Out:\n",
    "\n",
    "    Shape of recovered waveform: torch.Size([2, 276858])\n",
    "    \n",
    "\n",
    "我们最终可以将原始波形与其重构版本进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Compute median relative difference\n",
    "    err = ((waveform-reconstructed).abs() / waveform.abs()).median()\n",
    "    \n",
    "    print(\"Median relative difference between original and MuLaw reconstucted signals: {:.2%}\".format(err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out:\n",
    "    \n",
    "    Median relative difference between original and MuLaw reconstucted signals: 1.28%\n",
    "    \n",
    "\n",
    "## 从Kaldi迁移到Torchaudio\n",
    "用户可能熟悉 语音识别工具包[Kaldi](http://github.com/kaldi-asr/kaldi)。torchaudio在中提供与之的兼容性 `torchaudio.kaldi_io`。实际上，它可以通过以下方式从kaldi scp或ark文件或流中读取：\n",
    "\n",
    "  * read_vec_int_ark\n",
    "  * read_vec_flt_scp\n",
    "  * read_vec_flt_arkfile /流\n",
    "  * read_mat_scp\n",
    "  * read_mat_ark\n",
    "\n",
    "torchaudio为GPU提供支持 spectrogram 并 fbank受益于Kaldi兼容的转换，请参见[此处](compliance.kaldi.html)以获取更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    n_fft = 400.0\n",
    "    frame_length = n_fft / sample_rate * 1000.0\n",
    "    frame_shift = frame_length / 2.0\n",
    "    \n",
    "    params = {\n",
    "        \"channel\": 0,\n",
    "        \"dither\": 0.0,\n",
    "        \"window_type\": \"hanning\",\n",
    "        \"frame_length\": frame_length,\n",
    "        \"frame_shift\": frame_shift,\n",
    "        \"remove_dc_offset\": False,\n",
    "        \"round_to_power_of_two\": False,\n",
    "        \"sample_frequency\": sample_rate,\n",
    "    }\n",
    "    \n",
    "    specgram = torchaudio.compliance.kaldi.spectrogram(waveform, **params)\n",
    "    \n",
    "    print(\"Shape of spectrogram: {}\".format(specgram.size()))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(specgram.t().numpy(), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_007.png](https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_007.png)\n",
    "\n",
    "Out:\n",
    "\n",
    "    Shape of spectrogram: torch.Size([1383, 201])\n",
    "\n",
    "我们还支持根据波形计算滤波器组特征，与Kaldi的实现相匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fbank = torchaudio.compliance.kaldi.fbank(waveform, **params)\n",
    "    \n",
    "    print(\"Shape of fbank: {}\".format(fbank.size()))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(fbank.t().numpy(), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_008.png](https://pytorch.org/tutorials/_images/sphx_glr_audio_preprocessing_tutorial_008.png)\n",
    "\n",
    "Out:\n",
    "\n",
    "    Shape of fbank: torch.Size([1383, 23])\n",
    "\n",
    "## 结论\n",
    "\n",
    "我们使用示例原始音频信号或波形来说明如何使用torchaudio打开音频文件，以及如何预处理和转换此类波形。鉴于torchaudio是基于PyTorch构建的，则这些技术可在利用GPU的同时用作更高级音频应用（例如语音识别）的构建块。\n",
    "\n",
    "**脚本的总运行时间：** （0分钟2.343秒）\n",
    "\n",
    "[`Download Python source code:\n",
    "audio_preprocessing_tutorial.py`](https://pytorch.org/tutorials/_downloads/5ffe15ce830e55b3a9e9c294d04ab41c/audio_preprocessing_tutorial.py)\n",
    "\n",
    "[`Download Jupyter notebook:\n",
    "audio_preprocessing_tutorial.ipynb`](https://pytorch.org/tutorials/_downloads/7303ce3181f4dbc9a50bc1ed5bb3218f/audio_preprocessing_tutorial.ipynb)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
