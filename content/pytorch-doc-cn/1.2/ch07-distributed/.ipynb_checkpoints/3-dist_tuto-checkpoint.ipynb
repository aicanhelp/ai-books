{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\\. PyTorch编写分布式应用程序\n",
    "\n",
    "**作者** ：[ SEB阿诺德](https://seba1511.com)\n",
    "\n",
    "在这个简短的教程中，我们将要在PyTorch的分布式包。我们将看到如何建立分布式设置，使用不同的沟通策略，走在包装件的一些内部结构。\n",
    "\n",
    "## 设定\n",
    "\n",
    "包括在PyTorch分布式包（即，torch.distributed `\n",
    "`）使研究人员和从业人员跨进程和机器的集群来容易并行化的计算。为了这样做，它利用了消息传递语义允许每个进程进行数据通信的任何其他进程的。而不是在多处理（`\n",
    "torch.multiprocessing`）包，过程可以使用不同的通信后端和不限于在同一台机器上被执行。\n",
    "\n",
    "为了开始，我们需要同时运行多个流程的能力。如果你有机会到计算机集群，你应该用你的本地系统管理员检查或使用自己喜欢的协调工具。 （例如，[ PDSH\n",
    "](https://linux.die.net/man/1/pdsh)，[ clustershell ](https://cea-\n",
    "hpc.github.io/clustershell/)或[人](https://slurm.schedmd.com/)）对于本教程的目的，我们将使用一台机器和叉的多个进程使用以下模板。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "def run(rank, size):\n",
    "    \"\"\" Distributed function to be implemented later. \"\"\"\n",
    "    pass\n",
    "\n",
    "def init_processes(rank, size, fn, backend='Gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_processes, args=(rank, size, run))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "\n",
    "上述脚本派生两个过程谁将每个设置的分布式环境中，初始化处理组（`dist.init_process_group`），最后执行`运行给定的 `功能。\n",
    "\n",
    "让我们来看看`init_processes\n",
    "`功能。它确保每一道工序将能够通过一个主协调，使用相同的IP地址和端口。请注意，我们使用的是TCP后端，但我们也可以使用[ MPI\n",
    "](https://en.wikipedia.org/wiki/Message_Passing_Interface)或[ GLOO\n",
    "](https://github.com/facebookincubator/gloo)代替。 （参见 5.1节），我们会在魔术`\n",
    "dist.init_process_group`在本教程的最后发生的事情，但它基本上可以让进程间通信其他通过分享他们的位置。\n",
    "\n",
    "## 点对点通信\n",
    "\n",
    "[![Send and Recv](../img/send_recv.png)](../img/send_recv.png)\n",
    "\n",
    "传送和recv\n",
    "\n",
    "数据从一个处理A转移到另一个被称为点 - 点通信。这些通过取得的`发送 `和`的recv`的功能或它们的 _立即_ 反份，`isend`和`\n",
    "irecv`。\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "\n",
    "在上面的例子，这两个过程以零开始张量，然后处理增量0张量，并将其发送到处理1，使得它们都结束了1.0。请注意，过程1只需要以存储将接收数据分配内存。\n",
    "\n",
    "还要注意，`发送 `/ `的recv`是 **阻断** ：两个过程停止，直到通信完成。在另一方面的立即被 **非阻塞**\n",
    ";脚本将继续其执行和方法都返回一个`DistributedRequest`对象后，我们可以选择`等待（） `。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    req = None\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        req = dist.isend(tensor=tensor, dst=1)\n",
    "        print('Rank 0 started sending')\n",
    "    else:\n",
    "        # Receive tensor from process 0\n",
    "        req = dist.irecv(tensor=tensor, src=0)\n",
    "        print('Rank 1 started receiving')\n",
    "    req.wait()\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "\n",
    "当使用的立即，我们必须小心我们的发送和接收的张量的使用。因为我们不知道什么时候的数据将被传递给其它工艺做的，我们不应该修改发张量也不`req.wait（）\n",
    "`完成之前访问接收到的张量。换一种说法，\n",
    "\n",
    "  * 写`张量 ``dist.isend后（） `将导致未定义的行为。\n",
    "  * 从`读取张量 ``dist.irecv后（） `将导致未定义的行为。\n",
    "\n",
    "然而，`req.wait（） `已被执行之后，我们保证了通信发生了，并且，存储在`张量的值[0]`是1.0。\n",
    "\n",
    "点至点，当我们想在我们的流程的通信进行细粒度的控制通信是有益的。它们可以被用来实现花哨的算法，如[百度的DeepSpeech\n",
    "](https://github.com/baidu-research/baidu-allreduce)或[\n",
    "Facebook的大规模实验[HTG3。所使用的（c.f。](https://research.fb.com/publications/imagenet1kin1h/)第4.1节）\n",
    "\n",
    "## 集体通信\n",
    "\n",
    "\n",
    "| [![Scatter](../img/scatter.png)](https://pytorch.org/tutorials/_images/scatter.png)Scatter | [![Gather](../img/gather.png)](https://pytorch.org/tutorials/_images/gather.png)Gather |\n",
    "| ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| [![Reduce](../img/reduce.png)](https://pytorch.org/tutorials/_images/reduce.png)Reduce | [![All-Reduce](../img/all_reduce.png)](https://pytorch.org/tutorials/_images/all_reduce.png)All-Reduce |\n",
    "| [![Broadcast](../img/broadcast.png)](https://pytorch.org/tutorials/_images/broadcast.png)Broadcast | [![All-Gather](../img/all_gather.png)](https://pytorch.org/tutorials/_images/all_gather.png)All-Gather |\n",
    "\n",
    "  \n",
    "相对于点对点通信电子，集体允许对 **组** 中所有进程的通信模式。 A组是我们所有进程的一个子集。要创建一个组，我们可以通过职级为`\n",
    "dist.new_group（集团）名单 [HTG5。默认情况下，集体的对所有进程执行，也被称为\n",
    "**世界[HTG7。例如，为了获得在所有进程都张量的总和，我们可以使用`dist.all_reduce（张量， 运算， 组） `集体。**`\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\"\"\" All-Reduce example.\"\"\"\n",
    "def run(rank, size):\n",
    "    \"\"\" Simple point-to-point communication. \"\"\"\n",
    "    group = dist.new_group([0, 1])\n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "既然我们要在组中的所有张量的总和，我们使用`dist.reduce_op.SUM\n",
    "`为降低运营商。一般来说，任何可交换的数学运算，可以作为运营商。外的开箱，PyTorch配备了4个这样的运营商，都在逐元素级别工作：\n",
    "\n",
    "  * `dist.reduce_op.SUM`\n",
    "  * `dist.reduce_op.PRODUCT`\n",
    "  * `dist.reduce_op.MAX`\n",
    "  * `dist.reduce_op.MIN`。\n",
    "\n",
    "除了`dist.all_reduce（张量， 运算， 组） `，有一个总的目前PyTorch实现6个集体。\n",
    "\n",
    "  * `dist.broadcast（张量， SRC， 组） `：复制`张量 `从`SRC`到所有其它过程。\n",
    "  * `dist.reduce（张量， DST， 运算， 组） `：应用`OP`所有`结果张量 `，并存储在`DST`。\n",
    "  * `dist.all_reduce（张量， 运算， 组） `：同降低，但其结果被存储在所有进程。\n",
    "  * `dist.scatter（张量， SRC， scatter_list， 组） `：复制 \\（ I ^ {\\文本{第}} \\）张量`scatter_list [I]`到 \\（I ^ {\\文本{第}} \\）过程。\n",
    "  * `dist.gather（张量， DST， gather_list， 组） `：复制`张量 `从`DST`所有进程。\n",
    "  * `dist.all_gather（tensor_list， 张量， 组） `：复制`张量 `从所有流程，以`tensor_list`上的所有进程。\n",
    "  * `dist.barrier（组） `：块组的所有进程，直至每一个已经进入该功能。\n",
    "\n",
    "## 分布式训练\n",
    "\n",
    "**注：** 你可以在[这个GitHub的库](https://github.com/seba-1511/dist_tuto.pth/)本节的示例脚本。\n",
    "\n",
    "现在我们明白了分布式模块是如何工作的，让我们写的东西与它有用。我们的目标是复制的[ DistributedDataParallel\n",
    "](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)的功能。当然，这将是一个说教的例子，在现实世界situtation你应该使用官方的，经过严格测试和精心优化的版本上面链接。\n",
    "\n",
    "简单地说，我们要实现的随机梯度下降一个分布式的版本。我们的脚本将让所有的进程都计算在他们的批量数据的他们的模型的梯度，然后平均的梯度。为了改变进程的数目时，以确保类似的收敛结果，我们首先要分区我们的数据。\n",
    "（你也可以使用[ tnt.dataset.SplitDataset\n",
    "](https://github.com/pytorch/tnt/blob/master/torchnet/dataset/splitdataset.py#L4)，而不是片段下方。）\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Dataset partitioning helper \"\"\"\n",
    "class Partition(object):\n",
    "\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]\n",
    "\n",
    "\n",
    "class DataPartitioner(object):\n",
    "\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        data_len = len(data)\n",
    "        indexes = [x for x in range(0, data_len)]\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[0:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述片段中，我们现在可以简单地使用下面的几行分区中的任何数据集：\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\"\"\" Partitioning MNIST \"\"\"\n",
    "def partition_dataset():\n",
    "    dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "    size = dist.get_world_size()\n",
    "    bsz = 128 / float(size)\n",
    "    partition_sizes = [1.0 / size for _ in range(size)]\n",
    "    partition = DataPartitioner(dataset, partition_sizes)\n",
    "    partition = partition.use(dist.get_rank())\n",
    "    train_set = torch.utils.data.DataLoader(partition,\n",
    "                                         batch_size=bsz,\n",
    "                                         shuffle=True)\n",
    "    return train_set, bsz\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "\n",
    "假设我们有2个副本，那么每个进程将具有`train_set`60000/2 = 30000个样本。我们还除以副本的数量批量大小，以保持的128\n",
    "_总体_ 批量大小。\n",
    "\n",
    "现在，我们可以写我们通常前后，优化训练码，并添加一个函数调用来平均我们的模型的梯度。 （下面是从官方[ PyTorch\n",
    "MNIST例如](https://github.com/pytorch/examples/blob/master/mnist/main.py)很大程度上启发。）\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\" Distributed Synchronous SGD Example \"\"\"\n",
    "    def run(rank, size):\n",
    "        torch.manual_seed(1234)\n",
    "        train_set, bsz = partition_dataset()\n",
    "        model = Net()\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                              lr=0.01, momentum=0.5)\n",
    "    \n",
    "        num_batches = ceil(len(train_set.dataset) / float(bsz))\n",
    "        for epoch in range(10):\n",
    "            epoch_loss = 0.0\n",
    "            for data, target in train_set:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.nll_loss(output, target)\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                average_gradients(model)\n",
    "                optimizer.step()\n",
    "            print('Rank ', dist.get_rank(), ', epoch ',\n",
    "                  epoch, ': ', epoch_loss / num_batches)\n",
    "    \n",
    "    \n",
    "\n",
    "它仍然实现`average_gradients（型号） `功能，它只是发生在一个模型，在整个世界平均水平的梯度。\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\" Gradient averaging. \"\"\"\n",
    "    def average_gradients(model):\n",
    "        size = float(dist.get_world_size())\n",
    "        for param in model.parameters():\n",
    "            dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
    "            param.grad.data /= size\n",
    "    \n",
    "    \n",
    "\n",
    "_的Et瞧_ ！我们成功地实施分布式同步新元，并可能培养了大量的计算机集群上的任何模型。\n",
    "\n",
    "**注：[HTG1虽然最后一句是 _技术上_\n",
    "真实的，有[很多更多的技巧[HTG5】实行同步SGD的生产级的落实需要。再次，用什么](https://seba-1511.github.io/dist_blog)[已经过测试和优化[HTG7。](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)**\n",
    "\n",
    "### 我们自己的戒指，Allreduce\n",
    "\n",
    "作为一个额外的挑战，假设我们要落实DeepSpeech的高效环allreduce。这是使用点至点集体相当容易实现。\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\" Implementation of a ring-reduce with addition. \"\"\"\n",
    "    def allreduce(send, recv):\n",
    "        rank = dist.get_rank()\n",
    "        size = dist.get_world_size()\n",
    "        send_buff = th.zeros(send.size())\n",
    "        recv_buff = th.zeros(send.size())\n",
    "        accum = th.zeros(send.size())\n",
    "        accum[:] = send[:]\n",
    "    \n",
    "        left = ((rank - 1) + size) % size\n",
    "        right = (rank + 1) % size\n",
    "    \n",
    "        for i in range(size - 1):\n",
    "            if i % 2 == 0:\n",
    "                # Send send_buff\n",
    "                send_req = dist.isend(send_buff, right)\n",
    "                dist.recv(recv_buff, left)\n",
    "                accum[:] += recv[:]\n",
    "            else:\n",
    "                # Send recv_buff\n",
    "                send_req = dist.isend(recv_buff, right)\n",
    "                dist.recv(send_buff, left)\n",
    "                accum[:] += send[:]\n",
    "            send_req.wait()\n",
    "        recv[:] = accum[:]\n",
    "    \n",
    "    \n",
    "\n",
    "另外，在上述脚本中，`allreduce（发送， 的recv） `函数具有比PyTorch的那些稍微不同的签名。它需要一个`的recv\n",
    "`张量，将所有`发 `张量的总和存储在里面。作为一个练习留给读者，还有我们的版本和一个在DeepSpeech之间的一个区别：它们的实现划分梯度张成 _块_\n",
    "，从而以最佳方式利用通信带宽。 （提示：[ torch.chunk\n",
    "](https://pytorch.org/docs/stable/torch.html#torch.chunk)）\n",
    "\n",
    "## 高级主题\n",
    "\n",
    "我们现在就可以发现一些`torch.distributed`更先进的功能性。因为有很多覆盖，本节分为两个小节：\n",
    "\n",
    "  1. 通讯后端：我们学习如何使用MPI和GLOO的GPU-GPU通信。\n",
    "  2. 初始化方法：在我们了解如何最好地设置在`dist.init_process_group初始协调阶段（） [HTG3。`\n",
    "\n",
    "### 通信后端\n",
    "\n",
    "其中的`最优雅的方面torch.distributed\n",
    "`是它的抽象能力和建立在不同的后端之上。正如前面提到的，有目前有三个在后端实现PyTorch：TCP，MPI和GLOO。他们每个人都有不同的规格和权衡，根据所需的用例。支持的函数的比较表可以发现[这里](https://pytorch.org/docs/stable/distributed.html#module-\n",
    "torch.distributed)。需要注意的是第四后端，NCCL，已自创立本教程的补充。参见[本部分](https://pytorch.org/docs/stable/distributed.html#multi-\n",
    "gpu-collective-functions)中的`torch.distributed`文档有关其使用和值的详细信息的。\n",
    "\n",
    "**TCP后端**\n",
    "\n",
    "到目前为止，我们已经取得了TCP后端的广泛使用。这是作为一个开发平台非常方便，因为它是保证在大多数计算机和操作系统上运行。它还支持所有点至点和集体功能的CPU。然而，对于GPU和它的通信程序并不作为优化的MPI一个不支持。\n",
    "\n",
    "**GLOO后端**\n",
    "\n",
    "的[ GLOO后端](https://github.com/facebookincubator/gloo)提供了一种优化的实施 _集体_\n",
    "通信过程，无论对CPU和GPU。它特别照在GPU的，因为它可以在不使用[ GPUDirect\n",
    "](https://developer.nvidia.com/gpudirect)将数据传送到CPU的存储器进行通信。另外，也能够使用[ NCCL\n",
    "](https://github.com/NVIDIA/nccl)执行快速节点内的通信，并实现其[自己的算法[HTG9用于节点间的例程。](https://github.com/facebookincubator/gloo/blob/master/docs/algorithms.md)\n",
    "\n",
    "自从0.2.0版本中，GLOO后台自动包含PyTorch的预编译的二进制文件。正如你一定会注意到，如果你把`模型\n",
    "`在GPU上我们的分布式SGD例如不工作。让我们从第一替换`后端= 'GLOO' 修复 `在`init_processes（秩， 大小， FN，\n",
    "后端= 'TCP'） `。在这一点上，该脚本将仍然在CPU上运行，但使用的幕后GLOO后端。为了使用多GPU，让我们也做如下修改：\n",
    "\n",
    "  0. `init_processes（秩， 大小， FN， 后端= 'TCP'） `\\（\\ RIGHTARROW \\） `init_processes（秩， 大小， FN， 后端= 'GLOO'） `\n",
    "  1. 使用`装置 =  torch.device（ “CUDA：{}”。格式（评级）） `\n",
    "  2. `模型 =  净（） `\\（\\ RIGHTARROW \\） `模型 =  净（）。到（装置） `\n",
    "  3. 使用`数据， 目标 =  data.to（装置）， target.to（装置） `\n",
    "\n",
    "通过上述修改，我们的模型现在的训练在两个GPU和您可以监控他们与`利用观看 NVIDIA-SMI  [HTG5。`\n",
    "\n",
    "**MPI后端**\n",
    "\n",
    "消息传递接口（MPI）是从高性能计算领域标准化的工具。它允许做点至点和集体沟通，是为`torch.distributed\n",
    "`该API的主要灵感。存在MPI的若干实施方式（例如，[开放-MPI ](https://www.open-mpi.org/)，[ MVAPICH2\n",
    "](http://mvapich.cse.ohio-state.edu/)，[英特尔MPI ](https://software.intel.com/en-\n",
    "us/intel-mpi-library)），每个用于不同的目的进行了优化。使用MPI后端的优势在于MPI的广泛可用性 - 和优化的高层次 -\n",
    "大型计算机集群。 [HTG10一些 [最近](https://developer.nvidia.com/ibm-spectrum-mpi)\n",
    "[实现](https://www.open-mpi.org/)也能够利用CUDA IPC和GPU直接的技术，以便通过CPU来避免存储副本。\n",
    "\n",
    "不幸的是，PyTorch的可执行文件可以不包括MPI实现，我们必须手工重新编译。幸运的是，这个过程是相当简单的因为在编译时，PyTorch看起来 _本身_\n",
    "一个可用的MPI实现。下面的步骤安装MPI后端，通过从源安装PyTorch\n",
    "[。](https://github.com/pytorch/pytorch#from-source)\n",
    "\n",
    "  1. 创建并激活您的蟒蛇环境，安装所有下面的[导](https://github.com/pytorch/pytorch#from-source)的先决条件，但 **不是** 运行`巨蟒 setup.py  安装 `呢。\n",
    "  2. 选择并安装自己喜欢的MPI实现。请注意，启用CUDA感知MPI可能需要一些额外的步骤。在我们的例子中，我们将坚持开放MPI _无_ GPU的支持：`畅达 安装 -c  康达锻 的openmpi`\n",
    "  3. 现在，去你的克隆PyTorch回购和执行`巨蟒 setup.py  安装 [HTG7。`\n",
    "\n",
    "为了测试我们新安装的后端，则需要进行一些修改。\n",
    "\n",
    "  1. 更换下`含量如果 __name__  ==  '__main__'： `与`init_processes （0， 0， 运行， 后端= 'MPI'） `。\n",
    "  2. 运行`的mpirun  -N  4  蟒 myscript.py`。\n",
    "\n",
    "究其原因，这些变化是，MPI需要产卵的过程之前创建自己的环境。 MPI也将产生其自己的过程，并执行在初始化方法所述的握手，使得`秩 `和`大小\n",
    "`的参数`init_process_group`多余的。这实际上是相当强大的，你可以通过额外的参数`的mpirun\n",
    "[HTG17为了调整计算资源，为每个进程。 （比如像每个进程内核，手工分配机器特定列数和[一些更](https://www.open-\n",
    "mpi.org/faq/?category=running#mpirun-hostfile)）这样做，则应该得到相同的熟悉输出与其它通信后端。`\n",
    "\n",
    "### 初始化方法\n",
    "\n",
    "为了完成本教程，让我们来谈谈我们称为第一个函数：`dist.init_process_group（后端， init_method）HTG4]\n",
    "[HTG5。特别是，我们会在不同的初始化方法，这是负责每道工序之间的协调最初一步。这些方法允许你定义这种协调是如何实现的。根据您的硬件设置，这些方法之一应该是自然比其他人更适合。除了下面的部分，你也应该有一个看看[官方文档[HTG7。](https://pytorch.org/docs/stable/distributed.html#initialization)`\n",
    "\n",
    "跳水进入初始化方法之前，让我们快速浏览一下背后`init_process_group`从C / C ++的角度会发生什么。\n",
    "\n",
    "  1. 首先，参数解析和验证。\n",
    "  2. 后端经由`name2channel.at（） `功能解决。 A `频道 `类被返回，并且将用于进行该数据传输。\n",
    "  3. 的GIL被丢弃，并`THDProcessGroupInit（） `被调用。此实例化信道，并增加了主节点的地址。\n",
    "  4. 用列0的过程中会执行`主 `过程，而所有其他等级将是`工人 `。\n",
    "  5. 大师\n",
    "    1. 创建为所有工人插座。\n",
    "    2. 所有工人等待连接。\n",
    "    3. 发送他们有关的其他进程的位置信息。\n",
    "  6. 每个工人\n",
    "    1. 创建一个套接字的主人。\n",
    "    2. 将自己的位置信息。\n",
    "    3. 接收有关的其他工作人员的信息。\n",
    "    4. 打开一个插座和握手与所有其他工人。\n",
    "  7. 初始化完成后，每个人都被连接到每一个人。\n",
    "\n",
    "**环境变量**\n",
    "\n",
    "我们一直在使用本教程的环境变量初始化方法。通过设置所有计算机上的以下四个环境变量，所有进程将能够正确地连接到主，获取有关的其他进程的信息，并最终与他们握手。\n",
    "\n",
    "  * `MASTER_PORT`：将与等级0宿主的过程中机器上的空闲端口。\n",
    "  * `MASTER_ADDR`：将与等级0宿主的过程中机器的IP地址。\n",
    "  * `WORLD_SIZE`：总数的工艺，使主知道有多少工人等待。\n",
    "  * `RANK`：每个处理的等级，所以他们会知道它是否是一个工人的主人。\n",
    "\n",
    "**共享文件系统**\n",
    "\n",
    "共享文件系统需要的所有进程能够访问共享文件系统，并协调将通过共享文件。这意味着，每个进程将打开该文件，写入其信息，并等待，直到每个人都这样做了。以后有什么需要的所有信息将随时提供给所有的进程。为了避免竞态条件，则文件系统必须支持通过[的fcntl\n",
    "](http://man7.org/linux/man-\n",
    "pages/man2/fcntl.2.html)锁定。请注意，您可以手动指定行列或让流程弄清楚自己。可以定义一个独特的`组名\n",
    "`每次作业你可以使用相同的文件路径为多个作业，然后安全地避免冲突。\n",
    "\n",
    "    \n",
    "    \n",
    "    dist.init_process_group(init_method='file:///mnt/nfs/sharedfile', world_size=4,\n",
    "                            group_name='mygroup')\n",
    "    \n",
    "    \n",
    "\n",
    "**TCP初始化 &安培;组播**\n",
    "\n",
    "通过TCP初始化可以用两种不同的方式来实现：\n",
    "\n",
    "  1. 通过提供过程中的IP地址与等级0和世界大小。\n",
    "  2. 通过提供 _任何_ 有效的IP [多播地址](https://en.wikipedia.org/wiki/Multicast_address)和世界的大小。\n",
    "\n",
    "在第一种情况下，所有工人将能够与秩0连接至该过程，并按照上面描述的过程。\n",
    "\n",
    "    \n",
    "    \n",
    "    dist.init_process_group(init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4)\n",
    "    \n",
    "    \n",
    "\n",
    "在第二种情况下，多播地址指定组节点谁可能潜在地是活性和协调可以通过允许每个进程遵循上面的程序之前，有一个初始握手处理的。此外TCP组播初始化还支持`组名\n",
    "`参数（与共享文件的方法），从而允许多个作业要在同一群集中调度。\n",
    "\n",
    "    \n",
    "    \n",
    "    dist.init_process_group(init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456',\n",
    "                            world_size=4)\n",
    "    \n",
    "    \n",
    "\n",
    "**致谢**\n",
    "\n",
    "我想感谢PyTorch开发人员就其执行，文档和测试做这样一个好工作。当代码不清楚，我总能指望[文档](https://pytorch.org/docs/stable/distributed.html)或[测试](https://github.com/pytorch/pytorch/blob/master/test/test_distributed.py)找到答案。我特别要感谢Soumith\n",
    "Chintala，亚当Paszke，和Natalia Gimelshein提供有见地的意见和回答有关初稿的问题。\n",
    "\n",
    "[Next ![](../_static/images/chevron-right-\n",
    "orange.svg)](../beginner/aws_distributed_training_tutorial.html \"4.\n",
    "\\(advanced\\) PyTorch 1.0 Distributed Trainer with Amazon AWS\")\n",
    "[![](../_static/images/chevron-right-orange.svg) Previous](ddp_tutorial.html\n",
    "\"2. Getting Started with Distributed Data Parallel\")\n",
    "\n",
    "* * *\n",
    "\n",
    "Was this helpful?\n",
    "\n",
    "Yes\n",
    "\n",
    "No\n",
    "\n",
    "Thank you\n",
    "\n",
    "* * *\n",
    "\n",
    "©版权所有2017年，PyTorch。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * 3\\. PyTorch编写分布式应用\n",
    "    * 安装\n",
    "    * 点对点通讯\n",
    "    * 集群通信\n",
    "    * [HTG0分布式训练\n",
    "      * 我们自己的戒指，Allreduce \n",
    "    * 高级主题\n",
    "      * 通信后端\n",
    "      * 初始化方法\n",
    "\n",
    "![](https://www.facebook.com/tr?id=243028289693773&ev=PageView\n",
    "\n",
    "  &noscript=1)\n",
    "![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](https://www.facebook.com/pytorch) [](https://twitter.com/pytorch)\n",
    "\n",
    "分析流量和优化经验，我们为这个站点的Cookie。通过点击或导航，您同意我们的cookies的使用。因为这个网站目前维护者，Facebook的Cookie政策的适用。了解更多信息，包括有关可用的控制：[饼干政策[HTG1。](https://www.facebook.com/policies/cookies/)\n",
    "\n",
    "![](../_static/images/pytorch-x.svg)\n",
    "\n",
    "[](https://pytorch.org/)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
