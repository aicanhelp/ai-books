{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分布式数据并行(DDP)入门\n",
    "\n",
    "> **作者**：[Shen Li](https://mrshenli.github.io/)\n",
    ">\n",
    "> **译者**：[Hamish](https://sherlockbear.github.io)\n",
    ">\n",
    "> **校验**：[Hamish](https://sherlockbear.github.io)\n",
    "\n",
    "[DistributedDataParallel](https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html)(DDP)在模块级别实现数据并行性。它使用[torch.distributed](https://pytorch.org/tutorials/intermediate/dist_tuto.html)包中的通信集合体来同步梯度，参数和缓冲区。并行性在流程内和跨流程均可用。在一个过程中，DDP将输入模块复制到device_ids中指定的设备，相应地沿批处理维度分散输入，并将输出收集到output_device，这与[DataParallel](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)相似。在整个过程中，DDP在正向传递中插入必要的参数同步，在反向传递中插入梯度同步。用户可以将进程映射到可用资源，只要进程不共享GPU设备即可。推荐的方法（通常是最快的方法）是为每个模块副本创建一个过程，即在一个过程中不进行任何模块复制。本教程中的代码在8-GPU服务器上运行，但可以轻松地推广到其他环境。\n",
    "\n",
    "## `DataParallel`和`DistributedDataParallel`之间的比较\n",
    "\n",
    "在深入研究之前，让我们澄清一下为什么，尽管增加了复杂性，您还是会考虑使用`DistributedDataParallel`而不是`DataParallel`：\n",
    "\n",
    "- 首先，回想一下[之前的教程](1-model_parallel_tutorial.ipynb)，如果模型太大，无法被单个GPU容纳，则必须使用**模型并行化**将其拆分至多个GPU。`DistributedDataParallel`可以与**模型并行化**一起工作；`DataParallel`此时不工作。\n",
    "- `DataParallel`是单进程、多线程的，并且只在一台机器上工作；而`DistributedDataParallel`是多进程的，可用于单机和多机训练。因此，即使对于单机训练，数据足够小，可以放在一台机器上，`DistributedDataParallel`也会比`DataParallel`更快。`DistributedDataParallel`还可以预先复制模型，而不是在每次迭代时复制模型，从而可以避免全局解释器锁定。\n",
    "- 如果您的数据太大，无法在一台机器上容纳，**并且**您的模型也太大，无法在单个GPU上容纳，则可以将模型并行化（跨多个GPU拆分单个模型）与`DistributedDataParallel`结合起来。在这种机制下，每个`DistributedDataParallel`进程都可以使用模型并行化，同时所有进程都可以使用数据并行。\n",
    "\n",
    "## 基本用例\n",
    "\n",
    "要创建DDP模块，请首先正确设置进程组。更多细节可以在[使用PyTorch编写分布式应用程序](3-dist_tuto.ipynb)中找到。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function init_process_group in module torch.distributed.distributed_c10d:\n",
      "\n",
      "init_process_group(backend, init_method=None, timeout=datetime.timedelta(0, 1800), world_size=-1, rank=-1, store=None, group_name='')\n",
      "    Initializes the default distributed process group, and this will also\n",
      "    initialize the distributed package.\n",
      "    \n",
      "    There are 2 main ways to initialize a process group:\n",
      "        1. Specify ``store``, ``rank``, and ``world_size`` explicitly.\n",
      "        2. Specify ``init_method`` (a URL string) which indicates where/how\n",
      "           to discover peers. Optionally specify ``rank`` and ``world_size``,\n",
      "           or encode all required parameters in the URL and omit them.\n",
      "    \n",
      "    If neither is specified, ``init_method`` is assumed to be \"env://\".\n",
      "    \n",
      "    \n",
      "    Arguments:\n",
      "        backend (str or Backend): The backend to use. Depending on\n",
      "            build-time configurations, valid values include ``mpi``, ``gloo``,\n",
      "            and ``nccl``. This field should be given as a lowercase string\n",
      "            (e.g., ``\"gloo\"``), which can also be accessed via\n",
      "            :class:`Backend` attributes (e.g., ``Backend.GLOO``). If using\n",
      "            multiple processes per machine with ``nccl`` backend, each process\n",
      "            must have exclusive access to every GPU it uses, as sharing GPUs\n",
      "            between processes can result in deadlocks.\n",
      "        init_method (str, optional): URL specifying how to initialize the\n",
      "                                     process group. Default is \"env://\" if no\n",
      "                                     ``init_method`` or ``store`` is specified.\n",
      "                                     Mutually exclusive with ``store``.\n",
      "        world_size (int, optional): Number of processes participating in\n",
      "                                    the job. Required if ``store`` is specified.\n",
      "        rank (int, optional): Rank of the current process.\n",
      "                              Required if ``store`` is specified.\n",
      "        store(Store, optional): Key/value store accessible to all workers, used\n",
      "                                to exchange connection/address information.\n",
      "                                Mutually exclusive with ``init_method``.\n",
      "        timeout (timedelta, optional): Timeout for operations executed against\n",
      "            the process group. Default value equals 30 minutes.\n",
      "            This is applicable for the ``gloo`` backend. For ``nccl``, this is\n",
      "            applicable only if the environment variable ``NCCL_BLOCKING_WAIT``\n",
      "            is set to 1.\n",
      "        group_name (str, optional, deprecated): Group name.\n",
      "    \n",
      "    To enable ``backend == Backend.MPI``, PyTorch needs to built from source\n",
      "    on a system that supports MPI. The same applies to NCCL as well.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "    # Explicitly setting seed to make sure that models created in two processes\n",
    "    # start from same random weights and biases.\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "help(dist.init_process_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们创建一个玩具模块，用DDP包装它，并用一些虚拟输入数据给它输入。请注意，如果训练是从随机参数开始的，您可能需要确保所有DDP进程使用相同的初始值。否则，全局梯度同步将没有意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n",
    "    # rank 2 uses GPUs [4, 5, 6, 7].\n",
    "    n = torch.cuda.device_count() // world_size\n",
    "    device_ids = list(range(rank * n, (rank + 1) * n))\n",
    "\n",
    "    # create model and move it to device_ids[0]\n",
    "    model = ToyModel().to(device_ids[0])\n",
    "    # output_device defaults to device_ids[0]\n",
    "    ddp_model = DDP(model, device_ids=device_ids)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(device_ids[0])\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如您所见，DDP包装了较低级别的分布式通信细节，并提供了一个干净的API，就好像它是一个本地模型一样。对于基本用例，DDP只需要几个loc来设置流程组。在将DDP应用于更高级的用例时，需要注意一些注意事项。\n",
    "\n",
    "## 不均衡的处理速度\n",
    "\n",
    "在DDP中，构造函数、前向方法和输出的微分是分布式同步点。不同的进程将以相同的顺序到达同步点，并在大致相同的时间进入每个同步点。否则，快速进程可能会提前到达，并在等待散乱的进程时超时。因此，用户需要负责跨进程平衡工作负载的分配。有时，由于网络延迟、资源竞争、不可预测的工作量高峰，不均衡的处理速度是不可避免的。要避免在这些情况下超时，请确保在调用[init_process_group](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)时传递足够大的`timeout`值。\n",
    "\n",
    "## 保存和载入检查点\n",
    "\n",
    "在训练过程中，经常使用`torch.save`和`torch.load`为模块创建检查点，以及从检查点恢复。有关的详细信息，请参见[保存和加载模型](https://pytorch.org/tutorials/beginner/saving_loading_models.html)。在使用DDP时，一种优化方法是只在一个进程中保存模型，然后将其加载到所有进程中，从而减少写开销。这是正确的，因为所有进程都是从相同的参数开始的，并且梯度在反向过程中是同步的，因此优化器应该将参数设置为相同的值。如果使用这种优化方法，请确保在保存完成之前，所有进程都不会开始加载。此外，加载模块时，需要提供适当的`map_location`参数，以防止进程进入其他设备。如果缺少`map_location`，`torch.load`将首先将模块加载到CPU，然后将每个参数复制到其保存的位置，这将导致同一台计算机上的所有进程使用同一组设备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_checkpoint(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # setup devices for this process, rank 1 uses GPUs [0, 1, 2, 3] and\n",
    "    # rank 2 uses GPUs [4, 5, 6, 7].\n",
    "    n = torch.cuda.device_count() // world_size\n",
    "    device_ids = list(range(rank * n, (rank + 1) * n))\n",
    "\n",
    "    model = ToyModel().to(device_ids[0])\n",
    "    # output_device defaults to device_ids[0]\n",
    "    ddp_model = DDP(model, device_ids=device_ids)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n",
    "    if rank == 0:\n",
    "        # All processes should see same parameters as they all start from same\n",
    "        # random parameters and gradients are synchronized in backward passes.\n",
    "        # Therefore, saving it in one process is sufficient.\n",
    "        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n",
    "\n",
    "    # Use a barrier() to make sure that process 1 loads the model after process\n",
    "    # 0 saves it.\n",
    "    dist.barrier()\n",
    "    # configure map_location properly\n",
    "    rank0_devices = [x - rank * len(device_ids) for x in device_ids]\n",
    "    device_pairs = zip(rank0_devices, device_ids)\n",
    "    map_location = {'cuda:%d' % x: 'cuda:%d' % y for x, y in device_pairs}\n",
    "    ddp_model.load_state_dict(\n",
    "        torch.load(CHECKPOINT_PATH, map_location=map_location))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(device_ids[0])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Use a barrier() to make sure that all processes have finished reading the\n",
    "    # checkpoint\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank == 0:\n",
    "        os.remove(CHECKPOINT_PATH)\n",
    "\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合DDP与模型并行化\n",
    "\n",
    "DDP也适用于多GPU模型，但不支持进程内的复制。您需要为每个模块副本创建一个进程，这通常会比每个进程创建多个副本带来更好的性能。当使用大量数据训练大型模型时，DDP包装多GPU模型尤其有用。使用此功能时，需要小心地实现多GPU模型，以避免硬编码设备，因为不同的模型副本将被放置到不同的设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyMpModel(nn.Module):\n",
    "    def __init__(self, dev0, dev1):\n",
    "        super(ToyMpModel, self).__init__()\n",
    "        self.dev0 = dev0\n",
    "        self.dev1 = dev1\n",
    "        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.dev0)\n",
    "        x = self.relu(self.net1(x))\n",
    "        x = x.to(self.dev1)\n",
    "        return self.net2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将多GPU模型传递给DDP时，**不能**设置`device_ids`和`output_device`。输入和输出数据将由应用程序或模型`forward()`方法放置在适当的设备中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_model_parallel(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # setup mp_model and devices for this process\n",
    "    dev0 = rank * 2\n",
    "    dev1 = rank * 2 + 1\n",
    "    mp_model = ToyMpModel(dev0, dev1)\n",
    "    ddp_mp_model = DDP(mp_model)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # outputs will be on dev1\n",
    "    outputs = ddp_mp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(dev1)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo(demo_basic, 2)\n",
    "    run_demo(demo_checkpoint, 2)\n",
    "\n",
    "    if torch.cuda.device_count() >= 8:\n",
    "        run_demo(demo_model_parallel, 4)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
