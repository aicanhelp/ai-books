{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调 TorchVision 模型\n",
    "\n",
    "> **作者**：[Nathan Inkawhich](https://github.com/inkawhich)\n",
    "> \n",
    "> 译者：[片刻](https://github.com/jiangzhonglian)\n",
    "> \n",
    "> 校验：[片刻](https://github.com/jiangzhonglian)\n",
    "\n",
    "在本教程中，我们将更深入地研究如何微调和特征提取[Torchvision模型](https://pytorch.org/docs/stable/torchvision/models.html)，所有这些模型都已在1000类Imagenet数据集上进行了预训练。本教程将深入研究如何使用几种现代的CNN架构，并将建立一种直观的方法来微调任何PyTorch模型。由于每种模型的架构都不同，因此没有适用于所有场景的样板微调代码。相反，研究人员必须查看现有的体系结构，并对每个模型进行自定义调整。\n",
    "\n",
    "在本文档中，我们将执行两种类型的迁移学习：微调和特征提取。在**微调**中，我们从预先训练的模型开始，并为新任务更新模型的所有参数，实质上是对整个模型进行重新训练。在特征提取中，我们从预先训练的模型开始，仅更新最终的层权重，从中得出预测值。之所以称为特征提取，是因为我们将预训练的CNN用作固定的特征提取器，并且仅更改输出层。有关转学的更多技术信息，请参见[此处](https://cs231n.github.io/transfer-learning/)和[此处](https://ruder.io/transfer-learning/)。\n",
    "\n",
    "通常，两种转移学习方法都遵循相同的几个步骤：\n",
    "\n",
    "* 初始化预训练模型\n",
    "* 重塑最终图层，使其输出数量与新数据集中的类数相同\n",
    "* 为优化算法定义我们要在训练期间更新哪些参数\n",
    "* 运行训练步骤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out:\n",
    "    \n",
    "    PyTorch Version:  1.2.0\n",
    "    Torchvision Version:  0.4.0\n",
    "    \n",
    "\n",
    "## 输入\n",
    "\n",
    "这是要更改运行的所有参数。我们将使用可以在[此处下载](https://download.pytorch.org/tutorial/hymenoptera_data.zip)的hymenoptera_data数据集 。该数据集包含**bees**和**ants**两类，其结构使得我们可以使用 [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder) 数据集，而不必编写自己的自定义数据集。下载数据并将`data_dir`输入设置为数据集的根目录。输入的`model_name`是您要使用的模型的名称，必须从以下列表中进行选择：\n",
    "    \n",
    "    [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "\n",
    "其他输入如下：`num_classes`是数据集中的类数，`batch_size`是用于训练的批次大小，可以根据您计算机的能力进行调整，`num_epochs`是我们要运行的训练时期的数量，以及`feature_extract`是一个布尔值，它定义了我们是微调还是特征提取。如果`feature_extract = False`，则微调模型并更新所有模型参数。 如果`feature_extract = True`，则仅更新最后一层参数，其他参数保持固定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Top level data directory. Here we assume the format of the directory conforms\n",
    "    #   to the ImageFolder structure\n",
    "    data_dir = \"./data/hymenoptera_data\"\n",
    "    \n",
    "    # Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "    model_name = \"squeezenet\"\n",
    "    \n",
    "    # Number of classes in the dataset\n",
    "    num_classes = 2\n",
    "    \n",
    "    # Batch size for training (change depending on how much memory you have)\n",
    "    batch_size = 8\n",
    "    \n",
    "    # Number of epochs to train for\n",
    "    num_epochs = 15\n",
    "    \n",
    "    # Flag for feature extracting. When False, we finetune the whole model,\n",
    "    #   when True we only update the reshaped layer params\n",
    "    feature_extract = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数\n",
    "\n",
    "在编写用于调整模型的代码之前，让我们定义一些辅助函数。\n",
    "\n",
    "### 模型训练和验证码\n",
    "\n",
    "`train_model`函数处理给定模型的训练和验证。作为输入，它采用PyTorch模型，数据加载器字典，损失函数，优化器，要训练和验证的指定时期数以及当模型是Inception模型时的布尔标志。 `is_inception`标志用于适应Inception v3模型，因为该体系结构使用辅助输出，并且总体模型损失同时考虑了辅助输出和最终输出，如[此处](https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958)所述。 该函数针对指定的时期数进行训练，并且在每个时期之后运行完整的验证步骤。 它还跟踪最佳模型（在验证准确性方面），并且在训练结束时返回最佳模型。 在每个时期之后，将打印训练和验证准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "        since = time.time()\n",
    "    \n",
    "        val_acc_history = []\n",
    "    \n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "    \n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "    \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "    \n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "    \n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "    \n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        # Get model outputs and calculate loss\n",
    "                        # Special case for inception because in training it has an auxiliary output. In train\n",
    "                        #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                        #   but in testing we only consider the final output.\n",
    "                        if is_inception and phase == 'train':\n",
    "                            # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                            outputs, aux_outputs = model(inputs)\n",
    "                            loss1 = criterion(outputs, labels)\n",
    "                            loss2 = criterion(aux_outputs, labels)\n",
    "                            loss = loss1 + 0.4*loss2\n",
    "                        else:\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "    \n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "    \n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "                epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "                epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "    \n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "    \n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                if phase == 'val':\n",
    "                    val_acc_history.append(epoch_acc)\n",
    "    \n",
    "            print()\n",
    "    \n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "        # load best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        return model, val_acc_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置模型参数`.requires_grad`属性\n",
    "\n",
    "当我们进行特征提取时，此辅助函数将模型中参数的`.requires_grad`属性设置为`False`。默认情况下，当我们加载预训练的模型时，所有参数都具有`.requires_grad = True`，如果我们从头开始或进行微调训练，这很好。但是，如果我们要进行特征提取，并且只想为新初始化的图层计算梯度，那么我们希望所有其他参数都不需要梯度。稍后将更有意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def set_parameter_requires_grad(model, feature_extracting):\n",
    "        if feature_extracting:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化和重塑网络\n",
    "\n",
    "现在到最有趣的部分。我们在这里处理每个网络的重塑。注意，这不是自动过程，并且对于每个型号都是唯一的。回想一下，CNN模型的最后一层（通常是FC层的倍数）具有与数据集中的输出类数相同的节点数。由于所有模型都已在Imagenet上进行了预训练，因此它们都具有大小为1000的输出层，每个类一个节点。这里的目标是重塑最后一层，使其具有与以前相同的输入数量，并且具有与数据集中的类数相同的输出数量。在以下各节中，我们将讨论如何分别更改每个模型的体系结构。但是首先，有一个关于微调和特征提取之间差异的重要细节。\n",
    "\n",
    "特征提取时，我们只想更新最后一层的参数，换句话说，我们只想更新我们要重塑的层的参数。因此，我们不需要计算不变的参数的梯度，因此为了提高效率，我们将`.requires_grad`属性设置为`False`。这很重要，因为默认情况下，此属性设置为`True`。然后，当我们初始化新图层时，默认情况下，新参数的值为`.requires_grad = True`，因此仅新图层的参数将被更新。当我们进行微调时，我们可以将所有`.required_grad`的设置保留为默认值`True`。\n",
    "\n",
    "最后，请注意 inception_v3 要求输入大小为(299,299)，而所有其他模型都期望为(224,224)。\n",
    "\n",
    "### Resnet\n",
    "\n",
    "Resnet在[用于图像识别的深度残差学习](https://arxiv.org/abs/1512.03385)中进行了介绍。 有几种不同大小的变体，包括Resnet18，Resnet34，Resnet50，Resnet101和Resnet152，所有这些都可以从Torchvision模型中获得。 这里我们使用Resnet18，因为我们的数据集很小，只有两个类。 当我们打印模型时，我们看到最后一层是完全连接的层，如下所示：\n",
    "    \n",
    "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    "    \n",
    "\n",
    "因此，我们必须将`model.fc`重新初始化为具有512个输入要素和2个输出要素的线性层，其具有："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.fc = nn.Linear(512, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet\n",
    "\n",
    "Alexnet在[《使用深度卷积神经网络的ImageNet分类》](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)一书中进行了介绍，并且是ImageNet数据集上第一个非常成功的CNN。 当我们打印模型架构时，我们看到模型输出来自分类器的第六层\n",
    "    \n",
    "    (classifier): Sequential(\n",
    "        ...\n",
    "        (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "     )\n",
    "\n",
    "为了将模型与我们的数据集一起使用，我们将该层重新初始化为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    model.classifier[6] = nn.Linear(4096,num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG\n",
    "\n",
    "VGG在[用于大型图像识别的甚深度卷积网络](https://arxiv.org/pdf/1409.1556.pdf)中被介绍。TorchVision提供了八种不同长度的VGG版本，有些具有批归一化层。在这里，我们将VGG-11与批处理归一化一起使用。 输出层类似于Alexnet，即\n",
    "    \n",
    "    (classifier): Sequential(\n",
    "        ...\n",
    "        (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "     )\n",
    "\n",
    "因此，我们使用相同的技术来修改输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.classifier[6] = nn.Linear(4096,num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeezenet\n",
    "\n",
    "论文SqueezeNet中描述了Squeeznet体系结构：[AlexNet级别的精度，参数减少了50倍，模型尺寸小于0.5MB](https://arxiv.org/abs/1602.07360)，并且使用的输出结构与此处显示的任何其他模型都不相同。 TorchVision有两个版本的Squeezenet，我们使用1.0版。输出来自1x1卷积层，这是分类器的第一层：\n",
    "    \n",
    "    (classifier): Sequential(\n",
    "        (0): Dropout(p=0.5)\n",
    "        (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
    "        (2): ReLU(inplace)\n",
    "        (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
    "     )\n",
    "    \n",
    "为了修改网络，我们将Conv2d层重新初始化为深度为2的输出特征图为 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densenet\n",
    "\n",
    "Densenet在[《密集连接卷积网络》](https://arxiv.org/abs/1608.06993)一文中进行了介绍。 TorchVision有Densenet的四个变体，但这里我们仅使用Densenet-121。输出层是具有1024个输入要素的线性层：\n",
    "    \n",
    "    (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
    "    \n",
    "为了重塑网络，我们将分类器的线性层重新初始化为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    model.classifier = nn.Linear(1024, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception V3\n",
    "\n",
    "最后，在[重新思考计算机视觉的初始架构](https://arxiv.org/pdf/1512.00567v1.pdf)中首次描述了Inception v3。该网络是唯一的，因为在训练时它具有两个输出层。第二个输出称为辅助输出，包含在网络的AuxLogits部分中。 主要输出是网络末端的线性层。注意，在测试时，我们仅考虑主要输出。 加载模型的辅助输出和主要输出打印为：\n",
    "    \n",
    "    (AuxLogits): InceptionAux(\n",
    "        ...\n",
    "        (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
    "     )\n",
    "     ...\n",
    "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
    "\n",
    "要微调此模型，我们必须重塑这两层的形状。这可以通过以下步骤完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
    "    model.fc = nn.Linear(2048, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，许多模型具有相似的输出结构，但是每个模型的处理方式都必须略有不同。另外，请检查重塑网络的打印模型架构，并确保输出要素的数量与数据集中的类的数量相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "        # Initialize these variables which will be set in this if statement. Each of these\n",
    "        #   variables is model specific.\n",
    "        model_ft = None\n",
    "        input_size = 0\n",
    "    \n",
    "        if model_name == \"resnet\":\n",
    "            \"\"\" Resnet18\n",
    "            \"\"\"\n",
    "            model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "            set_parameter_requires_grad(model_ft, feature_extract)\n",
    "            num_ftrs = model_ft.fc.in_features\n",
    "            model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "            input_size = 224\n",
    "    \n",
    "        elif model_name == \"alexnet\":\n",
    "            \"\"\" Alexnet\n",
    "            \"\"\"\n",
    "            model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "            set_parameter_requires_grad(model_ft, feature_extract)\n",
    "            num_ftrs = model_ft.classifier[6].in_features\n",
    "            model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "            input_size = 224\n",
    "    \n",
    "        elif model_name == \"vgg\":\n",
    "            \"\"\" VGG11_bn\n",
    "            \"\"\"\n",
    "            model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "            set_parameter_requires_grad(model_ft, feature_extract)\n",
    "            num_ftrs = model_ft.classifier[6].in_features\n",
    "            model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "            input_size = 224\n",
    "    \n",
    "        elif model_name == \"squeezenet\":\n",
    "            \"\"\" Squeezenet\n",
    "            \"\"\"\n",
    "            model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "            set_parameter_requires_grad(model_ft, feature_extract)\n",
    "            model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "            model_ft.num_classes = num_classes\n",
    "            input_size = 224\n",
    "    \n",
    "        elif model_name == \"densenet\":\n",
    "            \"\"\" Densenet\n",
    "            \"\"\"\n",
    "            model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "            set_parameter_requires_grad(model_ft, feature_extract)\n",
    "            num_ftrs = model_ft.classifier.in_features\n",
    "            model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "            input_size = 224\n",
    "    \n",
    "        elif model_name == \"inception\":\n",
    "            \"\"\" Inception v3\n",
    "            Be careful, expects (299,299) sized images and has auxiliary output\n",
    "            \"\"\"\n",
    "            model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "            set_parameter_requires_grad(model_ft, feature_extract)\n",
    "            # Handle the auxilary net\n",
    "            num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "            model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "            # Handle the primary net\n",
    "            num_ftrs = model_ft.fc.in_features\n",
    "            model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "            input_size = 299\n",
    "    \n",
    "        else:\n",
    "            print(\"Invalid model name, exiting...\")\n",
    "            exit()\n",
    "    \n",
    "        return model_ft, input_size\n",
    "    \n",
    "    # Initialize the model for this run\n",
    "    model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "    \n",
    "    # Print the model we just instantiated\n",
    "    print(model_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out:\n",
    "\n",
    "    SqueezeNet(\n",
    "      (features): Sequential(\n",
    "        (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
    "        (1): ReLU(inplace=True)\n",
    "        (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
    "        (3): Fire(\n",
    "          (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (squeeze_activation): ReLU(inplace=True)\n",
    "          (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (expand1x1_activation): ReLU(inplace=True)\n",
    "          (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (expand3x3_activation): ReLU(inplace=True)\n",
    "        )\n",
    "        (4): Fire(\n",
    "          (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (squeeze_activation): ReLU(inplace=True)\n",
    "          (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (expand1x1_activation): ReLU(inplace=True)\n",
    "          (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (expand3x3_activation): ReLU(inplace=True)\n",
    "        )\n",
    "        (5): Fire(\n",
    "          (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (squeeze_activation): ReLU(inplace=True)\n",
    "          (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (expand1x1_activation): ReLU(inplace=True)\n",
    "          (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (expand3x3_activation): ReLU(inplace=True)\n",
    "        )\n",
    "        (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
    "        (7): Fire(\n",
    "          (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (squeeze_activation): ReLU(inplace=True)\n",
    "          (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (expand1x1_activation): ReLU(inplace=True)\n",
    "          (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (expand3x3_activation): ReLU(inplace=True)\n",
    "        )\n",
    "        (8): Fire(\n",
    "          (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (squeeze_activation): ReLU(inplace=True)\n",
    "          (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (expand1x1_activation): ReLU(inplace=True)\n",
    "          (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (expand3x3_activation): ReLU(inplace=True)\n",
    "        )\n",
    "        (9): Fire(\n",
    "          (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (squeeze_activation): ReLU(inplace=True)\n",
    "          (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (expand1x1_activation): ReLU(inplace=True)\n",
    "          (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (expand3x3_activation): ReLU(inplace=True)\n",
    "        )\n",
    "        (10): Fire(\n",
    "          (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (squeeze_activation): ReLU(inplace=True)\n",
    "          (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (expand1x1_activation): ReLU(inplace=True)\n",
    "          (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (expand3x3_activation): ReLU(inplace=True)\n",
    "        )\n",
    "        (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
    "        (12): Fire(\n",
    "          (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (squeeze_activation): ReLU(inplace=True)\n",
    "          (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (expand1x1_activation): ReLU(inplace=True)\n",
    "          (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (expand3x3_activation): ReLU(inplace=True)\n",
    "        )\n",
    "      )\n",
    "      (classifier): Sequential(\n",
    "        (0): Dropout(p=0.5, inplace=False)\n",
    "        (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "        (2): ReLU(inplace=True)\n",
    "        (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "      )\n",
    "    )\n",
    "    \n",
    "\n",
    "## 加载数据\n",
    "\n",
    "既然我们知道输入大小必须为多少，就可以初始化数据转换，图像数据集和数据加载器。请注意，模型已经过硬编码规范化值的预训练，[如下所述](https://pytorch.org/docs/master/torchvision/models.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    # Data augmentation and normalization for training\n",
    "    # Just normalization for validation\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    \n",
    "    # Create training and validation datasets\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "    # Create training and validation dataloaders\n",
    "    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "    \n",
    "    # Detect if we have a GPU available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out:\n",
    "    \n",
    "    Initializing Datasets and Dataloaders...\n",
    "\n",
    "## 创建优化器\n",
    "\n",
    "既然模型结构正确，那么微调和特征提取的最后一步就是创建一个仅更新所需参数的优化器。回想一下，在加载了预训练的模型之后，但是在重塑之前，如果`feature_extract = True`，我们将所有参数的`.requires_grad`属性手动设置为`False`。然后，默认情况下，重新初始化的图层的参数为.requires_grad = True。 因此，现在我们知道应该优化所有具有`.requires_grad = True`的参数。接下来，我们列出此类参数，并将此列表输入SGD算法构造函数。\n",
    "\n",
    "要验证这一点，请查看打印的参数以进行学习。 进行微调时，此列表应该很长，并且包括所有模型参数。 但是，在提取特征时，此列表应简短，并且仅包括重塑图层的权重和偏差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    # Send the model to GPU\n",
    "    model_ft = model_ft.to(device)\n",
    "    \n",
    "    # Gather the parameters to be optimized/updated in this run. If we are\n",
    "    #  finetuning we will be updating all parameters. However, if we are\n",
    "    #  doing feature extract method, we will only update the parameters\n",
    "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "    #  is True.\n",
    "    params_to_update = model_ft.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "    \n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out:\n",
    "\n",
    "    Params to learn:\n",
    "             classifier.1.weight\n",
    "             classifier.1.bias\n",
    "\n",
    "## 运行训练和验证步骤\n",
    "\n",
    "最后，最后一步是为模型设置损失，然后针对设定的时期数运行训练和验证功能。注意，根据时期数，此步骤在CPU上可能需要一段时间。同样，默认学习率并非对所有模型都最佳，因此要获得最大的准确性，有必要分别针对每个模型进行调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out:\n",
    "\n",
    "    Epoch 0/14\n",
    "    ----------\n",
    "    train Loss: 0.5200 Acc: 0.7336\n",
    "    val Loss: 0.3895 Acc: 0.8366\n",
    "    \n",
    "    Epoch 1/14\n",
    "    ----------\n",
    "    train Loss: 0.3361 Acc: 0.8566\n",
    "    val Loss: 0.3015 Acc: 0.8954\n",
    "    \n",
    "    Epoch 2/14\n",
    "    ----------\n",
    "    train Loss: 0.2721 Acc: 0.8770\n",
    "    val Loss: 0.2938 Acc: 0.8954\n",
    "    \n",
    "    Epoch 3/14\n",
    "    ----------\n",
    "    train Loss: 0.2776 Acc: 0.8770\n",
    "    val Loss: 0.2774 Acc: 0.9150\n",
    "    \n",
    "    Epoch 4/14\n",
    "    ----------\n",
    "    train Loss: 0.1881 Acc: 0.9139\n",
    "    val Loss: 0.2715 Acc: 0.9150\n",
    "    \n",
    "    Epoch 5/14\n",
    "    ----------\n",
    "    train Loss: 0.1561 Acc: 0.9467\n",
    "    val Loss: 0.3201 Acc: 0.9150\n",
    "    \n",
    "    Epoch 6/14\n",
    "    ----------\n",
    "    train Loss: 0.2536 Acc: 0.9016\n",
    "    val Loss: 0.3474 Acc: 0.9150\n",
    "    \n",
    "    Epoch 7/14\n",
    "    ----------\n",
    "    train Loss: 0.1781 Acc: 0.9303\n",
    "    val Loss: 0.3262 Acc: 0.9150\n",
    "    \n",
    "    Epoch 8/14\n",
    "    ----------\n",
    "    train Loss: 0.2321 Acc: 0.8811\n",
    "    val Loss: 0.3197 Acc: 0.8889\n",
    "    \n",
    "    Epoch 9/14\n",
    "    ----------\n",
    "    train Loss: 0.1616 Acc: 0.9344\n",
    "    val Loss: 0.3161 Acc: 0.9346\n",
    "    \n",
    "    Epoch 10/14\n",
    "    ----------\n",
    "    train Loss: 0.1510 Acc: 0.9262\n",
    "    val Loss: 0.3199 Acc: 0.9216\n",
    "    \n",
    "    Epoch 11/14\n",
    "    ----------\n",
    "    train Loss: 0.1485 Acc: 0.9385\n",
    "    val Loss: 0.3198 Acc: 0.9216\n",
    "    \n",
    "    Epoch 12/14\n",
    "    ----------\n",
    "    train Loss: 0.1098 Acc: 0.9590\n",
    "    val Loss: 0.3331 Acc: 0.9281\n",
    "    \n",
    "    Epoch 13/14\n",
    "    ----------\n",
    "    train Loss: 0.1449 Acc: 0.9385\n",
    "    val Loss: 0.3556 Acc: 0.9281\n",
    "    \n",
    "    Epoch 14/14\n",
    "    ----------\n",
    "    train Loss: 0.1405 Acc: 0.9303\n",
    "    val Loss: 0.4227 Acc: 0.8758\n",
    "    \n",
    "    Training complete in 0m 20s\n",
    "    Best val Acc: 0.934641\n",
    "\n",
    "## 与从头开始训练的模型比较\n",
    "\n",
    "只是为了好玩，让我们看看如果我们不使用转移学习，该模型将如何学习。 微调与特征提取的性能在很大程度上取决于数据集，但与从头开始训练的模型相比，总体而言，两种转移学习方法在训练时间和总体准确性方面均产生良好的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    # Initialize the non-pretrained version of the model used for this run\n",
    "    scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "    scratch_model = scratch_model.to(device)\n",
    "    scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "    scratch_criterion = nn.CrossEntropyLoss()\n",
    "    _,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "    \n",
    "    # Plot the training curves of validation accuracy vs. number\n",
    "    #  of training epochs for the transfer learning method and\n",
    "    #  the model trained from scratch\n",
    "    ohist = []\n",
    "    shist = []\n",
    "    \n",
    "    ohist = [h.cpu().numpy() for h in hist]\n",
    "    shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "    \n",
    "    plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "    plt.xlabel(\"Training Epochs\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "    plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "    plt.ylim((0,1.))\n",
    "    plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://pytorch.org/tutorials/_images/sphx_glr_finetuning_torchvision_models_tutorial_001.png](https://pytorch.org/tutorials/_images/sphx_glr_finetuning_torchvision_models_tutorial_001.png)\n",
    "\n",
    "Out:\n",
    "\n",
    "    Epoch 0/14\n",
    "    ----------\n",
    "    train Loss: 0.7032 Acc: 0.5205\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 1/14\n",
    "    ----------\n",
    "    train Loss: 0.6931 Acc: 0.5000\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 2/14\n",
    "    ----------\n",
    "    train Loss: 0.6931 Acc: 0.4549\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 3/14\n",
    "    ----------\n",
    "    train Loss: 0.6931 Acc: 0.5041\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 4/14\n",
    "    ----------\n",
    "    train Loss: 0.6931 Acc: 0.5041\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 5/14\n",
    "    ----------\n",
    "    train Loss: 0.6931 Acc: 0.5656\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 6/14\n",
    "    ----------\n",
    "    train Loss: 0.6931 Acc: 0.4467\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 7/14\n",
    "    ----------\n",
    "    train Loss: 0.6932 Acc: 0.5123\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 8/14\n",
    "    ----------\n",
    "    train Loss: 0.6931 Acc: 0.4918\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 9/14\n",
    "    ----------\n",
    "    train Loss: 0.6931 Acc: 0.4754\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 10/14\n",
    "    ----------\n",
    "    train Loss: 0.6931 Acc: 0.4795\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 11/14\n",
    "    ----------\n",
    "    train Loss: 0.6931 Acc: 0.5205\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 12/14\n",
    "    ----------\n",
    "    train Loss: 0.6931 Acc: 0.4754\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 13/14\n",
    "    ----------\n",
    "    train Loss: 0.6932 Acc: 0.4590\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Epoch 14/14\n",
    "    ----------\n",
    "    train Loss: 0.6932 Acc: 0.5082\n",
    "    val Loss: 0.6931 Acc: 0.4641\n",
    "    \n",
    "    Training complete in 0m 29s\n",
    "    Best val Acc: 0.464052\n",
    "    \n",
    "\n",
    "## 最后的思考和下一步是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试运行其他一些模型，看看精度如何。另外，请注意，特征提取花费的时间更少，因为在向后传递中，我们不必计算大多数梯度。 这里有很多地方。 你可以：\n",
    "\n",
    "* 使用更困难的数据集运行此代码，并查看迁移学习的更多好处\n",
    "* 使用此处描述的方法，使用转移学习来更新不同的模型，也许是在新的领域（例如NLP，音频等）\n",
    "* 对模型满意后，可以将其导出为ONNX模型，也可以使用混合前端对其进行跟踪以提高速度和优化机会。\n",
    "\n",
    "**脚本的总运行时间**：（0分钟57.562秒）"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
