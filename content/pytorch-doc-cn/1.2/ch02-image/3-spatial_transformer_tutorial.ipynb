{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 空间转换器网络教程\n",
    "\n",
    "> **作者**：[Ghassen HAMROUNI](https://github.com/GHamrouni)\n",
    "> \n",
    "> 译者：[片刻](https://github.com/jiangzhonglian)\n",
    "> \n",
    "> 校验：[片刻](https://github.com/jiangzhonglian)\n",
    "\n",
    "![https://pytorch.org/tutorials/_images/FSeq.png](https://pytorch.org/tutorials/_images/FSeq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本教程中，您将学习如何使用称为空间变换器网络的视觉注意力机制来扩充网络。 您可以在[DeepMind论文](https://arxiv.org/abs/1506.02025)中阅读有关空间变换器网络的更多信息。\n",
    "\n",
    "空间变换器网络是对任何空间变换的可区别关注的概括。空间变换器网络（简称STN）允许神经网络学习如何对输入图像执行空间变换，以增强模型的几何不变性。例如，它可以裁剪感兴趣的区域，缩放并校正图像的方向。这可能是一个有用的机制，因为CNN不会对旋转和缩放以及更一般的仿射变换保持不变。\n",
    "\n",
    "关于STN的最好的事情之一就是能够将其简单地插入到任何现有的CNN中，而无需进行任何修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # License: BSD\n",
    "    # Author: Ghassen Hamrouni\n",
    "    \n",
    "    from __future__ import print_function\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    import torchvision\n",
    "    from torchvision import datasets, transforms\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    plt.ion()   # interactive mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据\n",
    "\n",
    "在本文中，我们将尝试使用经典的MNIST数据集。使用标准卷积网络和空间变换器网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Training dataset\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(root='.', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])), batch_size=64, shuffle=True, num_workers=4)\n",
    "    # Test dataset\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])), batch_size=64, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out:\n",
    "    \n",
    "    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n",
    "    Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
    "    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n",
    "    Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
    "    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n",
    "    Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
    "    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
    "    Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
    "    Processing...\n",
    "    Done!\n",
    "    \n",
    "\n",
    "## 描绘空间变换器网络\n",
    "\n",
    "空间转换器网络可归结为三个主要组成部分：\n",
    "\n",
    "* 定位网络是常规的CNN，可以对转换参数进行回归。永远不会从该数据集中显式学习变换，而是网络会自动学习增强全局精度的空间变换。\n",
    "* 网格生成器在输入图像中生成与来自输出图像的每个像素相对应的坐标网格。\n",
    "* 采样器使用转换的参数，并将其应用于输入图像。\n",
    "\n",
    "![https://pytorch.org/tutorials/_images/stn-arch.png](https://pytorch.org/tutorials/_images/stn-arch.png)\n",
    "\n",
    "> Note\n",
    "> 我们需要包含affine_grid和grid_sample模块的最新版本的PyTorch。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "            self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "            self.conv2_drop = nn.Dropout2d()\n",
    "            self.fc1 = nn.Linear(320, 50)\n",
    "            self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "            # Spatial transformer localization-network\n",
    "            self.localization = nn.Sequential(\n",
    "                nn.Conv2d(1, 8, kernel_size=7),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(8, 10, kernel_size=5),\n",
    "                nn.MaxPool2d(2, stride=2),\n",
    "                nn.ReLU(True)\n",
    "            )\n",
    "    \n",
    "            # Regressor for the 3 * 2 affine matrix\n",
    "            self.fc_loc = nn.Sequential(\n",
    "                nn.Linear(10 * 3 * 3, 32),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(32, 3 * 2)\n",
    "            )\n",
    "    \n",
    "            # Initialize the weights/bias with identity transformation\n",
    "            self.fc_loc[2].weight.data.zero_()\n",
    "            self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "    \n",
    "        # Spatial transformer network forward function\n",
    "        def stn(self, x):\n",
    "            xs = self.localization(x)\n",
    "            xs = xs.view(-1, 10 * 3 * 3)\n",
    "            theta = self.fc_loc(xs)\n",
    "            theta = theta.view(-1, 2, 3)\n",
    "    \n",
    "            grid = F.affine_grid(theta, x.size())\n",
    "            x = F.grid_sample(x, grid)\n",
    "    \n",
    "            return x\n",
    "    \n",
    "        def forward(self, x):\n",
    "            # transform the input\n",
    "            x = self.stn(x)\n",
    "    \n",
    "            # Perform the usual forward pass\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "            x = x.view(-1, 320)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    \n",
    "    model = Net().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模式\n",
    "\n",
    "现在，让我们使用SGD算法训练模型。网络正在以监督方式学习分类任务。同时，该模型以端到端的方式自动学习STN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    \n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 500 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "    #\n",
    "    # A simple test procedure to measure STN the performances on MNIST.\n",
    "    #\n",
    "    \n",
    "    \n",
    "    def test():\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "    \n",
    "                # sum up batch loss\n",
    "                test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "                # get the index of the max log-probability\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "                  .format(test_loss, correct, len(test_loader.dataset),\n",
    "                          100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化STN结果\n",
    "\n",
    "现在，我们将检查学习到的视觉注意力机制的结果。 \n",
    "我们定义了一个小的辅助函数，以便在训练时可视化转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    def convert_image_np(inp):\n",
    "        \"\"\"Convert a Tensor to numpy image.\"\"\"\n",
    "        inp = inp.numpy().transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        inp = std * inp + mean\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "        return inp\n",
    "    \n",
    "    # We want to visualize the output of the spatial transformers layer\n",
    "    # after the training, we visualize a batch of input images and\n",
    "    # the corresponding transformed batch using STN.\n",
    "    \n",
    "    \n",
    "    def visualize_stn():\n",
    "        with torch.no_grad():\n",
    "            # Get a batch of training data\n",
    "            data = next(iter(test_loader))[0].to(device)\n",
    "    \n",
    "            input_tensor = data.cpu()\n",
    "            transformed_input_tensor = model.stn(data).cpu()\n",
    "    \n",
    "            in_grid = convert_image_np(\n",
    "                torchvision.utils.make_grid(input_tensor))\n",
    "    \n",
    "            out_grid = convert_image_np(\n",
    "                torchvision.utils.make_grid(transformed_input_tensor))\n",
    "    \n",
    "            # Plot the results side-by-side\n",
    "            f, axarr = plt.subplots(1, 2)\n",
    "            axarr[0].imshow(in_grid)\n",
    "            axarr[0].set_title('Dataset Images')\n",
    "    \n",
    "            axarr[1].imshow(out_grid)\n",
    "            axarr[1].set_title('Transformed Images')\n",
    "    \n",
    "    for epoch in range(1, 20 + 1):\n",
    "        train(epoch)\n",
    "        test()\n",
    "    \n",
    "    # Visualize the STN transformation on some input batch\n",
    "    visualize_stn()\n",
    "    \n",
    "    plt.ioff()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://pytorch.org/tutorials/_images/sphx_glr_spatial_transformer_tutorial_001.png](https://pytorch.org/tutorials/_images/sphx_glr_spatial_transformer_tutorial_001.png)\n",
    "\n",
    "Out:\n",
    "    \n",
    "    Train Epoch: 1 [0/60000 (0%)]   Loss: 2.290877\n",
    "    Train Epoch: 1 [32000/60000 (53%)]      Loss: 0.910913\n",
    "    \n",
    "    Test set: Average loss: 0.2449, Accuracy: 9312/10000 (93%)\n",
    "    \n",
    "    Train Epoch: 2 [0/60000 (0%)]   Loss: 0.489534\n",
    "    Train Epoch: 2 [32000/60000 (53%)]      Loss: 0.296471\n",
    "    \n",
    "    Test set: Average loss: 0.1443, Accuracy: 9563/10000 (96%)\n",
    "    \n",
    "    Train Epoch: 3 [0/60000 (0%)]   Loss: 0.410248\n",
    "    Train Epoch: 3 [32000/60000 (53%)]      Loss: 0.355454\n",
    "    \n",
    "    Test set: Average loss: 0.1019, Accuracy: 9687/10000 (97%)\n",
    "    \n",
    "    Train Epoch: 4 [0/60000 (0%)]   Loss: 0.217658\n",
    "    Train Epoch: 4 [32000/60000 (53%)]      Loss: 0.185522\n",
    "    \n",
    "    Test set: Average loss: 0.0818, Accuracy: 9751/10000 (98%)\n",
    "    \n",
    "    Train Epoch: 5 [0/60000 (0%)]   Loss: 0.471464\n",
    "    Train Epoch: 5 [32000/60000 (53%)]      Loss: 0.591574\n",
    "    \n",
    "    Test set: Average loss: 0.0770, Accuracy: 9760/10000 (98%)\n",
    "    \n",
    "    Train Epoch: 6 [0/60000 (0%)]   Loss: 0.119462\n",
    "    Train Epoch: 6 [32000/60000 (53%)]      Loss: 0.093015\n",
    "    \n",
    "    Test set: Average loss: 0.0817, Accuracy: 9744/10000 (97%)\n",
    "    \n",
    "    Train Epoch: 7 [0/60000 (0%)]   Loss: 0.074523\n",
    "    Train Epoch: 7 [32000/60000 (53%)]      Loss: 0.414406\n",
    "    \n",
    "    Test set: Average loss: 0.0944, Accuracy: 9714/10000 (97%)\n",
    "    \n",
    "    Train Epoch: 8 [0/60000 (0%)]   Loss: 0.100317\n",
    "    Train Epoch: 8 [32000/60000 (53%)]      Loss: 0.114539\n",
    "    \n",
    "    Test set: Average loss: 0.1519, Accuracy: 9510/10000 (95%)\n",
    "    \n",
    "    Train Epoch: 9 [0/60000 (0%)]   Loss: 0.205053\n",
    "    Train Epoch: 9 [32000/60000 (53%)]      Loss: 0.135724\n",
    "    \n",
    "    Test set: Average loss: 0.0892, Accuracy: 9749/10000 (97%)\n",
    "    \n",
    "    Train Epoch: 10 [0/60000 (0%)]  Loss: 0.213368\n",
    "    Train Epoch: 10 [32000/60000 (53%)]     Loss: 0.208627\n",
    "    \n",
    "    Test set: Average loss: 0.0634, Accuracy: 9813/10000 (98%)\n",
    "    \n",
    "    Train Epoch: 11 [0/60000 (0%)]  Loss: 0.078725\n",
    "    Train Epoch: 11 [32000/60000 (53%)]     Loss: 0.099131\n",
    "    \n",
    "    Test set: Average loss: 0.0580, Accuracy: 9834/10000 (98%)\n",
    "    \n",
    "    Train Epoch: 12 [0/60000 (0%)]  Loss: 0.133572\n",
    "    Train Epoch: 12 [32000/60000 (53%)]     Loss: 0.213358\n",
    "    \n",
    "    Test set: Average loss: 0.0506, Accuracy: 9854/10000 (99%)\n",
    "    \n",
    "    Train Epoch: 13 [0/60000 (0%)]  Loss: 0.289802\n",
    "    Train Epoch: 13 [32000/60000 (53%)]     Loss: 0.165571\n",
    "    \n",
    "    Test set: Average loss: 0.0542, Accuracy: 9842/10000 (98%)\n",
    "    \n",
    "    Train Epoch: 14 [0/60000 (0%)]  Loss: 0.219281\n",
    "    Train Epoch: 14 [32000/60000 (53%)]     Loss: 0.284233\n",
    "    \n",
    "    Test set: Average loss: 0.0505, Accuracy: 9856/10000 (99%)\n",
    "    \n",
    "    Train Epoch: 15 [0/60000 (0%)]  Loss: 0.218599\n",
    "    Train Epoch: 15 [32000/60000 (53%)]     Loss: 0.055698\n",
    "    \n",
    "    Test set: Average loss: 0.0507, Accuracy: 9848/10000 (98%)\n",
    "    \n",
    "    Train Epoch: 16 [0/60000 (0%)]  Loss: 0.048718\n",
    "    Train Epoch: 16 [32000/60000 (53%)]     Loss: 0.093410\n",
    "    \n",
    "    Test set: Average loss: 0.0502, Accuracy: 9855/10000 (99%)\n",
    "    \n",
    "    Train Epoch: 17 [0/60000 (0%)]  Loss: 0.071185\n",
    "    Train Epoch: 17 [32000/60000 (53%)]     Loss: 0.053381\n",
    "    \n",
    "    Test set: Average loss: 0.0587, Accuracy: 9829/10000 (98%)\n",
    "    \n",
    "    Train Epoch: 18 [0/60000 (0%)]  Loss: 0.127790\n",
    "    Train Epoch: 18 [32000/60000 (53%)]     Loss: 0.169319\n",
    "    \n",
    "    Test set: Average loss: 0.0484, Accuracy: 9863/10000 (99%)\n",
    "    \n",
    "    Train Epoch: 19 [0/60000 (0%)]  Loss: 0.224094\n",
    "    Train Epoch: 19 [32000/60000 (53%)]     Loss: 0.175750\n",
    "    \n",
    "    Test set: Average loss: 0.0628, Accuracy: 9817/10000 (98%)\n",
    "    \n",
    "    Train Epoch: 20 [0/60000 (0%)]  Loss: 0.251131\n",
    "    Train Epoch: 20 [32000/60000 (53%)]     Loss: 0.024119\n",
    "    \n",
    "    Test set: Average loss: 0.0445, Accuracy: 9869/10000 (99%)\n",
    "    \n",
    "\n",
    "**脚本的总运行时间：** （1分钟44.448秒）\n",
    "\n",
    "[`Download Python source code:\n",
    "spatial_transformer_tutorial.py`](../_downloads/8aa31a122008b8db8bbe28365db9ea47/spatial_transformer_tutorial.py)\n",
    "\n",
    "[`Download Jupyter notebook:\n",
    "spatial_transformer_tutorial.ipynb`](../_downloads/b0786fd6ca28ee4ff3f2aa27080cdf18/spatial_transformer_tutorial.ipynb)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
