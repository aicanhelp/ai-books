{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.sparse\n",
    "\n",
    "> 译者：[hijkzzz](https://github.com/hijkzzz)\n",
    "\n",
    "警告\n",
    "\n",
    "这个API目前还处于试验阶段, 可能在不久的将来会发生变化. \n",
    "\n",
    "Torch支持COO(rdinate )格式的稀疏张量, 这可以有效地存储和处理大多数元素为零的张量. \n",
    "\n",
    "稀疏张量表示为一对稠密张量:一个值张量和一个二维指标张量. 一个稀疏张量可以通过提供这两个张量, 以及稀疏张量的大小来构造(从这些张量是无法推导出来的!)假设我们要定义一个稀疏张量, 它的分量3在(0,2)处, 分量4在(1,0)处, 分量5在(1,2)处, 然后我们可以这样写\n",
    "\n",
    "```py\n",
    ">>> i = torch.LongTensor([[0, 1, 1],\n",
    " [2, 0, 2]])\n",
    ">>> v = torch.FloatTensor([3, 4, 5])\n",
    ">>> torch.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense()\n",
    " 0  0  3\n",
    " 4  0  5\n",
    "[torch.FloatTensor of size 2x3]\n",
    "\n",
    "```\n",
    "\n",
    "注意, LongTensor的输入不是索引元组的列表. 如果你想这样写你的指标, 你应该在把它们传递给稀疏构造函数之前进行转置:\n",
    "\n",
    "```py\n",
    ">>> i = torch.LongTensor([[0, 2], [1, 0], [1, 2]])\n",
    ">>> v = torch.FloatTensor([3,      4,      5    ])\n",
    ">>> torch.sparse.FloatTensor(i.t(), v, torch.Size([2,3])).to_dense()\n",
    " 0  0  3\n",
    " 4  0  5\n",
    "[torch.FloatTensor of size 2x3]\n",
    "\n",
    "```\n",
    "\n",
    "也可以构造混合稀疏张量, 其中只有前n个维度是稀疏的, 其余维度是密集的. \n",
    "\n",
    "```py\n",
    ">>> i = torch.LongTensor([[2, 4]])\n",
    ">>> v = torch.FloatTensor([[1, 3], [5, 7]])\n",
    ">>> torch.sparse.FloatTensor(i, v).to_dense()\n",
    " 0  0\n",
    " 0  0\n",
    " 1  3\n",
    " 0  0\n",
    " 5  7\n",
    "[torch.FloatTensor of size 5x2]\n",
    "\n",
    "```\n",
    "\n",
    "可以通过指定其大小来构造空的稀疏张量：\n",
    "\n",
    "```py\n",
    ">>> torch.sparse.FloatTensor(2, 3)\n",
    "SparseFloatTensor of size 2x3 with indices:\n",
    "[torch.LongTensor with no dimension]\n",
    "and values:\n",
    "[torch.FloatTensor with no dimension]\n",
    "\n",
    "```\n",
    "\n",
    "```py\n",
    "SparseTensor 具有以下不变量:\n",
    "```\n",
    "\n",
    "1.  sparse_dim + dense_dim = len(SparseTensor.shape)\n",
    "2.  SparseTensor._indices().shape = (sparse_dim, nnz)\n",
    "3.  SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])\n",
    "\n",
    "因为SparseTensor._indices()总是一个二维张量, 最小的sparse_dim = 1. 因此, sparse_dim = 0的稀疏张量的表示就是一个稠密张量. \n",
    "\n",
    "注意\n",
    "\n",
    "我们的稀疏张量格式允许_uncoalesced(未合并)_ 的稀疏张量, 其中索引中可能有重复的坐标;在这种情况下, 解释是索引处的值是所有重复值项的和. _uncoalesced_ 张量允许我们更有效地实现某些运算符. \n",
    "\n",
    "在大多数情况下, 你不需要关心一个稀疏张量是否coalesced(合并), 因为大多数操作在给出一个coalesced或uncoalesced稀疏张量的情况下都是一样的. 然而, 有两种情况您可能需要注意. \n",
    "\n",
    "第一, 如果您重复执行可以产生重复项的操作 (例如, [`torch.sparse.FloatTensor.add()`](#torch.sparse.FloatTensor.add \"torch.sparse.FloatTensor.add\")), 你应该偶尔将稀疏张量coalesced一起, 以防止它们变得太大.\n",
    "\n",
    "第二, 一些运算符将根据它们是否coalesced产生不同的值 (例如, [`torch.sparse.FloatTensor._values()`](#torch.sparse.FloatTensor._values \"torch.sparse.FloatTensor._values\") and [`torch.sparse.FloatTensor._indices()`](#torch.sparse.FloatTensor._indices \"torch.sparse.FloatTensor._indices\"), 以及 [`torch.Tensor.sparse_mask()`](tensors.html#torch.Tensor.sparse_mask \"torch.Tensor.sparse_mask\")). 这些操作符以下划线作为前缀, 表示它们揭示了内部实现细节, 应该小心使用, 因为使用合并稀疏张量的代码可能无法使用未合并稀疏张量;一般来说, 在使用这些操作符之前显式地合并是最安全的. \n",
    "\n",
    "例如, 假设我们想通过直接操作[`torch.sparse.FloatTensor._values()`](#torch.sparse.FloatTensor._values \"torch.sparse.FloatTensor._values\").来实现一个操作符.标量乘法可以用很明显的方法实现, 因为乘法分布于加法之上;但是, 平方根不能直接实现, 因为`sqrt(a + b) != sqrt(a) + sqrt(b)`(如果给定一个uncoalesced的张量, 就会计算出这个结果). \n",
    "\n",
    "```py\n",
    "class torch.sparse.FloatTensor\n",
    "```\n",
    "\n",
    "```py\n",
    "add()\n",
    "```\n",
    "\n",
    "```py\n",
    "add_()\n",
    "```\n",
    "\n",
    "```py\n",
    "clone()\n",
    "```\n",
    "\n",
    "```py\n",
    "dim()\n",
    "```\n",
    "\n",
    "```py\n",
    "div()\n",
    "```\n",
    "\n",
    "```py\n",
    "div_()\n",
    "```\n",
    "\n",
    "```py\n",
    "get_device()\n",
    "```\n",
    "\n",
    "```py\n",
    "hspmm()\n",
    "```\n",
    "\n",
    "```py\n",
    "mm()\n",
    "```\n",
    "\n",
    "```py\n",
    "mul()\n",
    "```\n",
    "\n",
    "```py\n",
    "mul_()\n",
    "```\n",
    "\n",
    "```py\n",
    "narrow_copy()\n",
    "```\n",
    "\n",
    "```py\n",
    "resizeAs_()\n",
    "```\n",
    "\n",
    "```py\n",
    "size()\n",
    "```\n",
    "\n",
    "```py\n",
    "spadd()\n",
    "```\n",
    "\n",
    "```py\n",
    "spmm()\n",
    "```\n",
    "\n",
    "```py\n",
    "sspaddmm()\n",
    "```\n",
    "\n",
    "```py\n",
    "sspmm()\n",
    "```\n",
    "\n",
    "```py\n",
    "sub()\n",
    "```\n",
    "\n",
    "```py\n",
    "sub_()\n",
    "```\n",
    "\n",
    "```py\n",
    "t_()\n",
    "```\n",
    "\n",
    "```py\n",
    "toDense()\n",
    "```\n",
    "\n",
    "```py\n",
    "transpose()\n",
    "```\n",
    "\n",
    "```py\n",
    "transpose_()\n",
    "```\n",
    "\n",
    "```py\n",
    "zero_()\n",
    "```\n",
    "\n",
    "```py\n",
    "coalesce()\n",
    "```\n",
    "\n",
    "```py\n",
    "is_coalesced()\n",
    "```\n",
    "\n",
    "```py\n",
    "_indices()\n",
    "```\n",
    "\n",
    "```py\n",
    "_values()\n",
    "```\n",
    "\n",
    "```py\n",
    "_nnz()\n",
    "```\n",
    "\n",
    "## 函数\n",
    "\n",
    "```py\n",
    "torch.sparse.addmm(mat, mat1, mat2, beta=1, alpha=1)\n",
    "```\n",
    "\n",
    "这个函数和 [`torch.addmm()`](torch.html#torch.addmm \"torch.addmm\") 在`forward`中做同样的事情, 除了它支持稀疏矩阵`mat1` 的 `backward`. `mat1`应具有 `sparse_dim = 2`.  请注意, `mat1`的梯度是一个合并的稀疏张量.\n",
    "\n",
    "参数: \n",
    "\n",
    "*   **mat** ([_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\")) – 被相加的稠密矩阵\n",
    "*   **mat1** (_SparseTensor_) – 被相乘的稀疏矩阵\n",
    "*   **mat2** ([_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\")) – 被相乘的稠密矩阵\n",
    "*   **beta** (_Number__,_ _optional_) – 乘数 `mat` (![](img/50705df736e9a7919e768cf8c4e4f794.jpg))\n",
    "*   **alpha** (_Number__,_ _optional_) – 乘数 ![](img/c4fda0ec33ee23096c7bac6105f7a619.jpg) (![](img/82005cc2e0087e2a52c7e43df4a19a00.jpg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "torch.sparse.mm(mat1, mat2)\n",
    "```\n",
    "\n",
    "执行稀疏矩阵`mat1` 和 稠密矩阵 `mat2`的矩阵乘法. 类似于 [`torch.mm()`](torch.html#torch.mm \"torch.mm\"), 如果 `mat1` 是一个 ![](img/b2d82f601df5521e215e30962b942ad1.jpg) tensor, `mat2` 是一个 ![](img/ec84c2d649caa2a7d4dc59b6b23b0278.jpg) tensor, 输出将会是 ![](img/42cdcd96fd628658ac0e3e7070ba08d5.jpg) 稠密的 tensor. `mat1` 应具有 `sparse_dim = 2`. 此函数也支持两个矩阵的向后. 请注意, `mat1`的梯度是一个合并的稀疏张量\n",
    "\n",
    "参数: \n",
    "\n",
    "*   **mat1** (_SparseTensor_) – 第一个要相乘的稀疏矩阵\n",
    "*   **mat2** ([_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\")) – 第二个要相乘的稠密矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子:\n",
    "\n",
    "```py\n",
    ">>> a = torch.randn(2, 3).to_sparse().requires_grad_(True)\n",
    ">>> a\n",
    "tensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n",
    " [0, 1, 2, 0, 1, 2]]),\n",
    " values=tensor([ 1.5901,  0.0183, -0.6146,  1.8061, -0.0112,  0.6302]),\n",
    " size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)\n",
    "\n",
    ">>> b = torch.randn(3, 2, requires_grad=True)\n",
    ">>> b\n",
    "tensor([[-0.6479,  0.7874],\n",
    " [-1.2056,  0.5641],\n",
    " [-1.1716, -0.9923]], requires_grad=True)\n",
    "\n",
    ">>> y = torch.sparse.mm(a, b)\n",
    ">>> y\n",
    "tensor([[-0.3323,  1.8723],\n",
    " [-1.8951,  0.7904]], grad_fn=<SparseAddmmBackward>)\n",
    ">>> y.sum().backward()\n",
    ">>> a.grad\n",
    "tensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n",
    " [0, 1, 2, 0, 1, 2]]),\n",
    " values=tensor([ 0.1394, -0.6415, -2.1639,  0.1394, -0.6415, -2.1639]),\n",
    " size=(2, 3), nnz=6, layout=torch.sparse_coo)\n",
    "\n",
    "```\n",
    "\n",
    "```py\n",
    "torch.sparse.sum(input, dim=None, dtype=None)\n",
    "```\n",
    "\n",
    "返回给定维度`dim`中每行SparseTensor `input`的总和. 如果 :attr::`dim` 是一个维度的`list`, reduce将在全部给定维度进行.如果包括全部的 `sparse_dim`, 此方法将返回 Tensor 代替 SparseTensor.\n",
    "\n",
    "所有被求和的 `dim` 将被 squeezed (see [`torch.squeeze()`](torch.html#torch.squeeze \"torch.squeeze\")),导致速出 tensor 的 :attr::`dim` 小于 `input`.\n",
    "\n",
    "backward 过程中, 仅仅 `input` 的 `nnz` 位置被反向传播.  请注意, `input`的梯度是合并的. \n",
    "\n",
    "参数: \n",
    "\n",
    "*   **input** ([_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\")) – t输入 SparseTensor\n",
    "*   **dim** ([_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\") _or_ _tuple of python:ints_) – 维度或者维度列表. Default: 所有维度.\n",
    "*   **dtype** (`torch.dtype`, optional) – 返回 Tensor 的数据类型. 默认值: dtype 和 `input` 一致.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子:\n",
    "\n",
    "```py\n",
    ">>> nnz = 3\n",
    ">>> dims = [5, 5, 2, 3]\n",
    ">>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),\n",
    " torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n",
    ">>> V = torch.randn(nnz, dims[2], dims[3])\n",
    ">>> size = torch.Size(dims)\n",
    ">>> S = torch.sparse_coo_tensor(I, V, size)\n",
    ">>> S\n",
    "tensor(indices=tensor([[2, 0, 3],\n",
    " [2, 4, 1]]),\n",
    " values=tensor([[[-0.6438, -1.6467,  1.4004],\n",
    " [ 0.3411,  0.0918, -0.2312]],\n",
    "\n",
    " [[ 0.5348,  0.0634, -2.0494],\n",
    " [-0.7125, -1.0646,  2.1844]],\n",
    "\n",
    " [[ 0.1276,  0.1874, -0.6334],\n",
    " [-1.9682, -0.5340,  0.7483]]]),\n",
    " size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)\n",
    "\n",
    "# when sum over only part of sparse_dims, return a SparseTensor\n",
    ">>> torch.sparse.sum(S, [1, 3])\n",
    "tensor(indices=tensor([[0, 2, 3]]),\n",
    " values=tensor([[-1.4512,  0.4073],\n",
    " [-0.8901,  0.2017],\n",
    " [-0.3183, -1.7539]]),\n",
    " size=(5, 2), nnz=3, layout=torch.sparse_coo)\n",
    "\n",
    "# when sum over all sparse dim, return a dense Tensor\n",
    "# with summed dims squeezed\n",
    ">>> torch.sparse.sum(S, [0, 1, 3])\n",
    "tensor([-2.6596, -1.1450])\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
