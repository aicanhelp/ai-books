{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation package - torch.autograd\n",
    "\n",
    "> 译者：[gfjiangly](https://github.com/gfjiangly)\n",
    "\n",
    "`torch.autograd` 提供类和函数，实现任意标量值函数的自动微分。 它要求对已有代码的最小改变---你仅需要用`requires_grad=True`关键字为需要计算梯度的声明`Tensor`。\n",
    "\n",
    "```py\n",
    "torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)\n",
    "```\n",
    "\n",
    "计算被给张量关于图的叶节点的梯度和。\n",
    "\n",
    "图使用链式法则微分。如何任何`tensors`是非标量（例如他们的数据不止一个元素）并且要求梯度，函数要额外指出`grad_tensors`。它应是一个匹配长度的序列，包含可微函数关于相应张量的梯度（`None`是一个对所有张量可接受的值，不需要梯度张量）。\n",
    "\n",
    "此函数在叶节点累积梯度 - 你可能需要在调用前把它初始化为0.\n",
    "\n",
    "参数：\n",
    "\n",
    "*   **tensors** (_Tensor序列_) – 计算导数的张量。\n",
    "*   **grad_tensors** (_[_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\") _或_ [_None_](https://docs.python.org/3/library/constants.html#None \"(in Python v3.7)\")序列_) – 关于相应张量每个元素的梯度。标量张量或不需要梯度的可用None指定。如果None对所有grad_tensors可接受，则此参数可选。\n",
    "*   **retain_graph** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 如果False，用于计算梯度的图将被释放。请注意，在几乎所有情况下，不需要将此选项设置为真，而且通常可以更有效地解决问题。默认为create_graph值。\n",
    "*   **create_graph** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 如果True，则构造导数图，以便计算更高阶导数，默认False。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)\n",
    "```\n",
    "\n",
    "计算和返回输出关于输入的梯度和。\n",
    "\n",
    "`grad_outputs` 应是长度匹配输出的序列，包含关于输出每个元素的预计算梯度。如果一个输出不要求梯度，则梯度是`None`。\n",
    "\n",
    "如果`only_inputs`是`True`，此函数将仅返回关于指定输入的梯度list。如果此参数是`False`，则关于其余全部叶子的梯度仍被计算，并且将累加到`.grad`属性中。\n",
    "\n",
    "参数: \n",
    "\n",
    "*   **outputs** (_Tensor序列_) – 可微函数输出\n",
    "*   **inputs** (_Tensor序列_) – 关于将返回梯度的输入(不累加到`.grad`)。\n",
    "*   **grad_outputs** (_Tensor序列_) – 关于每个输入的梯度。标量张量或不需要梯度的可用None指定。如果None对所有grad_tensors可接受，则此参数可选。默认：`None`。\n",
    "*   **retain_graph** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 如果`False`，用于计算梯度的图将被释放。请注意，在几乎所有情况下，不需要将此选项设置为真，而且通常可以更有效地解决问题。默认为`create_graph`值。\n",
    "*   **create_graph** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 如果`True`，则构造导数图，以便计算更高阶导数，默认`False`。\n",
    "*   **allow_unused** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 如果`False`, 当计算输出出错时指明不使用的输入 (因此它们的梯度一直是0)。 默认`False`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 局部禁用梯度计算\n",
    "\n",
    "```py\n",
    "class torch.autograd.no_grad\n",
    "```\n",
    "\n",
    "禁用梯度计算的上下文管理器。\n",
    "\n",
    "当你确认不会调用 `Tensor.backward()`，对于推断禁用梯度计算是有用的。它将减少计算的内存消耗，否则会有`requires_grad=True`。在这个模式中，每个计算结果将导致`requires_grad=False`, 即便输入有`requires_grad=True`。\n",
    "\n",
    "函数还可作为装饰器。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    ">>> x = torch.tensor([1], requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...   y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    ">>> @torch.no_grad()\n",
    "... def doubler(x):\n",
    "...     return x * 2\n",
    ">>> z = doubler(x)\n",
    ">>> z.requires_grad\n",
    "False\n",
    "\n",
    "```\n",
    "\n",
    "```py\n",
    "class torch.autograd.enable_grad\n",
    "```\n",
    "\n",
    "使能梯度计算的上下文管理器。\n",
    "\n",
    "在一个[`no_grad`](#torch.autograd.no_grad \"torch.autograd.no_grad\")上下文中使能梯度计算。在[`no_grad`](#torch.autograd.no_grad \"torch.autograd.no_grad\")外部此上下文管理器无影响\n",
    "\n",
    "函数还可作为装饰器。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    ">>> x = torch.tensor([1], requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...   with torch.enable_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "True\n",
    ">>> y.backward()\n",
    ">>> x.grad\n",
    ">>> @torch.enable_grad()\n",
    "... def doubler(x):\n",
    "...     return x * 2\n",
    ">>> with torch.no_grad():\n",
    "...     z = doubler(x)\n",
    ">>> z.requires_grad\n",
    "True\n",
    "\n",
    "```\n",
    "\n",
    "```py\n",
    "class torch.autograd.set_grad_enabled(mode)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置梯度计算打开或关闭的上下文管理器。\n",
    "\n",
    "`set_grad_enabled`将基于它的参数`mode`使用或禁用梯度。它也能作为一个上下文管理器或函数使用。\n",
    "\n",
    "| 参数: | **mode** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")) – 标记是否使能梯度（True），或使能（False）。这能被用在有条件的使能梯度。\n",
    "| --- | --- |\n",
    "\n",
    "示例：\n",
    "\n",
    "```py\n",
    ">>> x = torch.tensor([1], requires_grad=True)\n",
    ">>> is_train = False\n",
    ">>> with torch.set_grad_enabled(is_train):\n",
    "...   y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    ">>> torch.set_grad_enabled(True)\n",
    ">>> y = x * 2\n",
    ">>> y.requires_grad\n",
    "True\n",
    ">>> torch.set_grad_enabled(False)\n",
    ">>> y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "\n",
    "```\n",
    "\n",
    "## 关于Tensors的原位操作\n",
    "\n",
    "在autograd中支持原位操作是一件很难的事，并且我们在大多数情况下不鼓励使用它们。Autograd积极的缓冲区释放和重用使其非常高效，实际上原位操作会大幅降低内存使用量的情况非常少。你可能永远不会使用它们，除非正在很大的内存压力下操作。\n",
    "\n",
    "### 就地正确性检查\n",
    "\n",
    "全部的Tensor保持追踪应用到它们身上的原位操作，并且如果实现检测到在任何一个函数中，一个tensor为反向传播保存，但是随后被原位修改，一旦反向传播开始将抛出一个错误。此设计确保如果你正在使用原位操作函数并且没有看到任何错误，你可以确保计算的梯度是正确的。\n",
    "\n",
    "### Variable (弃用)\n",
    "\n",
    "警告\n",
    "\n",
    "Variable API已经被弃用。对张量使用自动求导不再需要Variable。Autograd自动支持`requires_grad`参数设置成`True`的张量。以下是有关更改内容的快速指南：\n",
    "\n",
    "*   `Variable(tensor)` 和`Variable(tensor, requires_grad)`仍然和预期一样工作，但它们返回Tensors代替Variables。\n",
    "*   `var.data` 和 `tensor.data`是一回事。\n",
    "*   方法如`var.backward(), var.detach(), var.register_hook()`现在在tensors上使用相同的名字起作用。\n",
    "\n",
    "此外，现在可以使用诸如[`torch.randn()`](torch.html#torch.randn \"torch.randn\"), [`torch.zeros()`](torch.html#torch.zeros \"torch.zeros\"), [`torch.ones()`](torch.html#torch.ones \"torch.ones\")等工厂方法创建requires_grad=True的张量，如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)`\n",
    "\n",
    "## 张量自动求导函数\n",
    "\n",
    "```py\n",
    "class torch.Tensor\n",
    "```\n",
    "\n",
    "```py\n",
    "backward(gradient=None, retain_graph=None, create_graph=False)\n",
    "```\n",
    "\n",
    "计算当前张量关于图叶节点的梯度。\n",
    "\n",
    "图使用链式反则微分。如果张量是非标量并且要求梯度，函数额外要求指梯度。它应是一个匹配类型和位置的张量，含有可微函数关于它本身的梯度。\n",
    "\n",
    "此函数在叶节点累加梯度-你可能需要在调用前将它初始化为0。\n",
    "\n",
    "参数：\n",
    "\n",
    "*   **gradient** ([_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\") _或_ [_None_](https://docs.python.org/3/library/constants.html#None \"(in Python v3.7)\")) – 关于张量的梯度。如果它是一个张量，它将被自动转换成不要求梯度的张量，除非`create_graph`是`True`。标量张量或不需要梯度的可用`None`指定。如果None对所有grad_tensors可接受，则此参数可选。\n",
    "*   **retain_graph** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 如果`False`，用于计算梯度的图将被释放。请注意，在几乎所有情况下，不需要将此选项设置为真，而且通常可以更有效地解决问题。默认为`create_graph`值。\n",
    "*   **create_graph** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 如果`True`，则构造导数图，以便计算更高阶导数，默认`False`。\n",
    "\n",
    "```py\n",
    "detach()\n",
    "```\n",
    "\n",
    "返回一个新的Tensor，从当前图中分离出来。\n",
    "\n",
    "结果不要求梯度。\n",
    "\n",
    "注意\n",
    "\n",
    "返回的张量与原始张量使用相同的数据。关于它们中任一个原位修改将被看见，并且可能在正确性检查中触发错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "detach_()\n",
    "```\n",
    "\n",
    "从创建它的图中分离张量，使其成为叶。不能就地分离视图。\n",
    "\n",
    "```py\n",
    "grad\n",
    "```\n",
    "\n",
    "此属性默认`None`，并且调用[`backward()`](#torch.Tensor.backward \"torch.Tensor.backward\")计算自身梯度时第一时间成为一个Tensor。此属性将含计算的梯度，以后调用[`backward()`](#torch.Tensor.backward \"torch.Tensor.backward\")将累加提到到自身。\n",
    "\n",
    "```py\n",
    "is_leaf\n",
    "```\n",
    "\n",
    "按惯例，所有[`requires_grad`](#torch.Tensor.requires_grad \"torch.Tensor.requires_grad\")=False的张量将是叶节点张量\n",
    "\n",
    "如果张量是由用户创建，[`requires_grad`](#torch.Tensor.requires_grad \"torch.Tensor.requires_grad\")的张量也是叶节点张量。这意味着它们不是一个操作的结果，并且`grad_fn`是`None`。\n",
    "\n",
    "仅叶节点张量在调用[`backward()`](#torch.Tensor.backward \"torch.Tensor.backward\")时填充它们的[`grad`](#torch.Tensor.grad \"torch.Tensor.grad\")。为得到从非叶节点张量填充的梯度，你可以使用[`retain_grad()`](#torch.Tensor.retain_grad \"torch.Tensor.retain_grad\").\n",
    "\n",
    "示例：\n",
    "\n",
    "```py\n",
    ">>> a = torch.rand(10, requires_grad=True)\n",
    ">>> a.is_leaf\n",
    "True\n",
    ">>> b = torch.rand(10, requires_grad=True).cuda()\n",
    ">>> b.is_leaf\n",
    "False\n",
    "# b 是由cpu Tensor投入cuda Tensor的操作创建的\n",
    ">>> c = torch.rand(10, requires_grad=True) + 2\n",
    ">>> c.is_leaf\n",
    "False\n",
    "# c 是由加操作创建的\n",
    ">>> d = torch.rand(10).cuda()\n",
    ">>> d.is_leaf\n",
    "True\n",
    "# d 不要求梯度，所以没有创建它的操作 (被自动求导引擎追踪)\n",
    ">>> e = torch.rand(10).cuda().requires_grad_()\n",
    ">>> e.is_leaf\n",
    "True\n",
    "# e 要求梯度并且没有创建它的操作\n",
    ">>> f = torch.rand(10, requires_grad=True, device=\"cuda\")\n",
    ">>> f.is_leaf\n",
    "True\n",
    "# f 要求梯度并且没有创建它的操作\n",
    "\n",
    "```\n",
    "\n",
    "```py\n",
    "register_hook(hook)\n",
    "```\n",
    "\n",
    "注册一个反向钩子\n",
    "\n",
    "此钩子每次在对应张量梯度被计算时调用。此钩子应有下面鲜明特征：\n",
    "\n",
    "```py\n",
    "hook(grad) -> Tensor or None\n",
    "\n",
    "```\n",
    "\n",
    "此钩子不应该修改它的参数，但它能可选地返回一个新的用于替代 `grad`的梯度。\n",
    "\n",
    "此函数返回一个句柄，其句柄方法为`handle.remove()`，用于从模块中删除钩子。\n",
    "\n",
    "示例：\n",
    "\n",
    "```py\n",
    ">>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
    ">>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
    ">>> v.backward(torch.tensor([1., 2., 3.]))\n",
    ">>> v.grad\n",
    "\n",
    " 2\n",
    " 4\n",
    " 6\n",
    "[torch.FloatTensor of size (3,)]\n",
    "\n",
    ">>> h.remove()  # removes the hook\n",
    "\n",
    "```\n",
    "\n",
    "```py\n",
    "requires_grad\n",
    "```\n",
    "\n",
    "如果梯度需要为此张量计算则是`True`，否则为`False`\n",
    "\n",
    "注意\n",
    "\n",
    "事实是梯度需要为此张量计算不意味着`grad`属性将被填充，更多细节见[`is_leaf`](#torch.Tensor.is_leaf \"torch.Tensor.is_leaf\")。\n",
    "\n",
    "```py\n",
    "retain_grad()\n",
    "```\n",
    "\n",
    "为非叶节点张量使能`.grad`属性\n",
    "\n",
    "## Function\n",
    "\n",
    "```py\n",
    "class torch.autograd.Function\n",
    "```\n",
    "\n",
    "记录操作历史，定义可微操作公式。\n",
    "\n",
    "在Tensor上执行的每个操作都会创建一个新的函数对象，执行计算，记录它发生的。历史记录以函数的DAG形式保留，DAG的边表示数据的依赖性（`input &lt;- output`）。然后，当backward被调用，图按拓扑顺序被处理，通过调用每个[`Function`](#torch.autograd.Function \"torch.autograd.Function\")对象的backward()方法，并且传递梯度给下一个[`Function`](#torch.autograd.Function \"torch.autograd.Function\")。\n",
    "\n",
    "通常，用户与函数交互的唯一方式是通过创建子类和定义新操作。这是一种被推荐的扩展`torch.autograd`的方式。\n",
    "\n",
    "每个函数对象只能使用一次（在正向传递中）。\n",
    "\n",
    "示例：\n",
    "\n",
    "```py\n",
    ">>> class Exp(Function):\n",
    ">>>\n",
    ">>>     @staticmethod\n",
    ">>>     def forward(ctx, i):\n",
    ">>>         result = i.exp()\n",
    ">>>         ctx.save_for_backward(result)\n",
    ">>>         return result\n",
    ">>>\n",
    ">>>     @staticmethod\n",
    ">>>     def backward(ctx, grad_output):\n",
    ">>>         result, = ctx.saved_tensors\n",
    ">>>         return grad_output * result\n",
    "\n",
    "```\n",
    "\n",
    "```py\n",
    "static backward(ctx, *grad_outputs)\n",
    "```\n",
    "\n",
    "定义一个公式计算操作导数。\n",
    "\n",
    "此函数被所有子类重载。\n",
    "\n",
    "它必须接受一个上下文`ctx`作为第一个参数，随后是`forward()`返回的大量输出，并且它应返回尽可能多的张量，作为`forward()`函数输入。每个参数是关于被给输出的梯度，并且每个返回值是关于相应输入的梯度。\n",
    "\n",
    "`ctx`上下文可用于恢复保存在前向传播过程的梯度。它有一个`ctx.needs_input_grad`属性，作为一个代表每个输入是否需要梯度的布尔元组。\n",
    "\n",
    "```py\n",
    "static forward(ctx, *args, **kwargs)\n",
    "```\n",
    "\n",
    "执行操作。\n",
    "\n",
    "此函数被所有子类重载。\n",
    "\n",
    "它必须接受一个上下文`ctx`作为第一个参数，随后是任意数量的参数（tensor或其它类型）。\n",
    "\n",
    "此上下文可被用来存储张量，随后可在反向传播过程取出。\n",
    "\n",
    "## 数值梯度检查\n",
    "\n",
    "```py\n",
    "torch.autograd.gradcheck(func, inputs, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True)\n",
    "```\n",
    "\n",
    "通过小的有限差分与关于浮点类型且`requires_grad=True`的输入张量来检查计算的梯度。\n",
    "\n",
    "在数组梯度和分析梯度之间检查使用[`allclose()`](torch.html#torch.allclose \"torch.allclose\")。\n",
    "\n",
    "注意\n",
    "\n",
    "默认值为双精度输入设计。如果输入欠精度此检查有可能失败，例如，`FloatTensor`。\n",
    "\n",
    "警告\n",
    "\n",
    "如果在输入中任何被检查的张量有重叠的内存，换句话说，指向相同内存地址的不同切片（例如，从torch.expand()），此检查将有可能失败，因为在这个索引通过点扰动计算的数值梯度将改变在全部其它索引处共享内存地址的值。\n",
    "\n",
    "参数\n",
    "\n",
    "*   **func** (_function_) – 一个Python函数，输入是张量，返回一个张量或张量元组\n",
    "*   **inputs** (_张量元组_ _or_ [_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\")) – func函数输入\n",
    "*   **eps** ([_float_](https://docs.python.org/3/library/functions.html#float \"(in Python v3.7)\")_,_ _可选_) – 有限差分的扰动\n",
    "*   **atol** ([_float_](https://docs.python.org/3/library/functions.html#float \"(in Python v3.7)\")_,_ _可选_) – 绝对容差\n",
    "*   **rtol** ([_float_](https://docs.python.org/3/library/functions.html#float \"(in Python v3.7)\")_,_ _可选_) – 相对容差\n",
    "*   **raise_exception** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 指示如果检查失败是否抛出一个异常。此异常给出关于失败的确切性质的更多信息。这在梯度检查调试时是有用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 返回: | 如果所有差都满足全部闭合条件，则为True |\n",
    "| --- | --- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "torch.autograd.gradgradcheck(func, inputs, grad_outputs=None, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True)\n",
    "```\n",
    "\n",
    "通过小的有限差分与关于在输入中张量的分析梯度，检查已计算梯度的梯度，并且在requires_grad=True 情况下，grad_outputs是浮点类型。\n",
    "\n",
    "此函数检查通过计算到给定grad_outputs的梯度的反向传播是否正确。\n",
    "\n",
    "在数值梯度和分析梯度之间使用[`allclose()`](torch.html#torch.allclose \"torch.allclose\")检查。\n",
    "\n",
    "注意\n",
    "\n",
    "默认值为双精度输入设计。如果输入欠精度此检查有可能失败，例如，`FloatTensor`。\n",
    "\n",
    "警告\n",
    "\n",
    "如果在输入中任何被检查的张量有重叠的内存，换句话说，指向相同内存地址的不同切片（例如，从`torch.expand()`），此检查将有可能失败，因为在这个索引通过点扰动计算的数值梯度将改变在全部其它索引处共享内存地址的值。\n",
    "\n",
    "参数：\n",
    "\n",
    "*   **func** (_function_) – 一个Python函数，输入是张量，返回一个张量或张量元组\n",
    "*   **inputs** (_张量元组_ _or_ [_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\")) – func函数输入\n",
    "*   **grad_outputs** (_tuple of Tensor_ _or_ [_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\")_,_ _可选_) – The gradients with respect to the function’s outputs.\n",
    "*   **eps** ([_float_](https://docs.python.org/3/library/functions.html#float \"(in Python v3.7)\")_,_ _可选_) – 有限差分的扰动\n",
    "*   **atol** ([_float_](https://docs.python.org/3/library/functions.html#float \"(in Python v3.7)\")_,_ _可选_) – 绝对容差\n",
    "*   **rtol** ([_float_](https://docs.python.org/3/library/functions.html#float \"(in Python v3.7)\")_,_ _可选_) – 相对容差\n",
    "*   **gen_non_contig_grad_outputs** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 如果 grad_outputs 是 None 并且 gen_non_contig_grad_outputs 是 True，随机生成的梯度输出是不连续的\n",
    "*   **raise_exception** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) –  指示如果检查失败是否抛出一个异常。此异常给出关于失败的确切性质的更多信息。这在梯度检查调试时是有用的。\n",
    "\n",
    "| 返回: | 如果所有差都满足全部闭合条件，则为True |\n",
    "| --- | --- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiler\n",
    "\n",
    "Autograd 包含一个事件探查器，让你洞察在你的模型中不同操作的代价-CPU和GPU中都有。现在有两种模式实现-CPU-仅使用[`profile`](#torch.autograd.profiler.profile \"torch.autograd.profiler.profile\")，和使用[`emit_nvtx`](#torch.autograd.profiler.emit_nvtx \"torch.autograd.profiler.emit_nvtx\")的nvprof（注册CPU和GPU活动）\n",
    "\n",
    "```py\n",
    "class torch.autograd.profiler.profile(enabled=True, use_cuda=False)\n",
    "```\n",
    "\n",
    "上下文管理器管理autograd事件探查器状态和保持一份汇总结果。\n",
    "\n",
    "参数: \n",
    "\n",
    "*   **enabled** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 设置成False 让此上下文管理一个 no-op. 默认：`True`。\n",
    "*   **use_cuda** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 使用cudaEvent API也可以启用CUDA事件的计时。 每个张量操作增加大约4us的开销。默认:  `False`\n",
    "\n",
    "示例：\n",
    "\n",
    "```py\n",
    ">>> x = torch.randn((1, 1), requires_grad=True)\n",
    ">>> with torch.autograd.profiler.profile() as prof:\n",
    "...     y = x ** 2\n",
    "...     y.backward()\n",
    ">>> # 注意：为简洁起见，删除了一些列\n",
    "... print(prof)\n",
    "-------------------------------------  ---------------  ---------------\n",
    "Name                                          CPU time        CUDA time\n",
    "-------------------------------------  ---------------  ---------------\n",
    "PowConstant                                  142.036us          0.000us\n",
    "N5torch8autograd9GraphRootE                   63.524us          0.000us\n",
    "PowConstantBackward                          184.228us          0.000us\n",
    "MulConstant                                   50.288us          0.000us\n",
    "PowConstant                                   28.439us          0.000us\n",
    "Mul                                           20.154us          0.000us\n",
    "N5torch8autograd14AccumulateGradE             13.790us          0.000us\n",
    "N5torch8autograd5CloneE                        4.088us          0.000us\n",
    "\n",
    "```\n",
    "\n",
    "```py\n",
    "export_chrome_trace(path)\n",
    "```\n",
    "\n",
    "将EventList导出为Chrome跟踪工具文件。\n",
    "\n",
    "检查点随后被加载和检查在`chrome://tracing URL`。\n",
    "\n",
    "| 参数: | **path** ([_str_](https://docs.python.org/3/library/stdtypes.html#str \"(in Python v3.7)\")) – 将写入跟踪的路径。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "key_averages()\n",
    "```\n",
    "\n",
    "平均键上的所有函数事件.\n",
    "\n",
    "| 返回: | 一个包含FunctionEventAvg对象的EventList。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "table(sort_by=None)\n",
    "```\n",
    "\n",
    "将EventList打印为格式良好的表。\n",
    "\n",
    "| 参数: | **sort_by** ([_str_](https://docs.python.org/3/library/stdtypes.html#str \"(in Python v3.7)\")_,_ _optional_) – 用来排序事件的属性。默认以它们被注册时顺序打印。 合法的关键字包括：`cpu_time`, `cuda_time`, `cpu_time_total`, `cuda_time_total`, `count`。 |\n",
    "| --- | --- |\n",
    "| 返回: | 一个包含表格的字符串。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "total_average()\n",
    "```\n",
    "\n",
    "平均化全部事件。\n",
    "\n",
    "| 返回: | 一个FunctionEventAvg事件。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "class torch.autograd.profiler.emit_nvtx(enabled=True)\n",
    "```\n",
    "\n",
    "让每个自动求导操作发出在一个NVTX范围内的上下文管理器。\n",
    "\n",
    "当在nvprof下运行程序是有用的：\n",
    "\n",
    "```py\n",
    "nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n",
    "\n",
    "```\n",
    "\n",
    "不幸地，没有办法强制nvprof将它收集的数据输出到磁盘，所以对于CUDA分析，必须使用此上下文管理器来声明nvprof跟踪并等待进程在检查之前退出。然后，可使用NVIDIA可视化Profiler(nvvp)来显示时间线，或[`torch.autograd.profiler.load_nvprof()`](#torch.autograd.profiler.load_nvprof \"torch.autograd.profiler.load_nvprof\")可加载结果以供检查，例如：在Python REPL中。\n",
    "\n",
    "| 参数: | **enabled** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")_,_ _可选_) – 设置成False 让此上下文管理一个 no-op. 默认：`True`。 |\n",
    "| --- | --- |\n",
    "\n",
    "示例：\n",
    "\n",
    "```py\n",
    ">>> with torch.cuda.profiler.profile():\n",
    "...     model(x) # Warmup CUDA memory allocator and profiler\n",
    "...     with torch.autograd.profiler.emit_nvtx():\n",
    "...         model(x)\n",
    "\n",
    "```\n",
    "\n",
    "**Forward-backward correlation**\n",
    "\n",
    "在Nvidia Visual Profiler中查看使用emit_nvtx创建的配置文件时，将每个反向传递操作与相应的前向传递操作相关联可能很困难。 为了简化此任务，emit_nvtx将序列号信息附加到它生成的范围。\n",
    "\n",
    "在前向传递过程，每个函数范围都用`seq=&lt;N&gt;`进行修饰。 `seq`是一个运行计数器，每次创建一个新的反向Function对象时会递增，并对前向不可见。 因此，与每个前向函数范围相关联的`seq=&lt;N&gt;`注释告诉你，如果此前向函数创建了反向的Function对象，则反向对象将收到序列号N。在反向传递过程，顶层范围包装每个C++反向函数的`apply()`调用都用不可见的`stashed seq=&lt;M&gt;`进行修饰。 M是创建反向对象的序列号。 通过比较在反向不可见的序列号和在前向的序列号，你可以跟踪哪个前向操作创建了每个反向函数。\n",
    "\n",
    "在反向传递期间执行的任何函数也用`seq=&lt;N&gt;`进行修饰。 在默认反向（使用`create_graph=False`）时，此信息无关紧要，事实上，对于所有此类函数，`N`可能只是0。 作为将这些Function对象与早期的向前传递相关联的方法，只有与反向Function对象的`apply()`方法关联的顶级范围才有用。\n",
    "\n",
    "**Double-backward**\n",
    "\n",
    "另一方面，如果正在进行`create_graph=True`的反向传递（换句话说，如果你设置为double-backward），则在反向期间执行每个被给一个非零，有用的`seq=&lt;N&gt;`的函数。 这些函数本身可以稍后在double-backward期间创建Function对象来执行，就像在前向传递中原始函数所做的一样。 反向和double-backward的关系在概念上与前向和反向的关系相同：函数仍然发出当前序列号标记的范围，它们创建的Function对象仍然存储那些序列号，并且在最终的double-backward期间 向后，Function对象的`apply()`范围仍然用 `stashed seq`数字标记，可以与反向传递中的`seq`数字进行比较。\n",
    "\n",
    "```py\n",
    "torch.autograd.profiler.load_nvprof(path)\n",
    "```\n",
    "\n",
    "打开一个nvprof跟踪文件并且解析autograd注释。\n",
    "\n",
    "| 参数: | **path** ([_str_](https://docs.python.org/3/library/stdtypes.html#str \"(in Python v3.7)\")) – nvprof跟踪路径 |\n",
    "| --- | --- |\n",
    "\n",
    "## 异常检测\n",
    "\n",
    "```py\n",
    "class torch.autograd.detect_anomaly\n",
    "```\n",
    "\n",
    "上下文管理器，为自动求导引擎使能异常检测。\n",
    "\n",
    "这做了两件事：- 在启用检测的情况下运行前向传递将允许反向传递打印创建失败的反向函数的前向操作跟踪。 - 任何生成“nan”值的反向计算都会引发错误。\n",
    "\n",
    "示例\n",
    "\n",
    "```py\n",
    ">>> import torch\n",
    ">>> from torch import autograd\n",
    ">>> class MyFunc(autograd.Function):\n",
    "...     @staticmethod\n",
    "...     def forward(ctx, inp):\n",
    "...         return inp.clone()\n",
    "...     @staticmethod\n",
    "...     def backward(ctx, gO):\n",
    "...         # Error during the backward pass\n",
    "...         raise RuntimeError(\"Some error in backward\")\n",
    "...         return gO.clone()\n",
    ">>> def run_fn(a):\n",
    "...     out = MyFunc.apply(a)\n",
    "...     return out.sum()\n",
    ">>> inp = torch.rand(10, 10, requires_grad=True)\n",
    ">>> out = run_fn(inp)\n",
    ">>> out.backward()\n",
    " Traceback (most recent call last):\n",
    " File \"<stdin>\", line 1, in <module>\n",
    " File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n",
    " torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
    " File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n",
    " allow_unreachable=True)  # allow_unreachable flag\n",
    " File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n",
    " return self._forward_cls.backward(self, *args)\n",
    " File \"<stdin>\", line 8, in backward\n",
    " RuntimeError: Some error in backward\n",
    ">>> with autograd.detect_anomaly():\n",
    "...     inp = torch.rand(10, 10, requires_grad=True)\n",
    "...     out = run_fn(inp)\n",
    "...     out.backward()\n",
    " Traceback of forward call that caused the error:\n",
    " File \"tmp.py\", line 53, in <module>\n",
    " out = run_fn(inp)\n",
    " File \"tmp.py\", line 44, in run_fn\n",
    " out = MyFunc.apply(a)\n",
    " Traceback (most recent call last):\n",
    " File \"<stdin>\", line 4, in <module>\n",
    " File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n",
    " torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
    " File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n",
    " allow_unreachable=True)  # allow_unreachable flag\n",
    " File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n",
    " return self._forward_cls.backward(self, *args)\n",
    " File \"<stdin>\", line 8, in backward\n",
    " RuntimeError: Some error in backward\n",
    "\n",
    "```\n",
    "\n",
    "```py\n",
    "class torch.autograd.set_detect_anomaly(mode)\n",
    "```\n",
    "\n",
    "上下文管理器，为自动求导引擎设置异常检测开或关。\n",
    "\n",
    "`set_detect_anomaly`将基于它的参数`mode`使能或禁用自动求导异常检测。它也能作为一个上下文管理器或函数使用。\n",
    "\n",
    "异常检测行为细节见上面`detect_anomaly`。\n",
    "\n",
    "| 参数: | **mode** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")) – 标记是否使能异常检测（`True`），或禁用（`False`）。 |\n",
    "| --- | --- |\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
