{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.cuda\n",
    "\n",
    "> 译者：[bdqfork](https://github.com/bdqfork)\n",
    "\n",
    "这个包添加了对CUDA张量类型的支持，它实现了与CPU张量同样的功能，但是它使用GPU进计算。\n",
    "\n",
    "它是懒加载的，所以你可以随时导入它，并使用 [`is_available()`](#torch.cuda.is_available \"torch.cuda.is_available\") 来决定是否让你的系统支持CUDA。\n",
    "\n",
    "[CUDA semantics](notes/cuda.html#cuda-semantics) 有关于使用CUDA更详细的信息。\n",
    "\n",
    "```python\n",
    "torch.cuda.current_blas_handle()\n",
    "```\n",
    "\n",
    "返回一个cublasHandle_t指针给当前的cuBLAS处理。\n",
    "\n",
    "```py\n",
    "torch.cuda.current_device()\n",
    "```\n",
    "\n",
    "返回当前选择地设备索引。\n",
    "\n",
    "```py\n",
    "torch.cuda.current_stream()\n",
    "```\n",
    "\n",
    "返回当前选择地 [`Stream`](#torch.cuda.Stream \"torch.cuda.Stream\")。\n",
    "\n",
    "```py\n",
    "class torch.cuda.device(device)\n",
    "```\n",
    "\n",
    "Context-manager 用来改变选择的设备。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _或者_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")) – 要选择的设备索引。如果这个参数是负数或者是 `None`，那么它不会起任何作用。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.device_count()\n",
    "```\n",
    "\n",
    "返回可用的GPU数量。\n",
    "\n",
    "```py\n",
    "torch.cuda.device_ctx_manager\n",
    "```\n",
    "\n",
    " [`torch.cuda.device`](#torch.cuda.device \"torch.cuda.device\") 的别名。\n",
    "\n",
    "```py\n",
    "class torch.cuda.device_of(obj)\n",
    "```\n",
    "\n",
    "Context-manager 将当前的设备改变成传入的对象。.\n",
    "\n",
    "你可以使用张量或者存储作为参数。如果传入的对象没有分配在GPU上，这个操作是无效的。\n",
    "\n",
    "| 参数: | **obj** ([_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\") _或者_ _Storage_) – 分配在已选择的设备上的对象。|\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "释放缓存分配器当前持有的所有未占用的缓存显存，使其可以用在其他GPU应用且可以在 `nvidia-smi`可视化。\n",
    "\n",
    "注意\n",
    "\n",
    "[`empty_cache()`](#torch.cuda.empty_cache \"torch.cuda.empty_cache\") 并不会增加PyTorch可以使用的GPU显存的大小。 查看 [显存管理](notes/cuda.html#cuda-memory-management) 来获取更多的GPU显存管理的信息。\n",
    "\n",
    "```py\n",
    "torch.cuda.get_device_capability(device)\n",
    "```\n",
    "\n",
    "获取一个设备的cuda容量。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _或者_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ 可选的) – 需要返回容量的设备。如果这个参数传入的是负数，那么这个方法不会起任何作用。如果[`device`](#torch.cuda.device \"torch.cuda.device\")是`None`（默认值），会通过 [`current_device()`](#torch.cuda.current_device \"torch.cuda.current_device\")传入当前设备。 |\n",
    "| --- | --- |\n",
    "| 返回: | 设备的最大和最小的cuda容量。 |\n",
    "| --- | --- |\n",
    "| 返回 类型: | [tuple](https://docs.python.org/3/library/stdtypes.html#tuple \"(in Python v3.7)\")([int](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\"), [int](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")) |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.get_device_name(device)\n",
    "```\n",
    "\n",
    "获取设备名称。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _或者_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 需要返回名称的设备。如果参数是负数，那么将不起作用。如果[`device`](#torch.cuda.device \"torch.cuda.device\")是`None`（默认值），会通过 [`current_device()`](#torch.cuda.current_device \"torch.cuda.current_device\")传入当前设备。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.init()\n",
    "```\n",
    "\n",
    "初始化PyTorch的CUDA状态。如果你通过C API与PyTorch进行交互，你可能需要显式调用这个方法。只有CUDA的初始化完成，CUDA的功能才会绑定到Python。用户一般不应该需要这个，因为所有PyTorch的CUDA方法都会自动在需要的时候初始化CUDA。\n",
    "\n",
    "如果CUDA的状态已经初始化了，将不起任何作用。\n",
    "\n",
    "```py\n",
    "torch.cuda.is_available()\n",
    "```\n",
    "\n",
    "返回一个bool值，表示当前CUDA是否可用。\n",
    "\n",
    "```py\n",
    "torch.cuda.max_memory_allocated(device=None)\n",
    "```\n",
    "\n",
    "返回给定设备的张量的最大GPU显存使用量（以字节为单位）。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _or_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _optional_) – 选择的设备。如果 [`device`](#torch.cuda.device \"torch.cuda.device\") 是`None`（默认的），将返回 [`current_device()`](#torch.cuda.current_device \"torch.cuda.current_device\")返回的当前设备的数据。 |\n",
    "| --- | --- |\n",
    "\n",
    "注意\n",
    "\n",
    "查看 [显存管理](notes/cuda.html#cuda-memory-management) 部分了解更多关于GPU显存管理部分的详细信息。\n",
    "\n",
    "```py\n",
    "torch.cuda.max_memory_cached(device=None)\n",
    "```\n",
    "\n",
    "返回给定设备的缓存分配器管理的最大GPU显存（以字节为单位）。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _或者_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 选择的设备。如果 [`device`](#torch.cuda.device \"torch.cuda.device\") 是`None`（默认的），将返回 [`current_device()`](#torch.cuda.current_device \"torch.cuda.current_device\")返回的当前设备的数据。|\n",
    "| --- | --- |\n",
    "\n",
    "注意\n",
    "\n",
    "查看 [显存管理](notes/cuda.html#cuda-memory-management) 部分了解更多关于GPU显存管理部分的详细信息。\n",
    "\n",
    "```py\n",
    "torch.cuda.memory_allocated(device=None)\n",
    "```\n",
    "\n",
    "返回给定设备的当前GPU显存使用量（以字节为单位）。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _或者_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 选择的设备。如果 [`device`](#torch.cuda.device \"torch.cuda.device\") 是`None`（默认的），将返回 [`current_device()`](#torch.cuda.current_device \"torch.cuda.current_device\")返回的当前设备的数据。 |\n",
    "| --- | --- |\n",
    "\n",
    "注意\n",
    "\n",
    "这可能比 `nvidia-smi` 显示的数量少，因为一些没有使用的显存会被缓存分配器持有，且一些上下文需要在GPU中创建。查看 [显存管理](notes/cuda.html#cuda-memory-management) 部分了解更多关于GPU显存管理部分的详细信息。\n",
    "\n",
    "```py\n",
    "torch.cuda.memory_cached(device=None)\n",
    "```\n",
    "\n",
    "返回由缓存分配器管理的当前GPU显存（以字节为单位）。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _或者_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 选择的设备。如果 [`device`](#torch.cuda.device \"torch.cuda.device\") 是`None`（默认的），将返回 [`current_device()`](#torch.cuda.current_device \"torch.cuda.current_device\")返回的当前设备的数据。|\n",
    "| --- | --- |\n",
    "\n",
    "注意\n",
    "\n",
    "查看 [显存管理](notes/cuda.html#cuda-memory-management) 部分了解更多关于GPU显存管理部分的详细信息。\n",
    "\n",
    "```py\n",
    "torch.cuda.set_device(device)\n",
    "```\n",
    "\n",
    "设置当前设备。\n",
    "\n",
    "不鼓励使用此功能以支持 [`device`](#torch.cuda.device \"torch.cuda.device\").。在多数情况下，最好使用`CUDA_VISIBLE_DEVICES`环境变量。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _或者_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")) – 选择的设备。如果参数是负数，将不会起任何作用。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.stream(stream)\n",
    "```\n",
    "\n",
    "给定流的上下文管理器。\n",
    "\n",
    "所有CUDA在上下文中排队的内核将会被添加到选择的流中。\n",
    "\n",
    "| 参数: | **stream** ([_Stream_](#torch.cuda.Stream \"torch.cuda.Stream\")) – 选择的流。如果为`None`，这个管理器将不起任何作用。|\n",
    "| --- | --- |\n",
    "\n",
    "注意\n",
    "\n",
    "流是针对每个设备的，这个方法只更改当前选择设备的“当前流”。选择一个不同的设备流是不允许的。\n",
    "\n",
    "```py\n",
    "torch.cuda.synchronize()\n",
    "```\n",
    "\n",
    "等待所有当前设备的所有流完成。\n",
    "\n",
    "## 随机数生成器\n",
    "\n",
    "```py\n",
    "torch.cuda.get_rng_state(device=-1)\n",
    "```\n",
    "\n",
    "以ByteTensor的形式返回当前GPU的随机数生成器的状态。\n",
    "\n",
    "| 参数: | **device** ([_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 需要返回RNG状态的目标设备。默认：-1 (例如，使用当前设备)。 |\n",
    "| --- | --- |\n",
    "\n",
    "警告\n",
    "\n",
    "此函数会立即初始化CUDA。\n",
    "\n",
    "```py\n",
    "torch.cuda.set_rng_state(new_state, device=-1)\n",
    "```\n",
    "\n",
    "设置当前GPU的随机数生成器状态。\n",
    "\n",
    "| 参数: | **new_state** ([_torch.ByteTensor_](tensors.html#torch.ByteTensor \"torch.ByteTensor\")) – 目标状态 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.manual_seed(seed)\n",
    "```\n",
    "\n",
    "设置为当前GPU生成随机数的种子。如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。\n",
    "\n",
    "| 参数: | **seed** ([_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")) – 目标种子。 |\n",
    "| --- | --- |\n",
    "\n",
    "警告\n",
    "\n",
    "如果您使用的是多GPU模型，那么这个函数不具有确定性。设置用于在所有GPU上生成随机数的种子，使用 [`manual_seed_all()`](#torch.cuda.manual_seed_all \"torch.cuda.manual_seed_all\").\n",
    "\n",
    "```py\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "```\n",
    "\n",
    "设置用于在所有GPU上生成随机数的种子。 如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。\n",
    "\n",
    "| 参数: | **seed** ([_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")) – 目标种子。|\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.seed()\n",
    "```\n",
    "\n",
    "将用于生成随机数的种子设置为当前GPU的随机数。 如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。\n",
    "\n",
    "警告\n",
    "\n",
    "如果您使用的是多GPU模型，此函数将只初始化一个GPU上的种子。在所有GPU上将用于生成随机数的种子设置为随机数， 使用 [`seed_all()`](#torch.cuda.seed_all \"torch.cuda.seed_all\").\n",
    "\n",
    "```py\n",
    "torch.cuda.seed_all()\n",
    "```\n",
    "\n",
    "在所有GPU上将用于生成随机数的种子设置为随机数。 如果CUDA不可用，可以安全地调用此函数；在这种情况下，它将被静默地忽略。\n",
    "\n",
    "```py\n",
    "torch.cuda.initial_seed()\n",
    "```\n",
    "\n",
    "返回当前GPU的当前随机种子。\n",
    "\n",
    "警告\n",
    "\n",
    "此函数会立即初始化CUDA。\n",
    "\n",
    "## 通信集合\n",
    "\n",
    "```py\n",
    "torch.cuda.comm.broadcast(tensor, devices)\n",
    "```\n",
    "\n",
    "将张量广播到多个GPU。\n",
    "\n",
    "| 参数: | \n",
    "\n",
    "*   **tensor** ([_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\")) – 需要广播的张量。\n",
    "*   **devices** (_Iterable_) – 一个要被广播的可迭代的张量集合。注意，它应该是这样的形式 (src, dst1, dst2, …)，其中第一个元素是广播的源设备。\n",
    "\n",
    "| 返回: | 一个包含`tensor`副本的元组，放置在与设备索引相对应的设备上。|\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760)\n",
    "```\n",
    "\n",
    "将序列张量广播到指定的GPU。 首先将小型张量合并到缓冲区中以减少同步次数。\n",
    "\n",
    "| 参数: | \n",
    "\n",
    "*   **tensors** (_sequence_) – 要被广播的张量。\n",
    "*   **devices** (_Iterable_) – 一个要被广播的可迭代的张量集合。注意，它应该是这样的形式 (src, dst1, dst2, …)，其中第一个元素是广播的源设备。\n",
    "*   **buffer_size** ([_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")) – 用于合并的缓冲区的最大大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 返回: | 一个包含`tensor`副本的元组，放置在与设备索引相对应的设备上。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.comm.reduce_add(inputs, destination=None)\n",
    "```\n",
    "\n",
    "从多个GPU上对张量进行求和。\n",
    "\n",
    "所有输入必须有相同的形状。\n",
    "\n",
    "| 参数: | \n",
    "\n",
    "*   **inputs** (_Iterable__[_[_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\")_]_) – 一个可迭代的要添加的张量集合。\n",
    "*   **destination** ([_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 输出所在的设备。(默认值: 当前设备)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 返回: | 一个包含按元素相加的所有输入的和的张量，在 `destination` 设备上。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.comm.scatter(tensor, devices, chunk_sizes=None, dim=0, streams=None)\n",
    "```\n",
    "\n",
    "将张量分散在多个GPU上。\n",
    "\n",
    "| 参数: | \n",
    "\n",
    "*   **tensor** ([_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\")) – 要分散的张量.\n",
    "*   **devices** (_Iterable__[_[_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_]_) – 可迭代的数字集合，指明在哪个设备上的张量要被分散。\n",
    "*   **chunk_sizes** (_Iterable__[_[_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_]__,_ _可选的_) – 每个设备上放置的块的大小。它应该和`devices`的长度相等，并相加等于`tensor.size(dim)`。如果没有指定，张量将会被分散成相同的块。\n",
    "*   **dim** ([_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 分块张量所在的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 返回: | 一个包含`tensor`块的元组，分散在给定的`devices`上。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.comm.gather(tensors, dim=0, destination=None)\n",
    "```\n",
    "\n",
    "从多个GPU收集张量。\n",
    "\n",
    "在所有维度中与`dim`不同的张量尺寸必须匹配。\n",
    "\n",
    "| 参数: | \n",
    "\n",
    "*   **tensors** (_Iterable__[_[_Tensor_](tensors.html#torch.Tensor \"torch.Tensor\")_]_) – 可迭代的张量集合。\n",
    "*   **dim** ([_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")) – 纬度，张量将会在这个维度上被连接。\n",
    "*   **destination** ([_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 输出设备(-1 表示 CPU, 默认值: 当前设备)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 返回: | 在`destination` 设备上的张量，这是沿着`dim`连接张量的结果。 |\n",
    "| --- | --- |\n",
    "\n",
    "## 流和事件\n",
    "\n",
    "```py\n",
    "class torch.cuda.Stream\n",
    "```\n",
    "\n",
    "围绕CUDA流的包装器。\n",
    "\n",
    "CUDA流是属于特定设备的线性执行序列，独立于其他流。 查看 [CUDA semantics](notes/cuda.html#cuda-semantics) 获取更详细的信息。\n",
    "\n",
    "参数:\n",
    "\n",
    "*   **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _或者_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 要在其上分配流的设备。 如果 [`device`](#torch.cuda.device \"torch.cuda.device\") 为`None`（默认值）或负整数，则将使用当前设备。\n",
    "*   **priority** ([_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 流的优先级。数字越小，优先级越高。\n",
    "\n",
    "```py\n",
    "query()\n",
    "```\n",
    "\n",
    "检查提交的所有工作是否已完成。\n",
    "\n",
    "| 返回: | 一个布尔值，表示此流中的所有内核是否都已完成。|\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "record_event(event=None)\n",
    "```\n",
    "\n",
    "记录一个事件。\n",
    "\n",
    "| 参数: | **event** ([_Event_](#torch.cuda.Event \"torch.cuda.Event\")_,_ _可选的_) – 需要记录的事件。如果没有给出，将分配一个新的。 |\n",
    "| --- | --- |\n",
    "| 返回: | 记录的事件。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "synchronize()\n",
    "```\n",
    "\n",
    "等待此流中的所有内核完成。\n",
    "\n",
    "注意\n",
    "\n",
    "这是一个围绕 `cudaStreamSynchronize()`的包装： 查看 [CUDA 文档](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html) 获取更详细的信息。\n",
    "\n",
    "```py\n",
    "wait_event(event)\n",
    "```\n",
    "\n",
    "使提交给流的所有未来工作等待事件。\n",
    "\n",
    "| 参数: | **event** ([_Event_](#torch.cuda.Event \"torch.cuda.Event\")) – 需要等待的事件。 |\n",
    "| --- | --- |\n",
    "\n",
    "注意\n",
    "\n",
    "这是一个围绕 `cudaStreamWaitEvent()`的包装： 查看 [CUDA 文档](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html) 获取更详细的信息。\n",
    "\n",
    "此函数返回时无需等待`event`： 只有未来的操作受到影响。\n",
    "\n",
    "```py\n",
    "wait_stream(stream)\n",
    "```\n",
    "\n",
    "与另一个流同步。\n",
    "\n",
    "提交给此流的所有未来工作将等到所有内核在呼叫完成时提交给给定流。\n",
    "\n",
    "| 参数: | **stream** ([_Stream_](#torch.cuda.Stream \"torch.cuda.Stream\")) – 要同步的流。 |\n",
    "| --- | --- |\n",
    "\n",
    "注意\n",
    "\n",
    "此函数返回时不等待`stream`中当前排队的内核 ： 只有未来的操作受到影响。\n",
    "\n",
    "```py\n",
    "class torch.cuda.Event(enable_timing=False, blocking=False, interprocess=False, _handle=None)\n",
    "```\n",
    "\n",
    "围绕CUDA事件的包装。\n",
    "\n",
    "参数:\n",
    "\n",
    "*   **enable_timing** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")) – 表示事件是否应该测量时间（默认值：`False`）\n",
    "*   **blocking** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")) – 如果是`True`， [`wait()`](#torch.cuda.Event.wait \"torch.cuda.Event.wait\") 将会阻塞 (默认值: `False`)\n",
    "*   **interprocess** ([_bool_](https://docs.python.org/3/library/functions.html#bool \"(in Python v3.7)\")) – 如果是 `True`，事件将会在进程中共享 (默认值: `False`)\n",
    "\n",
    "```py\n",
    "elapsed_time(end_event)\n",
    "```\n",
    "\n",
    "返回记录事件之前经过的时间。\n",
    "\n",
    "```py\n",
    "ipc_handle()\n",
    "```\n",
    "\n",
    "返回此事件的IPC句柄。\n",
    "\n",
    "```py\n",
    "query()\n",
    "```\n",
    "\n",
    "检测事件是否被记录。\n",
    "\n",
    "| 返回: | 一个布尔值，表示事件是否被记录。|\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "record(stream=None)\n",
    "```\n",
    "\n",
    "记录给定流的一个事件。\n",
    "\n",
    "```py\n",
    "synchronize()\n",
    "```\n",
    "\n",
    "和一个事件同步。\n",
    "\n",
    "```py\n",
    "wait(stream=None)\n",
    "```\n",
    "\n",
    "使给定的流等待一个事件。\n",
    "\n",
    "## 显存管理\n",
    "\n",
    "```py\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "释放当前由缓存分配器保存的所有未占用的缓存显存，以便可以在其他GPU应用程序中使用这些缓存并在`nvidia-smi`中可见。\n",
    "\n",
    "注意\n",
    "\n",
    "[`empty_cache()`](#torch.cuda.empty_cache \"torch.cuda.empty_cache\") 不会增加PyTorch可用的GPU显存量。 查看 [显存管理](notes/cuda.html#cuda-memory-management) 以了解更多GPU显存管理的详细信息。\n",
    "\n",
    "```py\n",
    "torch.cuda.memory_allocated(device=None)\n",
    "```\n",
    "\n",
    "返回给定设备的当前GPU显存使用量（以字节为单位）。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _或者_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 选定的设备。如果 [`device`](#torch.cuda.device \"torch.cuda.device\") 是`None`（默认的），将返回 [`current_device()`](#torch.cuda.current_device \"torch.cuda.current_device\")返回的当前设备的数据。 |\n",
    "| --- | --- |\n",
    "\n",
    "注意\n",
    "\n",
    "这可能比 `nvidia-smi` 显示的数量少，因为一些没有使用的显存会被缓存分配器持有，且一些上下文需要在GPU中创建。查看 [显存管理](notes/cuda.html#cuda-memory-management) 部分了解更多关于GPU显存管理部分的详细信息。\n",
    "\n",
    "```py\n",
    "torch.cuda.max_memory_allocated(device=None)\n",
    "```\n",
    "\n",
    "返回给定设备的张量的最大GPU显存使用量（以字节为单位）。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _或者_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) –  选择的设备。如果 [`device`](#torch.cuda.device \"torch.cuda.device\") 是`None`（默认的），将返回 [`current_device()`](#torch.cuda.current_device \"torch.cuda.current_device\")返回的当前设备的数据。  |\n",
    "| --- | --- |\n",
    "\n",
    "注意\n",
    "\n",
    "查看 [显存管理](notes/cuda.html#cuda-memory-management) 部分了解更多关于GPU显存管理部分的详细信息。\n",
    "\n",
    "```py\n",
    "torch.cuda.memory_cached(device=None)\n",
    "```\n",
    "\n",
    "返回由缓存分配器管理的当前GPU显存（以字节为单位）。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _or_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 选择的设备。如果 [`device`](#torch.cuda.device \"torch.cuda.device\") 是`None`（默认的），将返回 [`current_device()`](#torch.cuda.current_device \"torch.cuda.current_device\")返回的当前设备的数据。 |\n",
    "| --- | --- |\n",
    "\n",
    "注意\n",
    "\n",
    "查看 [显存管理](notes/cuda.html#cuda-memory-management) 部分了解更多关于GPU显存管理部分的详细信息。\n",
    "\n",
    "```py\n",
    "torch.cuda.max_memory_cached(device=None)\n",
    "```\n",
    "\n",
    "返回给定设备的缓存分配器管理的最大GPU显存（以字节为单位）。\n",
    "\n",
    "| 参数: | **device** ([_torch.device_](tensor_attributes.html#torch.torch.device \"torch.torch.device\") _或者_ [_int_](https://docs.python.org/3/library/functions.html#int \"(in Python v3.7)\")_,_ _可选的_) – 选择的设备。如果 [`device`](#torch.cuda.device \"torch.cuda.device\") 是`None`（默认的），将返回 [`current_device()`](#torch.cuda.current_device \"torch.cuda.current_device\")返回的当前设备的数据。|\n",
    "| --- | --- |\n",
    "\n",
    "注意\n",
    "\n",
    "查看 [显存管理](notes/cuda.html#cuda-memory-management) 部分了解更多关于GPU显存管理部分的详细信息。\n",
    "\n",
    "## NVIDIA Tools Extension (NVTX)\n",
    "\n",
    "```py\n",
    "torch.cuda.nvtx.mark(msg)\n",
    "```\n",
    "\n",
    "描述某个时刻发生的瞬时事件。\n",
    "\n",
    "| 参数: | **msg** (_string_) – 与时间相关的ASCII信息。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.nvtx.range_push(msg)\n",
    "```\n",
    "\n",
    "将范围推到嵌套范围跨度的堆栈上。 返回启动范围的从零开始的深度。\n",
    "\n",
    "| 参数: | **msg** (_string_) – 与时间相关的ASCII信息。 |\n",
    "| --- | --- |\n",
    "\n",
    "```py\n",
    "torch.cuda.nvtx.range_pop()\n",
    "```\n",
    "\n",
    "从一堆嵌套范围跨度中弹出一个范围。 返回结束范围的从零开始的深度。\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}