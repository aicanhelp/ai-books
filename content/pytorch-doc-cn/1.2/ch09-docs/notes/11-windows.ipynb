{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windows FAQ  \n",
    "\n",
    "> 译者：[冯宝宝](https://github.com/PEGASUS1993)\n",
    "\n",
    "## 从源码中构建  \n",
    "\n",
    "### 包含可选组件  \n",
    "\n",
    "Windows PyTorch有两个受支持的组件：MKL和MAGMA。 以下是使用它们构建的步骤。  \n",
    "\n",
    "```shell\n",
    "REM Make sure you have 7z and curl installed.\n",
    "\n",
    "REM Download MKL files\n",
    "curl https://s3.amazonaws.com/ossci-windows/mkl_2018.2.185.7z -k -O\n",
    "7z x -aoa mkl_2018.2.185.7z -omkl\n",
    "\n",
    "REM Download MAGMA files\n",
    "REM cuda90/cuda92/cuda100 is also available in the following line.\n",
    "set CUDA_PREFIX=cuda80\n",
    "curl -k https://s3.amazonaws.com/ossci-windows/magma_2.4.0_%CUDA_PREFIX%_release.7z -o magma.7z\n",
    "7z x -aoa magma.7z -omagma\n",
    "\n",
    "REM Setting essential environment variables\n",
    "set \"CMAKE_INCLUDE_PATH=%cd%\\\\mkl\\\\include\"\n",
    "set \"LIB=%cd%\\\\mkl\\\\lib;%LIB%\"\n",
    "set \"MAGMA_HOME=%cd%\\\\magma\"\n",
    "\n",
    "```\n",
    "\n",
    "### 为Windows构建加速CUDA  \n",
    "\n",
    "Visual Studio当前不支持并行自定义任务。 作为替代方案，我们可以使用Ninja来并行化CUDA构建任务。 只需键入几行代码即可使用它。 \n",
    "\n",
    "```shell\n",
    "REM Let's install ninja first.\n",
    "pip install ninja\n",
    "\n",
    "REM Set it as the cmake generator\n",
    "set CMAKE_GENERATOR=Ninja  \n",
    "``` \n",
    "\n",
    "### 脚本一键安装  \n",
    "\n",
    "你可以参考[这些脚本](https://github.com/peterjc123/pytorch-scripts)。它会给你指导方向。  \n",
    "\n",
    "## 扩展 \n",
    "\n",
    "### CFEI扩展  \n",
    "\n",
    "对[CFFI](https://cffi.readthedocs.io/en/latest/)扩展的支持是非常试验性的。在Windows下启用它通常有两个步骤。\n",
    "\n",
    "首先，在Extension对象中指定其他库以使其在Windows上构建。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ffi = create_extension(\n",
    "    '_ext.my_lib',\n",
    "    headers=headers,\n",
    "    sources=sources,\n",
    "    define_macros=defines,\n",
    "    relative_to=__file__,\n",
    "    with_cuda=with_cuda,\n",
    "    extra_compile_args=[\"-std=c99\"],\n",
    "    libraries=['ATen', '_C'] # Append cuda libaries when necessary, like cudart\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其次，这是“由`extern THCState *state`状态引起的未解决的外部符号状态”的工作场所;\n",
    "\n",
    "将源代码从C更改为C ++。 下面列出了一个例子。 \n",
    "\n",
    "```c\n",
    "#include <THC/THC.h>\n",
    "#include <ATen/ATen.h>\n",
    "\n",
    "THCState *state = at::globalContext().thc_state;\n",
    "\n",
    "extern \"C\" int my_lib_add_forward_cuda(THCudaTensor *input1, THCudaTensor *input2,\n",
    "                                        THCudaTensor *output)\n",
    "{\n",
    "    if (!THCudaTensor_isSameSizeAs(state, input1, input2))\n",
    "    return 0;\n",
    "    THCudaTensor_resizeAs(state, output, input1);\n",
    "    THCudaTensor_cadd(state, output, input1, 1.0, input2);\n",
    "    return 1;\n",
    "}\n",
    "\n",
    "extern \"C\" int my_lib_add_backward_cuda(THCudaTensor *grad_output, THCudaTensor *grad_input)\n",
    "{\n",
    "    THCudaTensor_resizeAs(state, grad_input, grad_output);\n",
    "    THCudaTensor_fill(state, grad_input, 1);\n",
    "    return 1;\n",
    "}\n",
    "\n",
    "```  \n",
    "\n",
    "### C++扩展  \n",
    "\n",
    "与前一种类型相比，这种类型的扩展具有更好的支持。不过它仍然需要一些手动配置。首先，打开VS 2017的x86_x64交叉工具命令提示符。然后，在其中打开Git-Bash。它通常位于C：\\Program Files\\Git\\git-bash.exe中。最后，您可以开始编译过程。  \n",
    "\n",
    "## 安装  \n",
    "\n",
    "### 在Win32 找不到安装包  \n",
    "\n",
    "```\n",
    "Solving environment: failed\n",
    "\n",
    "PackagesNotFoundError: The following packages are not available from current channels:\n",
    "\n",
    "- pytorch\n",
    "\n",
    "Current channels:\n",
    "- https://conda.anaconda.org/pytorch/win-32\n",
    "- https://conda.anaconda.org/pytorch/noarch\n",
    "- https://repo.continuum.io/pkgs/main/win-32\n",
    "- https://repo.continuum.io/pkgs/main/noarch\n",
    "- https://repo.continuum.io/pkgs/free/win-32\n",
    "- https://repo.continuum.io/pkgs/free/noarch\n",
    "- https://repo.continuum.io/pkgs/r/win-32\n",
    "- https://repo.continuum.io/pkgs/r/noarch\n",
    "- https://repo.continuum.io/pkgs/pro/win-32\n",
    "- https://repo.continuum.io/pkgs/pro/noarch\n",
    "- https://repo.continuum.io/pkgs/msys2/win-32\n",
    "- https://repo.continuum.io/pkgs/msys2/noarch\n",
    "\n",
    "```\n",
    "Pytorch不能在32位系统中工作运行。请安装使用64位的Windows和Python。  \n",
    "\n",
    "### 导入错误  \n",
    "\n",
    "```C\n",
    "from torch._C import *\n",
    "\n",
    "ImportError: DLL load failed: The specified module could not be found.\n",
    "```\n",
    "\n",
    "问题是由基本文件丢失导致的。实际上，除了VC2017可再发行组件和一些mkl库之外，我们几乎包含了PyTorch对conda包所需的所有基本文件。您可以通过键入以下命令来解决此问题。\n",
    "\n",
    "```c\n",
    "conda install -c peterjc123 vc vs2017_runtime\n",
    "conda install mkl_fft intel_openmp numpy mkl\n",
    "```\n",
    "\n",
    "至于wheel包(轮子)，由于我们没有包含一些库和VS2017可再发行文件，请手动安装它们。可以下载[VS 2017可再发行安装程序]((https://aka.ms/vs/15/release/VC_redist.x64.exe))。你还应该注意你的Numpy的安装。 确保它使用MKL而不是OpenBLAS版本的。您可以输入以下命令。  \n",
    "\n",
    "```c\n",
    "pip install numpy mkl intel-openmp mkl_fft\n",
    "```  \n",
    "\n",
    "另外一种可能是你安装了GPU版本的Pytorch但是电脑中并没有NVIDIA的显卡。碰到这种情况，就把GPU版本的Pytorch换成CPU版本的就好了。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "from torch._C import *\n",
    "\n",
    "ImportError: DLL load failed: The operating system cannot run %1.\n",
    "```\n",
    "\n",
    "这实际上是Anaconda的上游问题。使用conda-forge通道初始化环境时,将出现此问题。您可以通过此命令修复intel-openmp库。  \n",
    "\n",
    "## 使用（多处理）  \n",
    "\n",
    "### 无if语句保护的多进程处理错误  \n",
    "\n",
    "```py\n",
    "RuntimeError:\n",
    "    An attempt has been made to start a new process before the\n",
    "    current process has finished its bootstrapping phase.\n",
    "\n",
    "   This probably means that you are not using fork to start your\n",
    "   child processes and you have forgotten to use the proper idiom\n",
    "   in the main module:\n",
    "\n",
    "       if __name__ == '__main__':\n",
    "           freeze_support()\n",
    "           ...\n",
    "\n",
    "   The \"freeze_support()\" line can be omitted if the program\n",
    "   is not going to be frozen to produce an executable.\n",
    "\n",
    "```  \n",
    "\n",
    "在Windows上实现`多进程处理`是不同的，它使用的是spawn而不是fork。 因此，我们必须使用if子句包装代码，以防止代码执行多次。将您的代码重构为以下结构。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def main()\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # do something here\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多进程处理错误“坏道”  \n",
    "\n",
    "```\n",
    "ForkingPickler(file, protocol).dump(obj)\n",
    "\n",
    "BrokenPipeError: [Errno 32] Broken pipe\n",
    "```\n",
    "\n",
    "当在父进程完成发送数据之前子进程结束时，会发生此问题。您的代码可能有问题。您可以通过将DataLoader的num_worker减少为零来调试代码，并查看问题是否仍然存在。  \n",
    "\n",
    "### 多进程处理错误“驱动程序关闭”  \n",
    "\n",
    "```\n",
    "Couldn’t open shared file mapping: <torch_14808_1591070686>, error code: <1455> at torch\\lib\\TH\\THAllocator.c:154\n",
    "\n",
    "[windows] driver shut down\n",
    "```\n",
    "\n",
    "请更新您的显卡驱动程序。如果这种情况持续存在，则可能是您的显卡太旧或所需要的计算能力对您的显卡负担太重。请根据[这篇文章]((https://www.pugetsystems.com/labs/hpc/Working-around-TDR-in-Windows-for-a-better-GPU-computing-experience-777/).)更新TDR设置。\n",
    "\n",
    "### CUDA IPC操作  \n",
    "\n",
    "```\n",
    "THCudaCheck FAIL file=torch\\csrc\\generic\\StorageSharing.cpp line=252 error=63 : OS call failed or operation not supported on this OS\n",
    "```\n",
    "\n",
    "Windows不支持它们。在CUDA张量上进行多处理这样的事情无法成功，有两种选择:  \n",
    "\n",
    "1\\.不要使用多处理。将Data Loader的num_worker设置为零。  \n",
    "\n",
    "2\\.采用共享CPU张量方法。确保您的自定义`DataSet`返回CPU张量。\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
